# Note! The course is currently being updated for the 2021 version! The information will change!

# Introduction Course in Machine Learning at Uppsala University

Welcome to the github repo for the introduction course in Machine Learning at Uppsala University. This repo contain all necessary material and information for the course.

## Course Background 
The course is given to second-year master students in the Statistics masters program.

## Prerequisites
See course syllabus [here](https://www.uu.se/en/admissions/freestanding-courses/course-syllabus/?kKod=2IS074&lasar=).

## Course Goals
See course syllabus [here](https://www.uu.se/en/admissions/freestanding-courses/course-syllabus/?kKod=2IS074&lasar=).

## (Rough) Course Plan
A rough course plan with reading instructions can be found  [here](https://docs.google.com/spreadsheets/d/1HC_QN2mCq9bkCPzmkP8RaR3RokFQCWo9oPuU7rFyR8Y/edit?usp=sharing).

## Schedule
The course schedule can be found on TimeEdit [here](https://cloud.timeedit.net/uu/web/schema/) (search for course code 2ST122).

## Course Literature and Video Material
Below are the main references for the course. All books are available free online. Some articles may need access from a Uppsala University network.

### Literature

- ESL: Hastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. The elements of statistical learning: data mining, inference, and prediction. 2nd Edition. Springer Science & Business Media, 2009. [online access](https://web.stanford.edu/~hastie/ElemStatLearn/)
- DL: Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron. Deep learning. MIT Press, 2017. [online access](https://www.deeplearningbook.org/)
- DLR: Chollet, François, and Joseph J. Allaire. Deep Learning with R. Manning, 2018.  [online access](https://www.manning.com/books/deep-learning-with-r#toc)
- Sutton, R. S., and Barto, A. G.. Reinforcement learning: An introduction. MIT Press, 2020. [online access](http://incompleteideas.net/book/RLbook2020.pdf)

In addition, the following material will also be included (note that you might need to access the material through the Uppsala University network):

- Efron, B. (2020). Prediction, Estimation, and Attribution. Journal of the American Statistical Association, 115(530), 636-655. [online access](https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1762613)
- Ruder, S., 2016. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747. [online access](https://arxiv.org/abs/1609.04747)
- Salganik, M. J. et al. Measuring the predictability of life outcomes with a scientific mass collaboration. Proceedings of the National Academy of Sciences Apr 2020, 117 (15) 8398-8403; DOI: 10.1073/pnas.1915006117 [online access](https://www.pnas.org/content/117/15/8398)
- Precision and Recall at [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall).
- Kingma D.P. and Welling M. An Introduction to Variational Autoencoders, 2019. [online access](https://arxiv.org/pdf/1906.02691.pdf)
- Smyth, P. The EM algorithm for Gaussian Mixtures, 2020. [online access](https://www.ics.uci.edu/~smyth/courses/cs274/notes/EMnotes.pdf)
- Alammar, J. The illustrated Transformer, 2018a. [online](http://jalammar.github.io/illustrated-transformer/)
- Alammar, J. The illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning), 2018b. [online](http://jalammar.github.io/illustrated-bert/)
- Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. [online](https://arxiv.org/abs/1810.04805)
- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I., 2017. Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008). [online](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
- Olah, C. 2015. Understanding LSTM Networks. [online](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- Ng, A. 2019. The EM algorithm. CS229, Lecture Notes [online](https://see.stanford.edu/materials/aimlcs229/cs229-notes8.pdf)
- Griffiths, M. and Steyvers. 2004. Finding Scientific topics [online](https://www.pnas.org/content/pnas/101/suppl_1/5228.full.pdf)
- Blei, D. 2012. Probabilistic Topic Models [online](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)
- Rocca, J. 2019. Understanding Variational Autoencoders [online](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73).

The literature list might change slightly during the course.

### Video material

- Chen, T. (2016) XGBoost: A scalable Tree Boosting System  [online](https://www.youtube.com/watch?v=Vly8xGnNiWs)
- ISLV: Hastie and Tibshirani, Introduction to Statistical Learning (Video material) [online access](http://auapps.american.edu/alberto/www/analytics/ISLRLectures.html)
- 3B1B: Three Blue One Brown on Neural Networks [online access](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- Dirac, L. (2019) LSTM is dead! Long live Transformers. [online access](https://www.youtube.com/watch?v=S27pHKBEp30) 
- Ng, Andrew (2020) One convolutional layer. [online access](https://www.youtube.com/watch?v=jPOAS7uCODQ&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=7) 
- Hand, Paul, Variational Autoencoders [online](https://www.youtube.com/watch?app=desktop&v=c27SHdQr4lw)


## Course practicalities

Online course discussions will be held through Slack. See Studium for details on how to log in.

### Location

The course will have 1-2 guest lectures that will be given through Zoom, otherwise the course will be taken place on campus. Information and support for students on Zoom can be found [here](https://mp.uu.se/c/perm/link?p=267521030). 

## Teachers

Main Teacher: Måns Magnusson

Teaching assistant: Andreas Östling

## Course structure

### Main part
The course consists of 8 blocks (weeks) of material. Each week consists of the following (expected workload in parenthesis):
- Two combined lectures/computer labs (approx. 4h per week)
- Online video material and reading assignments (approx. 2-4 h per week)
- An individual computer assignment (approx. 12-16 h per week)

### Computer assignments
Each week an individual computer assignment is done with focus in implementation of the main part of the material. Each assignment is completed individually and should follow the computer assignment template.

The computer assignments should be returned no later than **Sunday 23.59 each week**. A second possibility to turn in assignments is possible at the end of the course. For a detailed list of deadlines see the [rough course plan](https://docs.google.com/spreadsheets/d/1HC_QN2mCq9bkCPzmkP8RaR3RokFQCWo9oPuU7rFyR8Y/edit?usp=sharing).

### Machine Learning, AI and ethics
A guest lecture will be given on AI and ethics by [Karim Jebari](https://www.iffs.se/en/research/researchers/karim-jebari/).

## Course Project
The last two weeks will be focused on a course project where a group of 2-3 students choose a data and create a supervised machine learning predictor for a real-world dataset. 

Details and instructions on the project work can be found [here](https://github.com/MansMeg/IntroML/blob/master/project/).


## Frequently Asked Questions (FAQ)

Frequently asked questions will be collected [here](https://github.com/MansMeg/IntroML/blob/master/FAQ.md).

