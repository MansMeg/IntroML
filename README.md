# Note! 
The course is currently being updated for the 2021 version. Some of the information might change until the start of the course. 

Now assignment 1-3 (block 1-4) has been updated to the 2021 version.

# Introduction Course in Machine Learning at Uppsala University

Welcome to the GitHub repo for the introduction course in Introduction to Machine Learning at Uppsala University. This repo contains all necessary material and information for the course.

## Course Background 
The course is given to second-year master students in the Statistics masters program.


## Prerequisites and Course Goals
See course syllabus [here](https://www.uu.se/en/admissions/freestanding-courses/course-syllabus/?kpid=39843&type=1).


## (Rough) Course Plan
You can find a rough course plan with reading instructions [here](https://docs.google.com/spreadsheets/d/1HC_QN2mCq9bkCPzmkP8RaR3RokFQCWo9oPuU7rFyR8Y/edit?usp=sharing).

## Schedule
You can find the course schedule on TimeEdit [here](https://cloud.timeedit.net/uu/web/schema/) (search for course code 2ST122).

## Grading
The course is graded with U (Underkänd/Fail), G (Godkänd/Pass), VG (väl godkänd/Pass with distinction).

To pass, you should pass all assignments and the mini-project. 

To get the grade VG on the course, a total of 7 or more VG points is needed. Each assignment has an additional task to complete to get VG on the assignment (and one VG-point). If the final mini-project gets VG, the students are awarded 2 VG points for the mini-project.


## Course Literature and Video Material
Below are the main references for the course. All books are available free online. Some articles may need access from an Uppsala University network.

### Literature

- ESL: Hastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. The elements of statistical learning: data mining, inference, and prediction. 2nd Edition. Springer Science & Business Media, 2009. [online access](https://web.stanford.edu/~hastie/ElemStatLearn/)
- DL: Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron. Deep learning. MIT Press, 2017. [online access](https://www.deeplearningbook.org/)
- DLR: Chollet, François, and Joseph J. Allaire. Deep Learning with R. Manning, 2018.  [online access](https://www.manning.com/books/deep-learning-with-r#toc)
- Sutton, R. S., and Barto, A. G.. Reinforcement learning: An introduction. MIT Press, 2020. [online access](http://incompleteideas.net/book/RLbook2020.pdf)

In addition, the following material will also be included (note that you might need to access the material through the Uppsala University network):

- Efron, B. (2020). Prediction, Estimation, and Attribution. Journal of the American Statistical Association, 115(530), 636-655. [online access](https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1762613)
- Salganik, M. J. et al. Measuring the predictability of life outcomes with a scientific mass collaboration. Proceedings of the National Academy of Sciences Apr 2020, 117 (15) 8398-8403; DOI: 10.1073/pnas.1915006117 [online access](https://www.pnas.org/content/117/15/8398)
- Precision and Recall at [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall).
- Kingma DP and Welling M. An Introduction to Variational Autoencoders, 2019. [online access](https://arxiv.org/pdf/1906.02691.pdf)
- Smyth, P. The EM algorithm for Gaussian Mixtures, 2020. [online access](https://www.ics.uci.edu/~smyth/courses/cs274/notes/EMnotes.pdf)
- Alamar, J. Visualizing neural machine translation mechanics, 2018a. [online](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- Alamar, J. The illustrated Transformer, 2018b. [online](http://jalammar.github.io/illustrated-transformer/)
- Alamar, J. The illustrated BERT, Elmo, and co. (How NLP Cracked Transfer Learning), 2018c. [online](http://jalammar.github.io/illustrated-bert/)
- Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. [online](https://arxiv.org/abs/1810.04805)
- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I., 2017. Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008). [online](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
- Olah, C. 2015. Understanding LSTM Networks. [online](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- Ng, A. 2019. The EM algorithm. CS229, Lecture Notes [online](https://see.stanford.edu/materials/aimlcs229/cs229-notes8.pdf)
- Griffiths, M. and Steyvers. 2004. Finding Scientific topics [online](https://www.pnas.org/content/pnas/101/suppl_1/5228.full.pdf)
- Blei, D. 2012. Probabilistic Topic Models [online](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)
- Rocca, J. 2019. Understanding Variational Autoencoders [online](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73).

The literature list might change slightly during the course.

### Video material

- Chen, T. (2016) XGBoost: A scalable Tree Boosting System  [online](https://www.youtube.com/watch?v=Vly8xGnNiWs)
- ISLV: Hastie and Tibshirani, Introduction to Statistical Learning (Video material) [online access](http://auapps.american.edu/alberto/www/analytics/ISLRLectures.html)
- 3B1B: Three Blue One Brown on Neural Networks [online access](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- Dirac, L. (2019) LSTM is dead! Long live Transformers. [online access](https://www.youtube.com/watch?v=S27pHKBEp30) 
- Ng, Andrew (2020) One convolutional layer. [online access](https://www.youtube.com/watch?v=jPOAS7uCODQ&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=7) 
- Hand, Paul, Variational Autoencoders [online](https://www.youtube.com/watch?app=desktop&v=c27SHdQr4lw)

## Recommended workflow for each block

1. Watch the videos (although, optional)
2. Read the literature according to the rough course plan
3. Do the assignment

## Course practicalities

Online course discussions will be held through Slack. See Studium for details on how to log in.

### Location

The course will have 1-2 guest lectures that the guest lecturers will give through Zoom. Otherwise, the course will be held on campus. You can find information and support for students on Zoom [here](https://mp.uu.se/c/perm/link?p=267521030). 

## Teachers

Main Teacher: Måns Magnusson

Teaching assistant: Andreas Östling

## Course structure

### Main part
The course consists of 8 blocks (weeks) of material. Each week consists of the following (expected workload in parenthesis):
- Two combined lectures/computer labs (approx. 2-4h per week)
- Online video material and reading assignments (approx. 2-6 h per week)
- An individual computer assignment (approx. 10-16 h per week)

### Computer assignments
Each week an individual computer assignment is done with a focus on implementing the main part of the material. Each assignment is completed individually and should follow the computer assignment template.

Students should return the computer assignments no later than **Sunday 23.59 each week**. A second possibility to turn in assignments is possible at the end of the course. For a detailed list of deadlines, see the [rough course plan](https://docs.google.com/spreadsheets/d/1HC_QN2mCq9bkCPzmkP8RaR3RokFQCWo9oPuU7rFyR8Y/edit?usp=sharing).

### Machine Learning, AI and ethics
A guest lecture will be given on AI and ethics by [Karim Jebari](https://www.iffs.se/en/research/researchers/karim-jebari/).

## Course Project
The last two weeks will focus on a course project where 2-3 students choose their data and create a supervised machine learning predictor for a real-world dataset. 

You can find details and instructions on the project work [here](https://github.com/MansMeg/IntroML/blob/master/project/).


## Frequently Asked Questions (FAQ)

Frequently asked questions will be collected [here](https://github.com/MansMeg/IntroML/blob/master/FAQ.md).

