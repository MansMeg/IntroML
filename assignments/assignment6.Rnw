\documentclass[11pt,a4paper,english]{article}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{listings}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Normal latex T1-fonts
\usepackage[T1]{fontenc}

% Figures
%\usepackage{graphicx,fancybox,subfigure}

 % AMS-stuff
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}

 % Misc
\usepackage{verbatim}
\usepackage{url}

\usepackage[round]{natbib}
\bibliographystyle{unsrtnat}


% Page size
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2cm}

% Paragraph
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% Horizontal line
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% No page numbers
\pagestyle{empty}

% New commands:
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\begin{document}

\begin{titlepage}

\center
\textsc{\LARGE Uppsala University}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.2]{files/uu_logo.png}\\[1cm]
\textsc{\Large Machine Learning}\\[0.5cm]
\textsc{\large }\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Assignment 6}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\vfill

\end{titlepage}


\HRule

\input{general_info.tex}

% General Knitr options
% (this cannot be input since the file runs knitr before LaTeX)
<<echo=FALSE,eval=TRUE>>=
options(continue="  ", prompt="> ")
knitr::opts_chunk$set(size = "small")
@

\HRule

\newpage

\tableofcontents

\newpage

\section*{Introduction to Unsupervised Learning}

This computer assignment will look into two to three widespread algorithms for unsupervised learning.

The dataset \texttt{iris} contain data on petal and sepal length and width for three different species of iris flowers. In addition, we will also use the \texttt{faithful} data on the Old Faithful Geyser in Yellowstone. The data consists of the eruption time (in minutes) and the waiting time (in minutes) until the next eruption. Finally we will also use the \texttt{mixture\_data} from Table 8.1 in  \citet{hastie2009elements}.

To access the data, just use:

<<echo=TRUE, eval=TRUE>>=
data("iris")
data("faithful")
library(uuml)
data("mixture_data")
@

For a more indepth description of the datasets, just run \texttt{?iris}, \texttt{?faithful}, or \texttt{?mixture\_data} in R.

%\section{General Questions}

%\input{general_questions_info.tex}

%\begin{enumerate}
%\item As a first step, describe, with your own words, what unsupervised learning is, and give relevant references to the course literature. (max 1 paragraph)
%\item What is the difference in the meaning of data augmentation in the EM and CNN senses?
%\item Describe the two steps of the EM algorithm. (max 1 paragraph)
%\item Describe the difference between a Gaussian mixture model and the k-means clustering model. (max 1 paragraph)
%Describe the gap statistic. What is the main idea?
%What is a principal component?
%\item What is the difference between Principal Component Analysis (PCA) and probabilistic PCA? (max 1 paragraph)
%\end{enumerate}

\section{The k-means Algorithm}

We are now going to build a k-means algorithm from scratch.

\begin{enumerate}
\item In the \texttt{iris} data we will use the \texttt{Speices} variable as one way to cluster the data. Visualize the \texttt{iris} as a scatterplot based on \texttt{Petal.length} and \texttt{Petal.width}. Show the different species using different colors. \emph{Hint!} Use \texttt{ggplot2} and \href{https://ggplot2.tidyverse.org/reference/geom_point.html}{geom\_point}.
\item Visualize (for yourself) the \texttt{faithful} data and manually allocate each data point to two different clusters of your choosing based on the \texttt{eruptions} and/or \texttt{waiting} variables.
\item Now, visualize the \texttt{faithful} dataset as a scatterplot by \texttt{eruptions} and \texttt{waiting}. Show your manually defined clusters in the Figure using different colors.
\item Now, we are going to implement the different parts of the k-means algorithm. This will follow Algorithm 14.1 in \citet{hastie2009elements}. First, implement Step 1 in this algorithm as a function called \texttt{compute\_cluster\_means(X, C)} that takes a $n \times p$ matrix \texttt{X} and a $p \times 1$ vector of cluster assignments called \texttt{C}, where $n$ is the number of rows (observations) and $p$ is the number of variables (columns). The algorithm should output the cluster means of the algorithm as a $K \times p$ matrix, where $K$ is the total number of clusters in \texttt{C}.
<<eval = FALSE>>=
set.seed(4711)
X <- as.matrix(faithful)
C <- sample(1:3, nrow(X), replace = TRUE)
m <- compute_cluster_means(X, C)
m

##      C eruptions  waiting
## [1,] 1  3.416354 70.57576
## [2,] 2  3.571106 71.44706
## [3,] 3  3.487659 70.72727
@
\item Next, implement the second step in Algorithm 14.1 of \citet{hastie2009elements} and call the function \texttt{compute\_cluster\_encoding(X, m)}. The function should take a $n\times p$ (design) matrix \texttt{X} and a $K\times p$ matrix \texttt{m} with one cluster mean per row. The function should return a $n \times 1$ vector of cluster assignments.
<<eval=FALSE>>=
C <- compute_cluster_encoding(X, m)
C[1:10]
## [1] 2 1 2 1 2 1 2 2 1 2
@
\item Now, use \texttt{compute\_cluster\_means(X, C)} and \texttt{compute\_cluster\_encoding(X, m)} to implement Algorithm 14.1 in \citet{hastie2009elements} as \texttt{k\_means(X,K)}. The \texttt{k\_means(X,K)} should take an $n\times p$ matrix \texttt{X} and the number of clusters $K$. The function should return a $n \times 1$ vector of cluster assignments \texttt{C} and a $K \times p$ mean matrix \texttt{m} after the algorithm has run until no cluster assignments change. Randomly initialize the cluster assignments as a part of the algorithm.
\item Implement a function called \texttt{k\_means\_W(X,C)} that computes the k-means within-point scatter (WPS) of Eq. 14.31 in \citet{hastie2009elements}.
<<eval = FALSE>>=
set.seed(4711)
X <- as.matrix(faithful)
C <- sample(1:3, nrow(X), replace = TRUE)
k_means_W(X, C)
## [1] 4601439
@
\item Now run your \texttt{k\_means(X,K)} a couple of times for both \texttt{faithful} with $K=2$ and \texttt{iris} with $K=3$ using the two variables used above. Use different seeds every time. Compute the WPS for the final cluster assignments. Below is an example how this could be done.
<<eval = FALSE>>=
set.seed(4711)
X <- as.matrix(faithful)
result1 <- k_means(X, K = 2)
wps1 <- k_means_W(X, result1$C)
result2 <- k_means(X, K = 2)
wps2 <- k_means_W(X, result2$C)
@
\item Visualize the clustering for the worst and the best clustering according to the WPS.

\emph{Note!} There might be multiple runs with the same (best) WPS, so you can make your own initizialization of the cluster assignments to force the algorithm to end up in a bad result with respect to WPS.
\item Describe and discuss the results you get.
\end{enumerate}

\newpage

\section{Probabilistic PCA}

As a last part of the assignment, we will look into the probabilistic Principal Component analysis model. Here, Ch. 13 - 13.1 in \citet{Goodfellow-et-al-2016} is helpful.

\begin{enumerate}
\item Implement a function \texttt{pPCA(W, b, sigma2)} that simulate data $\mathbf{x} \in \mathbb{R}^{D}$ from the probabilistic PCA model with $\mathbf{h} \sim N(0,I_K)$ where $\mathbf{h} \in \mathbb{R}^{K \times 1}$, $\mathbf{W} \in \mathbb{R}^{D \times K}$, $\mathbf{b} \in \mathbb{R}^{D \times 1}$ and $\sigma^2 = 1$.
\item Simulate 300 observation PCA model with $\mathbf{W}=(-1, 3)^T$, $\mathbf{b} = (0.5, 2)^T$, and $\sigma^2 = 1$. The resulting distribution should look something similar as in Figure \ref{pPCA}.
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{figs/pPCA.png}
\caption{Data from a pPCA model with $\mathbf{W} = (-1, 3)^T$, $\mathbf{b} = (0.5, 2)^T$ and $\sigma^2 = 1$}
\label{pPCA}
\end{figure}
\item Describe the different parts of the model. What are the parameters? What are the latent variables?
\item Try some other parameter values for $W$ and $b$ and visualize the result in a similar scatter plot as in Fig. \ref{pPCA}.
\item Now run principal component analysis on your simulated data (with $\mathbf{W} = (-1, 3)^T$, $\mathbf{b} = (0.5, 2)^T$) using
<<eval = FALSE>>=
pr <- prcomp(X, center = TRUE, scale. = FALSE)
@
Multiply the first \texttt{stdev} with the first principal component (\texttt{rotation}).
\item What parameters in your pPCA model do these results correspond to. \emph{Note!} You might have to multiply your result/vector with -1.
\item Finally, simulate values $\mathbf{x}$ of dimension five from a probabilistic PCA model for a given choice of parameters. The latent variables $\mathbf{h}$ should be of dimension two, i.e. $\mathbf{h} \in \mathbb{R}^{2 \times 1}$ for each simulated observation. You are free to choose the parameter values $\mathbf{W}$, $\mathbf{b}$ and $\sigma^2$ yourself, but the resulting distribution should have at least one negative and one positive correlation between the $X$s. Visualize the variables using a pair-wise scatterplot using \texttt{pairs(X)} in R.
\end{enumerate}

\newpage

\section{* The EM-algorithm for soft clustering: Univariate Two-Component Normal Mixture}
\label{uni_two_comp}

\input{distinction_task.tex}

In the next step, we will implement the EM algorithm conceptually similar to k-means clustering. We are going to implement the EM by testing at the mixture data of \citet{hastie2009elements}. This dataset has been added to the course R package.

<<>>=
library(uuml)
data("mixture_data")
theta0 <- list(mu_1 = 4.12, mu_2 = 0.94, sigma_1 = 2, sigma_2 = 2, pi = 0.5)
@


\begin{enumerate}
\item Simulate data from a univariate mixture model by implementing a mixture model as in Eq. 8.36 in \citet{hastie2009elements}. That is, create a hiearchical simulation that first sample $\delta$ for each observation, and then sample $Y_i$ from the relevant component. The function should be called \texttt{r\_uni\_two\_comp(n, theta)} take have the arguments \texttt{n} (the number of observations to simulate) (the number of simulated observations) and \texttt{theta}, an R list with with values \texttt{mu1}, \texttt{mu2}, \texttt{sigma1}, \texttt{sigma2}, and \texttt{pi}.
\item Simulate 200 observations using $\mu_1=-2$, $\mu_2=1.5$, $\sigma_1=2$, $\sigma_2=1$ and $\pi=0.3$. Visualize observations drawn in a histogram.
\item Implement a function \texttt{d\_uni\_two\_comp(x, theta)} that computes the density value for a given set of parameters \texttt{theta} for values of \texttt{x}. \emph{Hint!} The function \texttt{dnorm} might be handy.
\item Visualize the density for the mixture model above over the interval -6 to 6 (by steps of 0.01).
\item Visualize the \texttt{eruptions} variable in the \texttt{faithful} dataset using a histogram.
\item Manually choose values for $\mu_1$, $\mu_2$, $\sigma_1$, $\sigma_2$ and $\pi$ that gives a density you think is a good fit for the \texttt{eruptions} variable.
\item Implement a function called \texttt{e\_uni\_two\_comp(X, theta)} that returns a  vector of gamma values for each row in \texttt{X} (for component 2).  This function should implement the expectation step in Algorithm 8.1 in \citet{hastie2009elements}. For definition of $\phi$ and $\theta$, see \citet[][, Section 8.5.1]{hastie2009elements}. Subscript in Algorithm 8.1, e.g. in $\theta_2$, indicates the component/cluster.
<<eval = FALSE>>=
gamma <- e_uni_two_comp(mixture_data, theta0)
head(gamma)
## [1]  0.9106339 0.8716861 0.7797225 0.6645640 0.6484311 0.5178799
@
\emph{Note!} Gamma values for component 1 is just 1 - \texttt{gamma}.
\item How can we interpret $\gamma$ here?
\item Implement a function called \texttt{max\_uni\_two\_comp(X, gamma)} that returns a list with parameters \texttt{mu1}, \texttt{mu2}, \texttt{sigma1}, \texttt{sigma2} and \texttt{pi}. This function should implement the maximization step in algorithm 8.1 in \citet{hastie2009elements}.
<<eval = FALSE>>=
theta <- max_uni_two_comp(mixture_data, gamma)
theta
## $mu_1
## [1] 3.842941
##
## $mu_2
## [1] 1.450413
##
## $sigma_1
## [1] 1.700666
##
## $sigma_2
## [1] 1.47168
##
## $pi
## [1] 0.4883709
##
@
\item Implement the log-likelihood of the model as \texttt{ll\_uni\_two\_comp(x, theta)}. \emph{Hint!} See Eq. 8.39 in \citet{hastie2009elements}.
<<eval = FALSE>>=
ll_uni_two_comp(mixture_data, theta0)
## -43.1055
@
\item Now combine the implemented functions to an EM algorithm \texttt{em\_uni\_two\_comp(X, theta\_0, iter)} that takes in a $n \times p$ data matrix \texttt{X} and an initialization value for as \texttt{theta\_0}. Then the algorithm should be run \texttt{iter} number of iterations. For each iteration print out the log-likelihood value for that current iteration and store the value of $\theta$ for each iteration.
<<eval = FALSE>>=
theta_and_gamma_em3 <- em_uni_two_comp(mixture_data, theta0, iter = 3)
## Log Lik:  -41.53247
## Log Lik:  -41.11211
## Log Lik:  -40.48348
@
\item Test your algorithm on the \texttt{mixture\_data} for 20 iterations. Do you get the same results for $\hat{\pi}$ as in \citet[p. 275]{hastie2009elements}? Note that you might get slightly different results (at the second decimal place). Also, compare your log-likelihoods with Figure 8.6 in \citet{hastie2009elements}. Do you have a similar result? \emph{Hint!} There can be a slight different order depending on if you compute the log-likelihood value before or after the update of $\theta$.
<<eval = FALSE>>=
theta_and_gamma_em20 <- em_uni_two_comp(mixture_data, theta0, iter = 20)
## Log Lik:  -41.53247
## ...
@
\item Run your EM algorithm on the \texttt{eruptions} variable of the \texttt{faithful} data and on the \texttt{Petal.Length} variable of the \texttt{iris} data until you judge it has converged. What are the estimated parameters for the two datasets and how do you assess convergence?
\item Visualize the density for the two datasets in two different figures using the parameters estimated with your EM algorithm. Also show histograms for the real data in the same Figures (i.e. so you show the data and the estimated density). \emph{Hint!} You should be able to use \texttt{d\_uni\_two\_comp(x, theta)}.
\end{enumerate}



% BELOW ARE POTENTIAL ADDITIONAL ASSIGNMENTS

%\subsubsection{Univariare Three Component Normal Mixture}
%
%Now we are going to expand our model by adding an additional a mixture component. In this (and the next) section, \citet{smyth2020em} can be good to read.
%
%\begin{enumerate}
%\item Derive the expectation of the responsibilities for a three component mixture model. Use the same notation as in Algorithm 8.1 in \citet{hastie2009elements}.
%\item Derive the maximization the paramaters $\mu_1,..,\mu_3$, $\sigma_1,..,\sigma_3$, and $\pi_1,..,\pi_3$ for a three component mixture model. Use the same notation as in Algorithm 8.1 in \citet{hastie2009elements}.
%\item Just as in Section \ref{uni_two_comp} implement an EM algorithm by implementing the follow functions in R:
%\begin{enumerate}
%\item \texttt{d\_uni\_three\_comp(x, theta)}
%\item \texttt{ll\_uni\_three\_comp(x, theta)}
%\item \texttt{e\_uni\_three\_comp(X, theta)}
%\item \texttt{max\_uni\_three\_comp(X, gamma)}
%\item \texttt{em\_uni\_three\_comp(X, theta\_0, iter)}
%\end{enumerate}
%\item  Run your EM algorithm for the three component model on the \texttt{eruptions} variable of the \texttt{faithful} data and on the \texttt{Petal.Length} variable of the \texttt{iris} data. What are the estimated parameters for the two datasets?
%\item Visualize the density for the two datasets using the parameters estimated with your EM algorithm. \emph{Hint!} You should be able to use \texttt{d\_uni\_three\_comp(x, theta)}.
%\end{enumerate}

%\subsubsection{Bivariate Normal Mixtures}

%We have now tested to change the number of mixture components, as the last step we will also change the density of each component ($\phi$ in \citet{hastie2009elements} notation). Here we will do that change by going from a univariate normal with parameter $\mu_i$ and $\sigma_i$ to a bivariate normal with the $\mathbf{\mu}_i$ and $\Sigma_i$. In this Section, \citet{smyth2020em} can be good to read.


%\begin{enumerate}
%\item Derive the expectation of the responsibilities for both a two- and three component mixture model (you can give a general notation if you want). Use the same notation as in Algorithm 8.1 in \citet{hastie2009elements}.
%\item Derive the maximization the paramaters $\mathbf{\mu}_1,..,\mathbf{\mu}_K$, $\Sigma_1,..,\Sigma_K$, and $\pi_1,..,\pi_K$ for a two- and three component mixture model. Use the same notation as in Algorithm 8.1 in \citet{hastie2009elements}.
%\item Just as in Section \ref{uni_two_comp} implement an EM algorithm by implementing the follow functions in R. You can either implement them as one function handling both two and three components, or as one function that can handle both situations.
%\begin{enumerate}
%\item \texttt{d\_bi\_K\_comp(x, theta)}
%\item \texttt{ll\_bi\_K\_comp(x, theta)}
%\item \texttt{e\_bi\_K\_comp(X, theta)}
%\item \texttt{max\_bi\_K\_comp(X, gamma)}
%\item \texttt{em\_bi\_K\_comp(X, theta\_0, iter)}
%\end{enumerate}
%\item  Run your EM algorithm with two components on the \texttt{eruptions} and \texttt{waiting} variables of the \texttt{faithful} data and with three components on the \texttt{Petal.Length} and \texttt{Petal.width} variables of the \texttt{iris} data. What are the estimated parameters for the two datasets?
%\item Visualize the density for the two datasets using the parameters estimated with your EM algorithm. \emph{Hint!} You should be able to use  \texttt{d\_bi\_K\_comp(x, theta)}. Also \texttt{geom\_density\_2d} in \texttt{ggplot2} that can be found \href{https://www.r-graph-gallery.com/2d-density-plot-with-ggplot2.html}{here}, is a good way to visualize two-dimensional densities.
%\item Finally, compare the results with the results in the k-means clustering above. What are the differences? Which model would you prefer and why?
%\end{enumerate}



\newpage
\bibliography{bibliography}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
