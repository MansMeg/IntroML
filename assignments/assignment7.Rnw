% !Rnw weave = knitr

\documentclass[11pt,a4paper,english]{article}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{listings}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Normal latex T1-fonts
\usepackage[T1]{fontenc}

% Figures
%\usepackage{graphicx,fancybox,subfigure}

 % AMS-stuff
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}

 % Misc
\usepackage{verbatim}
\usepackage{url}

\usepackage[round]{natbib}
\bibliographystyle{unsrtnat}


% Page size
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2cm}

% Paragraph
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% Horizontal line
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% No page numbers
\pagestyle{empty}

% New commands:
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}



\begin{document}

\begin{titlepage}

\center
\textsc{\LARGE Uppsala University}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.2]{files/uu_logo.png}\\[1cm]
\textsc{\Large Machine Learning}\\[0.5cm]
\textsc{\large }\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Assignment 7}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\vfill

\end{titlepage}


\HRule

% General Knitr options
% (this cannot be input since the file runs knitr before LaTeX)
<<echo=FALSE,eval=TRUE>>=
#options(continue="  ", prompt="> ")
knitr::opts_chunk$set(size = "small")
@

\input{general_info.tex}

\HRule

\newpage

\tableofcontents

\newpage

% \section{General Questions}

% \input{general_questions_info.tex}
% \begin{enumerate}
% \item What is the relationship between PCA and linear autoencoders? (max 1 paragraph)
%\item Describe the encoder and decoder of a VAE.
%\item When we maximize the ELBO, what two things do we optimize in a VAE? (max 1 paragraph)
%\item What is a topic in a latent Dirichlet topic model? (max 1 paragraph)
%\item How does Blei (2012) describe the relationship between LDA and PCA? (max 1 paragraph)
%\item What is a mixed membership model? (max 1 paragraph)
% \end{enumerate}


\section{Variational Autoencoders}

We are now going to implement a variational autoencoder in R using Tensorflow. You can find a lot of the code needed for this assignment \href{https://github.com/rstudio/keras/blob/main/vignettes/examples/variational_autoencoder.R}{\textbf{here}}.

\emph{Note!} Running Keras can be computationally heavy, and I suggest not to run the code in \texttt{markdown}. Instead, run the code in R and copy the results as output (see the assignment template for an example).

\begin{enumerate}
\item (1p) Start by loading the MNIST data in R. See the link above or previous assignments for details.
\item (1p) Now, implement a one-layer (encoder and decoder) feed-forward variational autoencoder with two latent dimensions. Both the encoder layer and the decoder layer should have 200 hidden units. You should end up with a variational autoencoder with roughly 310 000 - 320 000 parameters.
\item Print the model and include it in your report.
\begin{enumerate}
\item (1p) How many weights (parameters) are used to compute $\mu$ and $\sigma^2$ for the latent variables?
\item (1p) What layer represent the latent variables?
\item (1p) What does the lambda layer do in the model?
\end{enumerate}
\item Now train your variational autoencoder on the MNIST data for 50 epochs. Visualize the latent state for the different numbers.
\begin{enumerate}
\item (1p) How do you interpret this latent state?
\item (1p) What numbers are better represented by the latent state?
\item (1p) What number is less well represented by the latent state?
\end{enumerate}
\item (2p) Finally, encode all the 2:s in the MNIST test dataset to the latent state using your encoder. What is the mean of the digits "2" in the two latent dimensions? \emph{Hint!} See \texttt{y\_test} for the MNIST numbers. \emph{Note!} Do not retrain your VAE. Just use the one you have already trained.
\item (1p) Visualize this value of the latent state as a 28 by 28-pixel image using your decoder.
\end{enumerate}


\section{Topic Models}

We will now analyze the classical book Pride and Prejudice by Jane Austen using a probabilistic topic model. If you have not read the book, \href{https://en.wikipedia.org/wiki/Pride_and_Prejudice}{\textbf{here}} you can read up on the story of this classical book.

For this part of the assignment, \citet{griffiths2004finding} is the primary reference. I would also recommend reading \citet{blei2012probabilistic} before starting with this part of the assignment.

We will use a Gibbs sampler to estimate ten different topics occurring in Pride and Prejudice and study where they occur. A tokenized version of the book and a \texttt{data.frame} with stopwords can be loaded as follows:

<<>>=
library(uuml)
library(dplyr)
library(tidytext)
data("pride_and_prejudice")
data("stopwords")
@


\begin{enumerate}
\item (1p) As a first step, we will remove stopwords (common English words without much semantic information):
<<>>=
pap <- pride_and_prejudice
pap <- anti_join(pap, y = stopwords[stopwords$lexicon == "snowball",])
@

\item (1p) Then we will remove rare words. Here we remove words that occur less than five times.
<<>>=
word_freq <- table(pap$word)
rare_words <- data.frame(word = names(word_freq[word_freq <= 5]), stringsAsFactors = FALSE)
pap <- anti_join(pap, y = rare_words)
@
\item (1p) Now we have a corpus we can used to implement a probabilistic topic model. We do this by using the \texttt{topicmodels} R package. As a first step we will compute a document term matrix using the \texttt{tm} package, where we treat each paragraph as a document. How many documents and terms (word types) do you have?
<<eval=FALSE>>=
library(tm)
crp <- aggregate(pap$word, by = list(pap$paragraph), FUN = paste0, collapse = " ")
names(crp) <- c("paragraph", "text")
s <- SimpleCorpus(VectorSource(crp$text))
m <- DocumentTermMatrix(s)
@
\item (1p) To compute a topic model with ten topics, we use a Gibbs sampling algorithm. Below is an example of how we can run a Gibbs sampler for 2000 iterations. Run your topic model for 2000 iterations.
<<eval=FALSE>>=
library(topicmodels)
K <- 10
# Note: delta is beta in Griffith and Steyvers (2004) notation.
control <- list(keep = 1, delta = 0.1, alpha = 1, iter = 2000)
tm <- LDA(m, k = K, method = "Gibbs", control)
@
\item (1p) In the \texttt{uuml} R package you have three convenience functions to extract $\Theta$, $\Phi$ and the log-likelihood values at each iteration. This is the parameter notation used in \citet{griffiths2004finding}.
<<eval=FALSE>>=
library(uuml)
lls <- extract_log_liks(tm)
theta <- extract_theta(tm)
phi <- extract_phi(tm)
@
\item (2p) As a first step, check that the model has converged by visualizing the log-likelihood over epochs/iterations. Does it seem like the model have converged?
\item (2p) Extract the 20 top words for each topic (i.e. the words with the highest probability in each topic). Choose two topics you find coherent/best (the top words seem to belong together). Interpret these two topics based on the storyline of the book. What have these two topics captured?
\item (2p) Visualize these two topics evolve over the paragraphs in the books by plotting the $\theta$ parameters for that topic over time (paragraphs) in the book. Think of this as the time-line of the book. On the y-axis, you should plot $\theta_i$ for your chosen topic $i$ and the x-axis should be the paragraph number (first paragraph has number 1 and so forth).
\item (2p) How do these two chosen topics evolve over the course in the book? If you want, you can take a rolling mean of the theta parameters to more easily show the changes in the topic over the book. \emph{Hint!} Here \texttt{zoo::rollmean()} might be a good function to use.
\end{enumerate}

\newpage

\section{* Variational Autoencoders using Convolutional Neural Networks}

\input{distinction_task.tex}

You can choose to either do this task or the task below to get VG on this assignment.

% https://learnopencv.com/variational-autoencoder-in-tensorflow/
As we have seen previously, for images, we can get better performance using Convolutional Neural Networks. Hence we are going to repeat the exercise above using a convolutional neural network as encoder and decoder. You can find detailed code \href{https://github.com/rstudio/keras/blob/main/vignettes/examples/variational_autoencoder_deconv.R}{\textbf{here}}.

\begin{enumerate}
\item (3p) Now implement a four-layer (encoder and decoder) convolutional neural network with two latent dimensions. There should be 50 filters in each convolutional layer. \emph{Note!} A dense layer should be included as the last step in the encoder and the first step in the decoder. These layers should have 100 hidden units. You should end up with a variational autoencoder with roughly 2M parameters.
\item (1p) Print the model and include it in your report. How many weights (parameters) are used to compute $\mu$ and $\sigma^2$ for the latent variables? What layer represent the latent variables?
\item (1p) Now train your CNN variational autoencoder on the MNIST data for five epochs. Visualize the latent state for the different numbers. How do you interpret this result? Compare these results with the results from the feed-forward autoencoder.
\item (1p) Finally, encode all the 2:s in the MNIST test dataset to the latent state using your encoder (Hint!, see \texttt{y\_test} for numbers). What is the mean of the digits "2" in the two latent dimensions?
\item (1p) Visualize the mean value of the digit 2 of the latent state as a 28 by 28-pixel image using your decoder.
\end{enumerate}

\newpage

\section{* Prompt engineering with ChatGPT}

You can choose to either do this task or the task above to get VG on this assignment.

\input{distinction_task.tex}

For this assignment, you need to have a user account at Open AI to generate an API token. See \href{https://help.openai.com/en/articles/4936850-where-do-i-find-my-api-key}{[this link]} for details on how to get an API token to use.

You also need to buy credits at Open AI. I think 1-2 dollars should be sufficient for this assignment.

As a second step you need to install the \texttt{ropenaiapi} R package. This can be done as follows:
<<eval=FALSE>>=
library(remotes)
remotes::install_github('MansMeg/ropenaiapi')
@
In the next step we set the API key as an environment variable with:
<<eval=FALSE>>=
library(ropenaiapi)
set_openai_api_key("[YOUR KEY GOES HERE]")
@

We can now call the API with the \texttt{openai\_chat} function.

<<eval=FALSE>>=
x <- openai_chat("Who is Gustav Vasa? Give a short answer.",
                 model = "gpt-3.5-turbo")
x
@

<<echo=FALSE,eval=TRUE>>=
cat("assistant:
Gustav Vasa was a Swedish king who led a successful rebellion
against Danish rule and became the founder of modern Sweden in
the 16th century.")
@

See the \texttt{ropenaiapi} documentation with ```?openai\_chat``` in R for further information on how to interact with the Open AI API.

\emph{Note!} If you get ```Error: You exceeded your current quota, please check your plan and billing details.``` that means that you don't have any credits to use for the calls. You then need to buy some credits from Open AI, 1-2 dollars should be sufficient for this task.


\subsection{Make ChatGPT hallucinate}

We are now going to understand how and when transformer-based decoder models 'hallucinate', i.e. when the models give factual incorrect information. See \cite[][, Section 7.1.2]{zhao2023survey} for an introduction and \cite{huang2023survey} for details on hallucinations in LLMs.

Now, create 3 different hallucinations with three different strategies (i.e. its not ok to use the same prompt for three incorrect generations) from the chatGPT models (you can choose which one you like). For each generated hallucination return:

\begin{enumerate}
\item The prompt and a the \texttt{seed} supplied to the API. You are free to both create hallucinations through a longer conversation and tuning parameters.
\item The response from the LLM.
\item Why the response is factually incorrect and weather it is an intrinsic or extrinsic hallucination.
\item Describe the strategy you use and the analysis why the hallucination happened.
\end{enumerate}

As a final step, when you have succeded to get the LLM to hallucinate, change the model to GPT-4 (include exact which model you use) and run the same prompt that you could get to hallucinate.

\begin{enumerate}
\item Return the response from GPT4 for each prompt.
\item Is the response still factually incorrect? If not, see if you can get at least one hallucination for GPT-4. If you don't succeed, just state that.
\end{enumerate}



\subsection{In-Context Learning for few-shot classification}

In-context learning is the idea to use large language models to complete tasks in a few-shot or zero-shot way. In this task we are going to try to classify political quotes in Swedish to weather they belong to the social democratic party manifesto or the conservative party (Moderaterna) manifesto.

\emph{Note!} This data is included in \texttt{uuml} version 0.4.0 and later.

<<eval=TRUE>>=
library(uuml)
data("pc_test")
data("pc_train")
@

Below is an example on how you can combine the data to a text string that can be combined to be used in a prompt. Note, you need to develop further on this.

<<eval=FALSE>>=
example_string <- paste0(pc_train$quote,
                         ": ",
                         pc_train$party, collapse = "\n\n")
cat(example_string)
@

See \cite[][, Section 8]{zhao2023survey} for details on how to best engineer prompts to solve the classification task.

Now use the techniques \cite[][, Section 8]{zhao2023survey} to generate as good prompts as possible to classify weather a quote is from the social democratic or the conservative manifesto.


\begin{enumerate}
\item Start with zero-shot learning, ie only prompt the LLM without any demonstrations. Test to classify the quotes in the test set.
\item Test how far you can get by using better prompt descriptions to classify the quotes. Try to get responses as classes from the model.
\item Now add demonstrations by adding examples from the training set. Does that improve the accuracy on the testset?
\item Try at least three different strategies from \cite[][, Section 8]{zhao2023survey} to improve the quality. Discuss your conclusions.
\end{enumerate}

\newpage

\bibliography{bibliography}


\end{document}



