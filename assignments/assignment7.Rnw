\documentclass[11pt,a4paper,english]{article}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{listings}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Normal latex T1-fonts
\usepackage[T1]{fontenc}

% Figures
%\usepackage{graphicx,fancybox,subfigure}

 % AMS-stuff
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}

 % Misc
\usepackage{verbatim}
\usepackage{url}

\usepackage[round]{natbib}
\bibliographystyle{unsrtnat}


% Page size
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2cm}

% Paragraph
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% Horizontal line
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% No page numbers
\pagestyle{empty}

% New commands:
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\begin{document}

\begin{titlepage}

\center
\textsc{\LARGE Uppsala University}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.2]{files/uu_logo.png}\\[1cm]
\textsc{\Large Introduction to Machine Learning, Big Data, and AI}\\[0.5cm]
\textsc{\large }\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Assignment 7}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\vfill

\end{titlepage}


\HRule

\input{general_info.tex}

% General Knitr options
% (this cannot be input since the file runs knitr before LaTeX)
<<echo=FALSE,eval=TRUE>>=
options(continue="  ", prompt="> ")
knitr::opts_chunk$set(size = "small")
@

\HRule

\newpage

\section{Introduction to Unsupervised Learning}

In this computer assignment we will look into three very common algorithms for unsupervised learning and will study the algorithms on three different datasets.

The dataset \texttt{iris} contain data on petal and sepal length and width for three different speices of iris flowers. In addition we will also use the \texttt{faithful} data on the Old Faithful Geyser in Yellowstone. The data consists of the eruption time (in minutes) and the waiting time (in minutes) until the next eruption. Finally we will also use the \texttt{mixture\_data} from Table 8.1 in  \citet{hastie2009elements}.

To access the data, just use:

<<echo=TRUE, eval=TRUE>>=
data("iris")
data("faithful")
library(uuml)
data("mixture_data")
@

For a more indepth description of the datasets, just run \texttt{?iris}, \texttt{?faithful}, or \texttt{?mixture\_data} in R.

\section{General questions}

\begin{enumerate}
\item As a first step, describe, with your own words, what unsupervised learning is, and give relevant references to the course literature. (max 1 paragraph)
%\item What is the difference in meaning of data augmentation in the EM sense and in the CNN sense?
\item Describe the two steps of the EM algorithm. (max 1 paragraph)
\item Describe the difference between a Gaussian mixture model and the k-means clustering model. (max 1 paragraph)
%Describe the gap statistic, what is the main idea?
%What is a principal component?
\item What is the difference betwen Principal Component Analysis (PCA) and probabilistic PCA? (max 1 paragraph)
\end{enumerate}

\section{The k-means Algorithm}

We are now going to build a k-means algorithm from scratch.

\begin{enumerate}
\item In the \texttt{iris} data we will use the \texttt{Speices} variable as one way to cluster the data. Visualize (for yourself) the \texttt{faithful} data and manually allocate each data point to two different clusters of your choosing.
\item Visualize the \texttt{iris} as a scatterplot based on \texttt{Petal.length} and \texttt{Petal.width}. Show the different species using different colors. \emph{Hint!} Use \texttt{ggplot2} and \href{https://ggplot2.tidyverse.org/reference/geom_point.html}{geom\_point}.
\item Similarily, visualize the \texttt{faithful} dataset as a scatterplot by \texttt{eruptions} and \texttt{waiting}. Show your manually assigned clusters in the Figure using different colors.
\item Now, we are going to implement the different parts of the k-means algorithm. This will follow Algorithm 14.1 in \citet{hastie2009elements}. First, implement Step 1 in this algorithm as a function called \texttt{compute\_cluster\_means(X, C)} that takes a $n \times p$ matrix \texttt{X} and a $p \times 1$ vector of cluster assignments called \texttt{C}, where $n$ is the number of rows (observations) and $p$ is the number of variables (columns). The algorithm should output the cluster means of the algorithm as a $K \times p$ matrix, where $K$ is the total number of clusters in \texttt{C}.
<<eval = FALSE>>=
set.seed(4711)
X <- as.matrix(faithful)
C <- sample(1:3, nrow(X), replace = TRUE)
m <- compute_cluster_means(X, C)
m

##      C eruptions  waiting
## [1,] 1  3.416354 70.57576
## [2,] 2  3.571106 71.44706
## [3,] 3  3.487659 70.72727
@
\item Next, implement the second step in Algorithm 14.1 of \citet{hastie2009elements} and call the function \texttt{compute\_cluster\_encoding(X, m)}. The function should take a $n\times p$ (design) matrix \texttt{X} and a $K\times p$ matrix \texttt{m} with one cluster mean per row. The function should return a $n \times 1$ vector of cluster assignments.
<<eval=FALSE>>=
C <- compute_cluster_encoding(X, m)
C[1:10]
## [1] 2 1 2 1 2 1 2 2 1 2
@
\item Now, use \texttt{compute\_cluster\_means(X, C)} and \texttt{compute\_cluster\_encoding(X, m)} to implement Algorithm 14.1 in \citet{hastie2009elements} as \texttt{k\_means(X,K)}. The \texttt{k\_means(X,K)} should take an $n\times p$ matrix \texttt{X} and the number of clusters $K$. The function should return a $p \times 1$ vector of cluster assignments after the algorithm has run until no cluster assignments change. Use random initialization of cluster means.
\item Implement a function called \texttt{k\_means\_W(X,C)} that computes the k-means within-point scatter of Eq. 14.31 in \citet{hastie2009elements}.
<<eval = FALSE>>=
set.seed(4711)
X <- as.matrix(faithful)
C <- sample(1:3, nrow(X), replace = TRUE)
k_means_W(X, C)
## [1] 4601439
@
\item Now run your k-means a couple of times for both \texttt{faithful} with $K=2$ and \texttt{iris} with $K=3$ using the two variables used above.
\item Visualize the clustering for the worst and the best clustering according to the within-point scatter (if there are any differences between the runs).
\item Describe and discuss the results you get.
\end{enumerate}

\section{Probabilistic PCA}

As a last part of the assignment, we will look into the probabilistic Principal Component analysis model. Here, Ch. 14 - 14.1 in Goodfellow et al. (2017) is useful.

\begin{enumerate}
\item Implement a function \texttt{pPCA(W, b, sigma2)} and that simulated data from a probabilistic PCA model with $h \sim N(0,I)$ and $\sigma^2 = 1$
\item Simulate 300 observation PCA model with $W=(-1, 3)$, $b = (0.5, 2)$, and $\sigma^2 = 1$. The resulting distribution should look something similar as in Figure \ref{pPCA}.
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{figs/pPCA.png}
\caption{Data from a pPCA model with $W = (-1, 3)^T$, $b = (0.5, 2)$ and $\sigma^2 = 1$}
\label{pPCA}
\end{figure}
\item Describe the different parts of the model. What are the parameters? What are the latent variables?
\item Try some other parameter values for $W$ and $b$ and visualize the result in a similar scatter plot as in Fig. \ref{pPCA}.
\item Now run principal component analysis on your simulated data (with $W = (-1, 3)^T$, $b = (0.5, 2)$) using
<<eval = FALSE>>=
pr <- prcomp(X, center = TRUE, scale. = FALSE)
@
Multiply the first \texttt{stdev} with the first principal component (\texttt{rotation}).
\item What parameters in your pPCA does this correspond to. \emph{Note!} You might have to multiply your result with -1.
\item Finally, simulate values $\mathbf{x}$ of dimension five from a probabilistic PCA model for a given choice of parameters. The latent variables $H$ should be of dimension two, i.e. $H \in \mathbb{R}^{300 \times 2}$. You are free to choose the parameter values $W$, $b$ and $\sigma$ yourself, but the resulting distribution should have one at least one negative and one positive correlation between the $X$s. Visualize the variables using a pair-wise scatterplot using \texttt{pairs(X)}.
\end{enumerate}

\section{* The EM-algorithm for soft clustering: Univariate Two-Component Normal Mixture}
\label{uni_two_comp}

This assignment is a VG point assignment. If you do not want to get a VG-point, you can ignore this assignment.

In the next step we will implement the EM algorithm, that is conceptually very similar to k-means clustering.We are going to implement the EM by testing at the mixture data of \citet{hastie2009elements}. This has been added to the course R package.

<<>>=
library(uuml)
data("mixture_data")
theta0 <- list(mu_1 = 4.12, mu_2 = 0.94, sigma_1 = 2, sigma_2 = 2, pi = 0.5)
y <- mixture_data
@


\begin{enumerate}
\item Simulate data from a univariate mixture model by implementing a mixture model as in Eq. 8.36 in \citet{hastie2009elements}. That is, create a hiearchical simulation that first sample $\delta$ for each observation, and then sample $Y_i$ from the relevant component. The function should be called \texttt{r\_uni\_two\_comp(n, theta)} take have the arguments \texttt{n} (the number of observations to simulate) (the number of simulated observations) and \texttt{theta}, an R list with with values \texttt{mu1}, \texttt{mu2}, \texttt{sigma1}, \texttt{sigma2}, and \texttt{pi}.
\item Simulate 200 observations using $\mu_1=-2$, $\mu_2=1.5$, $\sigma_1=2$, $\sigma_2=1$ and $\pi=0.3$. Visualize observations drawn in a histogram.
\item Implement a function \texttt{d\_uni\_two\_comp(x, theta)} that computes the density value for a given set of parameters \texttt{theta} for values of \texttt{x}. \emph{Hint!} The function \texttt{dnorm} might be handy.
\item Visualize the density for the mixture model above over the interval -4 to 4 (by steps of 0.01).
\item Visualize the \texttt{eruptions} variable in the \texttt{faithful} dataset using a histogram.
\item Manually choose values for $\mu_1$, $\mu_2$, $\sigma_1$, $\sigma_2$ and $\pi$ that gives a density you think is a good fit for the \texttt{eruptions} variable.
\item Implement a function called \texttt{e\_uni\_two\_comp(X, theta)} that returns a  vector of gamma values for each row in \texttt{X} (for component 2).  This function should implement the expectation step in algorithm 8.1 in \citet{hastie2009elements}.
<<eval = FALSE>>=
gamma <- e_uni_two_comp(y, theta0)
head(gamma)
## [1]  0.9106339 0.8716861 0.7797225 0.6645640 0.6484311 0.5178799
@
\emph{Note!} Gamma values for component 1 is essentially just 1 - \texttt{gamma}.
\item How can we interpret $\gamma$ here?
\item Implement a function called \texttt{max\_uni\_two\_comp(X, gamma)} that returns a list with parameters \texttt{mu1}, \texttt{mu2}, \texttt{sigma1}, \texttt{sigma2} and \texttt{pi}. This function should implement the maximization step in algorithm 8.1 in \citet{hastie2009elements}.
<<eval = FALSE>>=
theta <- max_uni_two_comp(y, gamma)
theta
## $mu_1
## [1] 3.842941
##
## $mu_2
## [1] 1.450413
##
## $sigma_1
## [1] 1.700666
##
## $sigma_2
## [1] 1.47168
##
## $pi
## [1] 0.4883709
##
@
\item Implement the log-likelihood of the model as \texttt{ll\_uni\_two\_comp(x, theta)}. \emph{Hint!} You can use large parts of tour \texttt{em\_uni\_two\_comp(x, theta)}
<<eval = FALSE>>=
ll_uni_two_comp(y, theta0)
## -43.1055
@
\item Now combine the implemented functions to an EM algorithm \texttt{em\_uni\_two\_comp(X, theta\_0, iter)} that takes in a $n \times p$ data matrix \texttt{X} and an initialization value for as \texttt{theta\_0}. Then the algorithm should be run \texttt{iter} number of iterations. For each iteration print out the log-likelihood value for that current iterations.
<<eval = FALSE>>=
theta_em <- em_uni_two_comp(y, theta0)
## Log Lik:  -41.53247
## Log Lik:  -41.11211
## Log Lik:  -40.48348
@
\item Test your algorithm on the \texttt{mixture\_example} for 20 iterations. Do you get the same results as in \citet[p. 275]{hastie2009elements}? Note that you might get slightly different results (at the second decimal place). Also compare your log likelihoods with Figure 8.6 in \citet{hastie2009elements}. Do you have a similar result?
\item Run your EM algorithm on the \texttt{eruptions} variable of the \texttt{faithful} data and on the \texttt{Petal.Length} variable of the \texttt{iris} data. What are the estimated parameters for the two datasets?
\item Visualize the density for the two datasets using the parameters estimated with your EM algorithm. \emph{Hint!} You should be able to use \texttt{d\_uni\_two\_comp(x, theta)}.
\end{enumerate}





% BELOW ARE POTENTIAL ADDITIONAL ASSIGNMENTS

%\subsubsection{Univariare Three Component Normal Mixture}
%
%Now we are going to expand our model by adding an additional a mixture component. In this (and the next) section, \citet{smyth2020em} can be good to read.
%
%\begin{enumerate}
%\item Derive the expectation of the responsibilities for a three component mixture model. Use the same notation as in Algorithm 8.1 in \citet{hastie2009elements}.
%\item Derive the maximization the paramaters $\mu_1,..,\mu_3$, $\sigma_1,..,\sigma_3$, and $\pi_1,..,\pi_3$ for a three component mixture model. Use the same notation as in Algorithm 8.1 in \citet{hastie2009elements}.
%\item Just as in Section \ref{uni_two_comp} implement an EM algorithm by implementing the follow functions in R:
%\begin{enumerate}
%\item \texttt{d\_uni\_three\_comp(x, theta)}
%\item \texttt{ll\_uni\_three\_comp(x, theta)}
%\item \texttt{e\_uni\_three\_comp(X, theta)}
%\item \texttt{max\_uni\_three\_comp(X, gamma)}
%\item \texttt{em\_uni\_three\_comp(X, theta\_0, iter)}
%\end{enumerate}
%\item  Run your EM algorithm for the three component model on the \texttt{eruptions} variable of the \texttt{faithful} data and on the \texttt{Petal.Length} variable of the \texttt{iris} data. What are the estimated parameters for the two datasets?
%\item Visualize the density for the two datasets using the parameters estimated with your EM algorithm. \emph{Hint!} You should be able to use \texttt{d\_uni\_three\_comp(x, theta)}.
%\end{enumerate}

%\subsubsection{Bivariate Normal Mixtures}

%We have now tested to change the number of mixture components, as the last step we will also change the density of each component ($\phi$ in \citet{hastie2009elements} notation). Here we will do that change by going from a univariate normal with parameter $\mu_i$ and $\sigma_i$ to a bivariate normal with the $\mathbf{\mu}_i$ and $\Sigma_i$. In this Section, \citet{smyth2020em} can be good to read.


%\begin{enumerate}
%\item Derive the expectation of the responsibilities for both a two- and three component mixture model (you can give a general notation if you want). Use the same notation as in Algorithm 8.1 in \citet{hastie2009elements}.
%\item Derive the maximization the paramaters $\mathbf{\mu}_1,..,\mathbf{\mu}_K$, $\Sigma_1,..,\Sigma_K$, and $\pi_1,..,\pi_K$ for a two- and three component mixture model. Use the same notation as in Algorithm 8.1 in \citet{hastie2009elements}.
%\item Just as in Section \ref{uni_two_comp} implement an EM algorithm by implementing the follow functions in R. You can either implement them as one function handling both two and three components, or as one function that can handle both situations.
%\begin{enumerate}
%\item \texttt{d\_bi\_K\_comp(x, theta)}
%\item \texttt{ll\_bi\_K\_comp(x, theta)}
%\item \texttt{e\_bi\_K\_comp(X, theta)}
%\item \texttt{max\_bi\_K\_comp(X, gamma)}
%\item \texttt{em\_bi\_K\_comp(X, theta\_0, iter)}
%\end{enumerate}
%\item  Run your EM algorithm with two components on the \texttt{eruptions} and \texttt{waiting} variables of the \texttt{faithful} data and with three components on the \texttt{Petal.Length} and \texttt{Petal.width} variables of the \texttt{iris} data. What are the estimated parameters for the two datasets?
%\item Visualize the density for the two datasets using the parameters estimated with your EM algorithm. \emph{Hint!} You should be able to use  \texttt{d\_bi\_K\_comp(x, theta)}. Also \texttt{geom\_density\_2d} in \texttt{ggplot2} that can be found \href{https://www.r-graph-gallery.com/2d-density-plot-with-ggplot2.html}{here}, is a good way to visualize two-dimensional densities.
%\item Finally, compare the results with the results in the k-means clustering above. What are the differences? Which model would you prefer and why?
%\end{enumerate}




\bibliography{bibliography}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
