\documentclass[11pt,a4paper,english]{article}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{listings}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Normal latex T1-fonts
\usepackage[T1]{fontenc}

% Figures
%\usepackage{graphicx,fancybox,subfigure}

 % AMS-stuff
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}

 % Misc
\usepackage{verbatim}
\usepackage{url}

\usepackage[round]{natbib}
\bibliographystyle{unsrtnat}

% Page size
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2cm}

% Paragraph
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% Horizontal line
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% No page numbers
\pagestyle{empty}

% New commands:
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\begin{document}
%\SweaveOpts{concordance=TRUE}

\begin{titlepage}

\center
\textsc{\LARGE Uppsala University}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.2]{files/uu_logo.png}\\[1cm]
\textsc{\Large Machine Learning}\\[0.5cm]
\textsc{\large }\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Assignment 1}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\vfill

\end{titlepage}


\HRule

\input{general_info.tex}

\HRule

% General Knitr options
% (this cannot be input since the file runs knitr before LaTeX)

<<echo=FALSE, eval=TRUE>>=
options(continue="  ", prompt="> ")
knitr::opts_chunk$set(size = "small")
@


\newpage

\tableofcontents

\newpage

%\section{General Questions}

%\input{general_questions_info.tex}

%\begin{enumerate}
%\item (2p) Describe, with your own words, what \citet{efron2020prediction} see as the difference between the \emph{traditional regression methods} and the \emph{pure prediction algorithms}. (max 1 paragraph)
%\item In many machine learning applications, we use Stochastic Gradient Descent, even though other optimization algorithms that use second-order derivatives are better. Why is this the case? Relate this to $N$ and $P$ and describe with your own words (max 1 paragraph)
%\item \citet{Goodfellow-et-al-2016} describe machine learning algorithms with tasks (T), performance (P) and experience (E). Describe these three concepts in your own words. (1-2 paragraphs)
%\item Describe the free lunch theorem with your own words? (max 1 paragraph)
% \item Describe the concept of model complexity/capacity with your own words? (max 1 paragraph)
%\item (2p) Describe the bias-variance trade-off with your own words. (max 1 paragraph)
%\end{enumerate}

\section{Basic, Stochastic, and Mini-Batch Gradient Descent}

This assignment will study different ways to optimize common objective functions in many areas of Machine Learning, namely, stochastic gradient descent. Here we will test to implement these optimizers for a well-known model, logistic regression. See \citet{ruder2016overview} for pseudo-code and details on the algorithms.

We are going to work with this data as a test case:

<<echo=TRUE,eval=TRUE>>=
library(uuml)
data("binary")

binary$gre_sd <- (binary$gre - mean(binary$gre))/sd(binary$gre)
binary$gpa_sd <- (binary$gpa - mean(binary$gpa))/sd(binary$gpa)
X <- model.matrix(admit ~ gre_sd + gpa_sd, binary)
y <- binary$admit
@


\subsection{Implement the gradient for logistic regression}


The likelihood function for logistic regression is
\[
L(\theta, \mathbf{y}, \mathbf{X}) = \prod^n_{i=1} p_i^{y_i} (1 - p_i)^{1-y_i} \,,
\]
where
\[
\log \frac{p_i}{1-p_i} = \mathbf{x}_i \theta  \,,
\]
and $\mathbf{x}_i$ is the $i$th row from the design matrix $\mathbf{X}$ and $\theta \in \mathbb{R}^P$ is a $1 \times P$ matrix with the parameters of interest.

Commonly, to find maximum likelihood estimates of $\theta$, we usually use the log-likelihood as the objective function we want to optimize, i.e.
\begin{align}
l(\theta, \mathbf{y}, \mathbf{X}) = & \sum_{i=1}^n y_i \log(p_i) + (1 - y_i) \log(1-p_i)\\
 = & \sum_{i=1}^n y_i \mathbf{x}_i\theta + \log(1-p_i)\\
 = & \sum_{i=1}^n y_i \mathbf{x}_i\theta - \log(1+\exp(\mathbf{x}_i\theta))\,.
\end{align}

Although, in our case we instead want to minimize the negative log likelihood NLL$(\theta, \mathbf{y}, \mathbf{X})=-l(\theta, \mathbf{y}, \mathbf{X})$, which is essentially the same as the cross-entropy loss.

\begin{enumerate}
\item (4p) Derive the the gradient for NLL$(\theta, \mathbf{y}, \mathbf{X})$ with respect to $\theta$.\\\emph{Hint!} See \citet[Ch. 4.4]{hastie2009elements}.
\item (2p) Implement the gradient as a function in R. Below are two examples of how it should work. Note, \texttt{l\_grad(y, X, theta)} return the gradient of $(1/n) l(\theta, \mathbf{y}, \mathbf{X})$. Show that your implementation give the same result as these test cases in your report.
<<echo=FALSE,eval=TRUE>>=
ll_grad <- function(y, X, theta){
c("(Intercept)" = -0.1825, "gre_sd" = 0.0857, "gpa_sd" = 0.0829)
}
@
<<echo=TRUE,eval=TRUE>>=
ll_grad(y, X, theta = c(0,0,0))
@

<<echo=FALSE,eval=TRUE>>=
ll_grad <- function(y, X, theta){
c("(Intercept)" = 0.0217, "gre_sd" = -0.0395, "gpa_sd" = -0.0426)
}
@
<<echo=TRUE,eval=TRUE>>=
ll_grad(y, X, theta = c(-1,0.5,0.5))
@

\end{enumerate}

\subsection{Implement Gradient Descent}
We now have the primary tool for implementing gradient descent and stochastic gradient descent. The log-likelihood function has been implemented in the \texttt{uuml} package and can be accessed as follows.
<<echo=TRUE,eval=TRUE>>=
# The inside of the function
ll
# How we can use it
theta <- c(0, 0, 0)
ll(y, X, theta)
@

\emph{Hint!} A common error is to get the sign of the gradient descent update wrong (confusing gradient for the negative log-likelihood and the gradient for the log-likelihood). If you have problems with getting your algorithm to converge, this might be a reason and something to check.

\begin{enumerate}
\item (1p) Run logistic regression in R to get an MLE estimate of \texttt{theta} using the \texttt{glm()} function. Print the three parameter estimates.

\emph{Hint!} The design matrix \texttt{X} already include an intercept so you need to remove that either from \texttt{X} or from the R formula using \texttt{y ~ -1 + X}.
\item Implement the following gradient descent algorithms \citep[see][for details and psuedo-code]{ruder2016overview} as three separate R functions. Print these functions in your report.

\emph{Note!} Each epoch should include all data points. A tip is to order the data points randomly in each iteration.

\begin{enumerate}
\item (2p) ordinary (full/batch) gradient descent
\item (2p) stochastic gradient descent
\item (2p) mini-batch (stochastic) gradient descent using ten samples to estimate the gradient
\end{enumerate}



A boilerplate for the algorithm can be found here.
<<echo=TRUE,eval=FALSE>>=
mbsgd <- function(y, X, sample_size, eta, epochs){
  # Setup output matrix
  results <- matrix(0.0, ncol = ncol(X) + 2L, nrow = epochs)
  colnames(results) <- c("epoch", "nll", colnames(X))

  # Run algorithm
  theta <- rep(0.0, ncol(X)) # Init theta to the 0 vector
  for(j in 1:epochs){

    ### Put the algorithm code here ###

    # Store epoch, nll and output results
    results[j, "epoch"] <- j
    results[j, "nll"] <- ll(y, X, theta)
    results[j, -(1:2)] <- theta
  }
  return(results)
}
@
The function can then be called as:
<<echo=TRUE,eval=FALSE>>=
result <- mbsgd(y, X, sample_size = 10, eta = 0.1, epochs = 500)
@

\item Try different learning parameters $\eta$ and run the algorithm for roughly 500 epochs. You should find three different $\eta$ for each algorithm. One $\eta$ where the optimizer diverge, one $\eta$ where the optimizer converge very slowly and one $\eta$ that converges quick(er). Note that $\eta$ should be fixed/the same during all 500 epochs. Visualize your results in two plots per algorithm and choice of $\eta$ as a line graph (hence 18 figures in total).
\begin{enumerate}
\item (2p) The (negative) log-likelihood value \emph{for all observations} for the given $\theta$ (y-axis) and the epochs (full data iterations, x-axis).
\item (2p) The value of one $\theta$ parameter of your choice (y-axis) and the epochs (full data iterations, x-axis). Also include the true values you got from using the \texttt{glm()} function above as a horizontal line in the figure.
\end{enumerate}
Below is an example of how you can plot the loss and the parameter over epochs using \texttt{ggplot2}. Note, this is just random generated data as an example.
<<echo=TRUE,eval=TRUE, fig.height=2>>=
library(ggplot2)
# create data
set.seed(4711)
epoch <- 1:500
nll <- rnorm(500,1.5) # Just random data.
data <- data.frame(nll, epoch)
# Plot
ggplot(data, aes(x=epoch, y=nll)) +
  geom_line() +
  geom_hline(yintercept = 1.5, color = "blue") +
  ggtitle("Example")
@
\end{enumerate}


\newpage

\section{Regularized Regression}

The datasets \texttt{prob2\_train} and \texttt{prob2\_test} contains simulated data with 240 explanatory variables (\texttt{V1-V240}) and 1 numerical response variable (\texttt{y}). As per the dataset names, the first dataset contains training data and the second contains test data for this problem. To access the data, just run:

<<eval=TRUE>>=
library(uuml)
data("prob2_train")
data("prob2_test")
dim(prob2_train)
X <- as.matrix(prob2_train[,-241])
y <- as.matrix(prob2_train[,"y"])
X_test <- as.matrix(prob2_test[,-241])
y_test <- as.matrix(prob2_test[,"y"])
@


You should do the following and present the results in your report:
\begin{enumerate}
\item (1p) Fit a linear model to the training data. What are the results? Why does this happen?
\item (1p) Use \texttt{glmnet()} function from the \texttt{glmnet} package to fit a linear lasso regression to the training data with $\lambda=1$. How many coefficients are used in the model? \emph{Hint!} \texttt{coefficients()}
\item Split the data into ten folds randomly.
\emph{Hint!} Use the \texttt{sample()} function and create a new variable called fold. Here is an example on how to select the folds
<<eval=FALSE>>=
fold <- sample(1:10, nrow(X), replace = TRUE)

X_trainfold1 <- X[fold==1,]
X_valfold1 <- X[fold!=1,]
y_trainfold1 <- y[fold==1,]
y_valfold1 <- y[fold!=1,]
@
\item (5p) Now implement 10-fold cross-validation on the training data.
\emph{Hint!} Implement this as function called \texttt{glmnet\_cv()} that takes the \texttt{fold} variable, \texttt{X}, \texttt{y} and \texttt{lambda} and then outputs the RMSE.

\begin{enumerate}
\item For each fold, hold out the validation fold and train on the the rest of the training set. See below.
<<eval=FALSE>>=
mod <- glmnet(X_trainfold, y_trainfold, alpha = 1, lambda = [different lambda here])
pred_y_valfold <- predict(mod, newx = X_valfold)
@
\item Use the \texttt{rmse()} function in the \texttt{uuml} R package to compute the root mean squared error (RMSE) for each fold as follows:
<<eval=FALSE>>=
rmse(pred_y_valfold, y_valfold)
@
\item Take the mean RMSE for the five folds.
\end{enumerate}
\item (1p) Now, compute the RMSE for $\lambda=1$ using 5-fold cross-validation.
\item (2p) Test different values for the hyper parameter $\lambda$. What is the best RMSE you can get with 10-fold cross-validation? What is the $\lambda$ for this value?
\item (1p) Now use the best model to do predictions on the test set. What is your RMSE on the test set?
\end{enumerate}

\newpage

\section{* Gradient Descent for penalized logistic regression}

To get a pass with distinction point, you can choose between this task or the task below.

\input{distinction_task.tex}

Here we use the same data as in task 1 above.

<<echo=TRUE,eval=FALSE>>=
library(uuml)
data("binary")

binary$gre_sd <- (binary$gre - mean(binary$gre))/sd(binary$gre)
binary$gpa_sd <- (binary$gpa - mean(binary$gpa))/sd(binary$gpa)
X <- model.matrix(admit ~ gre_sd + gpa_sd, binary)
y <- binary$admit
@

In ridge regression we penalize the regression coefficients by $\lambda$. This gives the likelihood function for logistic regression with ridge penalty as
\begin{align}
l_r(\theta, \mathbf{y}, \mathbf{X}, \lambda) = \frac{1}{n} l(\theta, \mathbf{y}, \mathbf{X}) - \frac{\lambda}{2} \sum_{i=1}^P \theta_i^2\,,
\end{align}
where $l(\theta, \mathbf{y}, \mathbf{X})$ is defined as in task 1 above. We also want to minimize the negative penalized log likelihood $\text{NLL}_r(\theta, \mathbf{y}, \mathbf{X}, \lambda)=-l_r(\theta, \mathbf{y}, \mathbf{X}, \lambda)$ here as well.

\emph{Note!} In general, we should not regularize the intercept \citep[see ][Ch. 3.4]{hastie2009elements}. Hence below the gradient don't regularize intercept.

\emph{Note!} The objective function above is the one used in the \texttt{glmnet} package. We can define this slightly different (for example not divide $l$ with $n$). This will give different estimates because $\lambda$ will have different meanings. More information on the \texttt{glmnet} objective function can be found \href{https://glmnet.stanford.edu/articles/glmnet.html#logistic-regression-family-binomial-}{here}.

\begin{enumerate}
\item (2p) Derive the the gradient for $\text{NLL}_r(\theta, \mathbf{y}, \mathbf{X},\lambda)$ with respect to $\theta$.
\item (1p) Implement the gradient as a function in R. Below are two examples of how it should work. \emph{Note!} \texttt{l\_grad(y, X, theta, lambda)} implemented above return the gradient of $(1/n) l(\theta, \mathbf{y}, \mathbf{X}, \lambda)$. Show that your implementation give the same result as these test cases in your report.
<<echo=FALSE,eval=TRUE>>=
lr_grad <- function(y, X, theta, lambda){
c("(Intercept)" = -0.1825, "gre_sd" = 0.0857, "gpa_sd" = 0.0829)
}
@
<<echo=TRUE,eval=TRUE>>=
lr_grad(y, X, theta = c(0,0,0), lambda = 0)
@

<<echo=FALSE,eval=TRUE>>=
lr_grad <- function(y, X, theta, lambda){
c("(Intercept)" = -0.1825, "gre_sd" = 0.0857, "gpa_sd" = 0.0829)
}
@
<<echo=TRUE,eval=TRUE>>=
lr_grad(y, X, theta = c(0,0,0), lambda = 1)
@

<<echo=FALSE,eval=TRUE>>=
lr_grad <- function(y, X, theta, lambda){
    c("(Intercept)" = 0.0217, "gre_sd" = -0.5395, "gpa_sd" = -0.5426)
}
@
<<echo=TRUE,eval=TRUE>>=
lr_grad(y, X, theta = c(-1,0.5,0.5), lambda = 1)
@
\item (1p) Implement the regularized log-likelihood $l_r$ or the negative log-likelihood $\text{NLL}_r$ in R. \emph{Note!} This differs from the log-likelihood above (where we used the sum over the observations). Show that your implementation give the same result as these test cases in your report.
<<echo=FALSE,eval=TRUE>>=
lr <- function(y, X, theta, lambda){
-0.6931
}
@
<<echo=TRUE,eval=TRUE>>=
lr(y, X, theta = c(0,0,0), lambda = 0)
@

<<echo=FALSE,eval=TRUE>>=
lr <- function(y, X, theta, lambda){
-0.6931
}
@
<<echo=TRUE,eval=TRUE>>=
lr(y, X, theta = c(0,0,0), lambda = 1)
@

<<echo=FALSE,eval=TRUE>>=
lr <- function(y, X, theta, lambda){
-0.8613
}
@
<<echo=TRUE,eval=TRUE>>=
lr(y, X, theta = c(-1,0.5,0.5), lambda = 1)
@
\item (1p) Run logistic regression with ridge penalty in R using \texttt{glmnet} estimate of \texttt{theta}. Set \texttt{lambda} to 1 and \texttt{alpha} to 0 to run a penalized logistic regression. You also need to remove the intercept from \texttt{X}.
\item (2p) Implement the following gradient descent algorithms for the penalized logistic objective:
\begin{enumerate}
\item ordinary (full or batch) gradient descent
\item mini-batch (stochastic) gradient descent using ten samples to estimate the gradient
\end{enumerate}
\item (2p) Try different learning parameters $\eta$ and $\lambda$. When does the algorithm converge or diverge? Visualize the iterations (x-axis) and the log-likelihood (y-axis). Show at least one plot per algorithm that converges.
\end{enumerate}

\newpage

\section{* Implementation of the Adam optimizer}

To get a pass with distinction point, you can choose between this task or the task above

\input{distinction_task.tex}

Here we use the same data as in task 1 above.

<<echo=TRUE,eval=FALSE>>=
library(uuml)
data("binary")

binary$gre_sd <- (binary$gre - mean(binary$gre))/sd(binary$gre)
binary$gpa_sd <- (binary$gpa - mean(binary$gpa))/sd(binary$gpa)
X <- model.matrix(admit ~ gre_sd + gpa_sd, binary)
y <- binary$admit
@
\begin{enumerate}

\item Implement the Adam mini-batch gradient descent algorithm \citep[see][for details and psuedo-code]{ruder2016overview} and \citep[][ for details]{kingma2014adam} as an R function. Not that in \citet{kingma2014adam}, the stepsize is denoted by $\alpha$.

\emph{Note!} Each epoch should include all data points. A tip is to order the data points randomly in each iteration.

A boilerplate for the algorithm can be found here.
<<echo=TRUE,eval=FALSE>>=
adam <- function(y, X, sample_size, eta, beta1, beta2, epochs){
  # Setup output matrix
  results <- matrix(0.0, ncol = ncol(X) + 2L, nrow = epochs)
  colnames(results) <- c("epoch", "nll", colnames(X))

  # Run algorithm
  theta <- rep(0.0, ncol(X)) # Init theta to the 0 vector
  for(j in 1:epochs){

    ### Put the algorithm code here ###

    # Store epoch, nll and output results
    results[j, "epoch"] <- j
    results[j, "nll"] <- ll(y, X, theta)
    results[j, -(1:2)] <- theta
  }
  return(results)
}
@
The function can then be called as:
<<echo=TRUE,eval=FALSE>>=
result <- mbsgd(y, X, sample_size = 10, eta = 0.1, epochs = 500)
@
\item Print the function in your report.
\item Try different learning parameters $\eta$, $\beta_1$ and $\beta_2$, and run the algorithm for roughly 500 epochs. You should test three different parameters setup. Visualize your results in the same way as in the SGD task above, namely:
\begin{enumerate}
\item (2p) The (negative) log-likelihood value \emph{for all observations} for the given $\theta$ (y-axis) and the epochs (full data iterations, x-axis).
\item (2p) The value of one $\theta$ parameter of your choice (y-axis) and the epochs (full data iterations, x-axis). Also include the true values you got from using the \texttt{glm()} function above as a horizontal line in the figure.
\end{enumerate}
\end{enumerate}

\newpage



\bibliography{bibliography}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
