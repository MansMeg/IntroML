% !Rnw weave = knitr

\documentclass[11pt,a4paper,english]{article}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{listings}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Normal latex T1-fonts
\usepackage[T1]{fontenc}

% Figures
%\usepackage{graphicx,fancybox,subfigure}

 % AMS-stuff
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}

 % Misc
\usepackage{verbatim}
\usepackage{url}

\usepackage[round]{natbib}
\bibliographystyle{unsrtnat}


% Page size
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2cm}

% Paragraph
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% Horizontal line
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% No page numbers
\pagestyle{empty}

% New commands:
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}



\begin{document}

\begin{titlepage}

\center
\textsc{\LARGE Uppsala University}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.2]{files/uu_logo.png}\\[1cm]
\textsc{\Large Machine Learning}\\[0.5cm]
\textsc{\large }\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Assignment 8}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\vfill

\end{titlepage}


\HRule

% General Knitr options
% (this cannot be input since the file runs knitr before LaTeX)
<<echo=FALSE,eval=TRUE>>=
#options(continue="  ", prompt="> ")
knitr::opts_chunk$set(size = "small")
@

\input{general_info.tex}

\HRule

\newpage

\tableofcontents

\newpage

% \section{General Questions}

% \input{general_questions_info.tex}

% \begin{enumerate}
%\item What is reinforcement learning? (max 1 paragraph)
% \item Describe the exploration-exploitation dilemma. (max 1 paragraph)
% Describe an rl policy
% Describe an rl value function
% \item Explain the concept of reward signal. (max 1 paragraph)
% When might a greedy bandit be better than an epsilon 0.1 bandit? And why?
% \item Describe the concept of state in a Markov Decision Process. (max 1 paragraph)
% Why should not RL agents be rewarded for a subgoal. Come up with an example?
% What is a policy in an MDP?
% \end{enumerate}



\section{Bandits}

In the first part, we will study the $k$-armed bandit problems more in detail.

\begin{enumerate}
\item (2p) Implement a stationary 5-armed bandit reward function that generates a reward $R$ for a given category according to a true underlying (stationary) reward. Implement a function \texttt{stationary\_bandit(a)} that takes an action $\mathcal{A} = (1,...,5)$. The reward function should return a reward according to
\[
R \sim N(q^\star_a, 1)\,,
\]
where $q^\star=(1.62, 1.20, 0.70, 0.72, 2.03)$. Below is an example of how it \emph{could} work. Note that since the function returns a random value, you might get a different result, and it might still be a correct implementation. However, to test, you can run your bandit 1000 times and take the mean (see below). Then you should be close to $q^\star$ if your implementation is correct.
<<echo=FALSE>>=
stationary_bandit<- function(a){
  if(a == 1) return(2.99044)
  if(a == 2) return(3.019735)
  if(a == 3) return(0.6819202)
}
<<>>=
set.seed(4711)
stationary_bandit(2)
stationary_bandit(1)
R <- numeric(1000)
for(i in 1:1000){
  R[i] <- stationary_bandit(3)
}
mean(R)
@
\item (2p) Also, we will also implement a non-stationary reward function. For all actions $\mathcal{A} = (2,...,5)$ the non-stationary should work as the stationary reward function. Although for action $a=1$, the function should return a reward based on the iteration according to
\[
R \sim N(-3 + 0.01t, 1)\,.
\]
Implement the reward function as \texttt{nonstationary\_bandit(a, t)} and it should work as follows. Again, note that these are random values, you might have another (correct) solution giving different results.
<<echo=FALSE>>=
nonstationary_bandit<- function(a, t){
  if(a == 1 & t == 1) return(-1.793682)
  if(a == 1 & t == 1000) return(6.593121)
  if(a == 2 & t == 1) return(3.019735)
  if(a == 2 & t == 1000) return(2.57044)
}
@
<<>>=
set.seed(4711)
nonstationary_bandit(2, t = 1)
nonstationary_bandit(2, t = 1000)
nonstationary_bandit(1, t = 1)
nonstationary_bandit(1, t = 1000)
@
\item (2p) Implement a greedy policy algorithm that always chooses the greedy action, according to Eq. (2.2) in \citet{sutton2020reinforcement}. The function called \texttt{greedy\_algorithm(Q1, bandit)} should input a vector of initial estimates $Q_1$ and a reward function. Then the algorithm should run the bandit for 1000 steps and return the following; (1) The rewards $R_t$ for all steps, (2) the mean reward over all the 1000 steps, (3) the value estimates $Q_t(a)$, and (4) the total number of choices of each action $N_t(a)$. Here is an example of how it could work. \emph{Note!} This is a random algorithm so you might get a different result, even if you use the exact same seed and have a correct algorithm.
<<eval = FALSE>>=
Q1 <- rep(0, 5)
set.seed(4711)
greedy_algorithm(Q1, stationary_bandit)
## $Qt
## [1] 1.602744 0.000000 0.000000 0.000000 0.000000
##
## $Nt
## [1] 1000    0    0    0    0
##
## $R_bar
## [1] 1.602744
##
## $Rt
## [1]  3.439735114  2.990439504  2.816318235  1.213120797 ...

set.seed(4711)
Q1 <- rep(0, 5)
greedy_algorithm(Q1, nonstationary_bandit)
## $Qt
## [1] -1.170265  1.180905  0.000000  0.000000  0.000000
##
## $Nt
## [1] 1 999   0   0   0
##
## $R_bar
## [1] 1.178554
##
## $Rt
## [1]  -1.170264886  2.570439504  2.396318235  0.793120797 ...
@
\item (2p) Similarly, implement the $\epsilon$-bandit algorithm that takes an additional argument $\epsilon$ that is the probability that the algorithm instead takes makes an explorative action. Implement this algorithm as \texttt{epsilon\_algorithm(Q1, bandit, epsilon)}, note that it should return the same output structure as the greedy algorithm. It is also clear that the randomness of the exploration can give quite some different results in different runs.
<<eval = FALSE>>=
set.seed(4711)
Q1 <- rep(0, 5)
epsilon_algorithm(Q1, stationary_bandit, 0.1)
## $Qt
## [1] 1.6093183 1.1491153 0.7382012 0.5975228 2.0272784
##
## $Nt
## [1] 234  17  19  16 714
##
## $R_bar
## [1] 1.867178
##
## $Rt
## [1]  1.77251764  2.81631824  0.63350863  2.01754941 ...

set.seed(4711)
Q1 <- rep(0, 5)
epsilon_algorithm(Q1, nonstationary_bandit, 0.1)
## $Qt
## [1] 1.5887316 1.0956679 0.6962471 0.5975228 2.0468788
##
## $Nt
## [1]  24 159  20  16 781
##
## $R_bar
## [1] 1.834438
##
## $Rt
## [1]  -2.83748236  2.39631824  0.21350863  2.01754941 ...
@
\item (2p) Now implement the non-stationary bandit algorithm in a similar way with parameter $\alpha$ as \texttt{nonstationary\_algorithm(Q1, bandit, epsilon, alpha)}.
<<eval = FALSE>>=
set.seed(4711)
Q1 <- rep(0, 5)
nonstationary_algorithm(Q1, stationary_bandit, epsilon = 0.1, alpha = 0.2)
## $Qt
## [1] 1.414101 1.293336 1.237085 0.517214 2.134716
##
## $Nt
## [1] 302  16  19  16 647
##
## $R_bar
## [1] 1.840128
##
## $Rt
## [1] 1.77251764  2.81631824  0.63350863  2.01754941 ...

set.seed(4711)
Q1 <- rep(0, 5)
nonstationary_algorithm(Q1, nonstationary_bandit, epsilon =  0.1, alpha = 0.2)
## $Qt
## [1] 7.055805 1.287957 1.243996 0.517214 2.569759
##
## $Nt
## [1] 387 193  70  16 334
##
## $R_bar
## [1] 2.818588
##
## $Rt
## [1] -2.83748236  2.39631824  0.21350863  2.01754941 ...
@

\item (2p) Run 500 simulations of each algorithm and compute the mean of the mean rewards for each algorithm for both the stationary and non-stationary bandits. Do this for the greedy, the $\epsilon$-algorithm, and the non-stationary algorithm. Try both $\alpha=0.5$ and $\alpha=0.9$ for the non-stationary algorithm. For both the non-stationary and the $\epsilon$-algorithm, use $\epsilon=0.1$. Summarize all the results in a table. You should have eight different results (four per bandit and two per algorithm).

\emph{Note!} The algorithm should run 1000 steps for each of the 500 simulations.
\item (2p) What are your conclusions from these results? Which algorithm seems to be the best one, and why?
\item (2p) Plot the average reward per step for the 500 runs for the worst and best algorithm, such as in Figure 2.2. in \citet{sutton2020reinforcement}. Use the output $R_t$ from your implementations.
\end{enumerate}

\newpage

\section{Markov Decision Processes}

Now we are going to implement a Markov Decision Process (MDP) where the agent makes a decision, $A_t$ based on the current state $S_t$ and a reward $R_t$ that will, in turn, return a new state $S_{t+1}$ and a reward $R_{t+1}$. In this part of the assignment, we are going to use the recycling robot example \citep[Example 3.3 in ][]{sutton2020reinforcement}.

\begin{enumerate}
\item The MDP in \citet{sutton2020reinforcement} has been implemented as a function \texttt{recycling\_mdp(alpha, beta, r\_wait, r\_search)} in the \texttt{uuml} R package and can be accessed as follows.
<<>>=
library(uuml)
recycling_mdp(0.5, 0.8, 0.1, 1)
@
\item (2p) Now implement a function \texttt{always\_search\_policy(MDP)} that takes an MPD specified above and run the MDP for 1000 steps/actions. The policy should be that the agent always chooses to search, irrespective of the state. The function should return the return divided by the number of time steps and times in each state. The robot should start in the state \texttt{high}. Below is an example of how it should work. Note again, since this is a random algorithm, you might get a slightly different answer.
<<eval=FALSE>>=
set.seed(4711)
always_search_policy(mdp)
## $Nt
## high low
## 288  712
##
## $R_bar
## [1] 0.452
@
\item (2p) Now implement a function \texttt{charge\_when\_low\_policy(MDP)} that takes an MPD specified above and run the MDP for 1000 steps. The policy should be that the agent always chooses to recharge if the energy level is low and always search if the energy level is high. The function should return the same output structure as above.
\item (2p) Compare the two policies for MDP with $\alpha = \beta = 0.9$ and $\alpha = \beta = 0.4$ with $r_\text{wait} = 0.1 \text{ and } r_\text{search} = 1$. Run at least ten times per policy and compute the mean reward. What policy is prefered in the two MDPs?
\end{enumerate}

\newpage

\section{* Some additional bandit algorithms}

\input{distinction_task.tex}

We are here going to implement two additional bandits and compare them to the previous implementations above.

\begin{enumerate}
\item (2p) Implement the UCB algorithm as a function called \texttt{ucb\_algorithm(Q1, bandit, c)}. \emph{Hint!} If $N_t(a) = 0$, then $a$ should be considered as the maximizing action (i.e. should be chosen).
<<eval = FALSE>>=
set.seed(4711)
Q1 <- rep(0, 5)
ucb_algorithm(Q1, stationary_bandit, c = 2)
## $Qt
## [1]  1.43350715  1.09734192 -0.01287212  0.64762674  2.02954004
##
## $Nt
## [1]  48  26   6  13 907
##
## $R_bar
## [1] 1.946474
##
## $Rt
## [1]  3.43973511  2.57043950  1.89631824 ...

set.seed(4712)
Q1 <- rep(0, 5)
ucb_algorithm(Q1, stationary_bandit, c = 2)

## $Qt
## [1] 1.4367906 1.0364514 0.4346884 0.4140891 1.9864393
##
## $Nt
## [1]  57  24  11  10 898
##
## $R_bar
## [1] 1.899517
##
## $Rt
## [1]  1.9818356939  1.1212566109  0.9286269996 ...
@
\item (2p) Implement the gradient bandit algorithm as a function called \texttt{gradient\_bandit\_algorithm(bandit, alpha)}. Initialize all $H_t(a) = 1$. The algorithm should print the final preference and the probability of each action instead of $Q_t$.
<<eval = FALSE>>=
set.seed(4711)
gradient_bandit_algorithm(stationary_bandit, alpha = 0.1)
## $Ht
## [1] -0.4771226 -1.0045831 -1.7018038 -1.7427317  4.9262412
##
## $pit
## [1] 0.004 0.003 0.001 0.001 0.990
##
## $Nt
## [1]  27  24  11  12 926
##
## $R_bar
## [1] 1.941705
##
## $Rt
## [1]  1.772517635  2.396318235 -0.286491374 ...

set.seed(4712)
gradient_bandit_algorithm(stationary_bandit, alpha = 0.1)
## $Ht
## [1] -1.217231 -1.201571 -1.938602 -1.241492  5.598897
##
## $pit
## [1] 0.001 0.001 0.001 0.001 0.996
##
## $Nt
## [1]  26  30  18  20 906
##
## $R_bar
## [1] 1.885579
##
## $Rt
## [1]  0.62963402  0.94862700 -0.30089716 ...
@
\item (2p) Test to run the gradient bandit with $\alpha = 0$. What happens and why?
\item (2p) Run the gradient bandit and the UCB algorithms 500 times/simulations and compute the mean of the mean rewards (for each simulation) for each of the algorithms. \emph{Note!} The algorithm should run 1000 steps for each of the 500 simulations. Use both the stationary and non-stationary bandit. For the UCB, try $c=0.5$ and $c=2$ and for the gradient bandit $\alpha = 0.1$ and $\alpha = 1$. Summarize all the results in a table and compare the results with the previous bandit algorithms. Which algorithm performs the best?
\end{enumerate}

% TODO (later): Instead implement a full MDP

\newpage
\bibliography{bibliography}

% Maybe future assignments
% \item Compute the optimal value function for the states for the two different MDP above with a discount of $\gamma=0.5$. What is the optimal value for the two different states? Hint, see Example 3.11 (Bellman Optimality Equations for the Recycling Robot) in Sutton and Barto (2017). % TODO: give an example how to compute example
% \item Implement a policy function called \texttt{optimal\_greed(mdp)} that uses a \emph{greedy} policy using the optimal values. Run the algorithm at least ten times for 1000 steps and compare with the two previous policies. What are your conclusions?

%\subsection{OpenAI Gym (Optional)}
%Play around with the Open AI gym
%https://gym.openai.com/docs/

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
