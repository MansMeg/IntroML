% !Rnw weave = knitr

\documentclass[11pt,a4paper,english]{article}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{listings}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Normal latex T1-fonts
\usepackage[T1]{fontenc}

% Figures
%\usepackage{graphicx,fancybox,subfigure}

 % AMS-stuff
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}

 % Misc
\usepackage{verbatim}
\usepackage{url}

\usepackage[round]{natbib}
\bibliographystyle{unsrtnat}

% Page size
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2cm}

% Paragraph
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% Horizontal line
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% No page numbers
\pagestyle{empty}

% New commands:
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\begin{document}

\begin{titlepage}

\center
\textsc{\LARGE Uppsala University}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.2]{files/uu_logo.png}\\[1cm]
\textsc{\Large Introduction to Machine Learning, Big Data, and AI}\\[0.5cm]
\textsc{\large }\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Assignment 5}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\vfill

\end{titlepage}


\HRule

\input{general_info.tex}

% General Knitr options
% (this cannot be input since the file runs knitr before LaTeX)
<<echo=FALSE,eval=TRUE>>=
options(continue="  ", prompt="> ")
knitr::opts_chunk$set(size = "small")
@

\HRule


\newpage

\tableofcontents

\newpage

\section{General Questions}

\input{general_questions_info.tex}
\begin{enumerate}
%\item What is the difference between a recurrent neural network and an ordinary feed-forward neural network? (max 1 paragraph)
%\item What is LSTM neural network, and why does it work better on long sequences than a  naive  RNN? (max 1 paragraph)
%\item What is the main difference between a GRU and LSTM neural network? (max 1 paragraph)
\item Name at least one difference between the BERT model and the original transformer model? (max 1 paragraph)
\end{enumerate}


\section{Transformers and Attention}

In this assignment, we will implement a multi-head attention transformer layer. These layers are currently used within all transformer-based models, such as GPT and different BERT models. A good read for this assignment is the Illustrated transformer by Jay Alammar, which you can find [here](http://jalammar.github.io/illustrated-transformer/).

Start by loading the parameters that we are going to use for the implementation as follows.
<<>>=
library(uuml)
data("transformer_example")
@

\begin{enumerate}
\item Start out by computing the query, key and value matrices for one attention head by implementing it in a function you call \texttt{qkv()}. The function should work like this.
<<eval=FALSE, >>=
# Pick out the matrix for the first attention head
Wq <- transformer_example$Wq[,,1]
Wk <- transformer_example$Wk[,,1]
Wv <- transformer_example$Wv[,,1]

# Pick out the first three words and their embeddings
X <- transformer_example$embeddings[1:3,]

qkv(X, Wq, Wk, Wv)

## $Q
##            [,1]        [,2]       [,3]
## the    0.4722259  0.04995783 -0.5350845
## quick -0.3662435  0.12144160  0.3454785
## brown -0.1029677 -0.12728414  0.1817097
##
## $K
##               [,1]         [,2]       [,3]
## the    0.094360579 -0.203807092 -0.1851229
## quick -0.033313240  0.279012100  0.2530560
## brown -0.004457052  0.001013468  0.0133802
##
## $V
##               [,1]        [,2]        [,3]
## the    0.317318525 -0.35023010  0.13284078
## quick  0.009929565  0.04208206 -0.15412097
## brown -0.316413241  0.27717408  0.02725089
@
\item Now, based on your query, key and value, compute the attention of that given attention head for the three chosen tokens.
<<eval=FALSE>>=
attention(res$Q, res$K, res$V)
## $Z
##               [,1]          [,2]         [,3]
## the    0.012395453 -0.0212420459  0.009404870
## quick -0.003759269 -0.0008360029 -0.005108890
## brown  0.002412222 -0.0088974612  0.001147999
##
## $attention
##             the     quick     brown
## the   0.3601932 0.3080896 0.3317172
## quick 0.3088780 0.3582373 0.3328847
## brown 0.3300375 0.3360583 0.3339042
@
\item Interpret the attention values. What does the second row mean?
\item Now we have everything in place for implementing it all as a multi-head attention layer. The layer will take in embeddings and then return a 3-dimensional embedding per word. Run your code on all words in the included example.
<<eval=FALSE>>=
multi_head_self_attention(X,
                          transformer_example$Wq,
                          transformer_example$Wk,
                          transformer_example$Wv,
                          transformer_example$W0)
##               [,1]          [,2]         [,3]
## the   -0.014189613 -0.0040299008 -0.006756286
## quick -0.009963516 -0.0010724342 -0.001996524
## brown -0.006394562 -0.0006626115 -0.002219108
@
\end{enumerate}

\newpage

\section{Implementing a simple RNN}

%\input{distinction_task.tex}

We are going to implement a one-layer recurrent neural network based on the \texttt{rnn\_example} data. We are going to implement a simple layer described in Section 10.2 in \citet{Goodfellow-et-al-2016}. See this section for details.

We will set the dimensionality of the hidden state to 4 and the output dimension to 3. In this task, we will set $h_0$, the initial starting state, to a zero-vector. The input will be the embeddings in the \texttt{rnn\_example} data.


<<>>=
library(uuml)
data("rnn_example")
@

\begin{enumerate}
\item We start out by implementing a simple RNN linear unit that takes the parameters \texttt{W}, \texttt{U}, \texttt{b}, the previous hidden state \texttt{h\_t\_minus\_one} and an input embedding \texttt{x\_t} and return the activation input $a_t$. It should work as follows.
<<eval=FALSE, >>=
X <- rnn_example$embeddings[1,,drop=FALSE]
h_t_minus_one <- matrix(0, nrow = hidden_dim, ncol = 1)
a_t <- rnn_unit(h_t_minus_one, X,
                W = rnn_example$W,
                U = rnn_example$U,
                b = rnn_example$b)
a_t
##
##             the
## [1,]  0.5819145
## [2,] -2.2686535
## [3,] -0.6410312
## [4,]  1.4891931
##
@
\item Now implement the \texttt{tanh()} activation function.
<<eval=FALSE, >>=
h_t <- activation(a_t)
h_t
##
##             the
## [1,]  0.5240555
## [2,] -0.9788223
## [3,] -0.5656013
## [4,]  0.9031762
##
@
\item As the next step, implement the output function and the softmax function. These functions should work in the following way.
<<eval=FALSE, >>=
yhat_t <- softmax(output_rnn(h_t, rnn_example$V, rnn_example$c))
yhat_t
##
##            the
## [1,] 0.3063613
## [2,] 0.2930885
## [3,] 0.4005502
##
@
\item Now we are ready to implement the full recurrent layer. It should take an input matrix X and the neural network parameters and return the hidden states and the softmax output as follows.
<<eval=FALSE, >>=
X <- rnn_example$embeddings[1:3,,drop=FALSE]
rnn_layer(X,
          W = rnn_example$W,
          V = rnn_example$V,
          U = rnn_example$U,
          rnn_example$b,
          rnn_example$c)
##
## $ht
##              [,1]        [,2]       [,3]       [,4]
## the    0.52405551 -0.97882227 -0.5656013  0.9031762
## quick -0.05951368  0.03988226  0.8241800 -0.6562744
## brown -0.08984008  0.92822217 -0.1563247 -0.6657626
##
## $yhat
##            [,1]      [,2]      [,3]
## the   0.3063613 0.2930885 0.4005502
## quick 0.2838013 0.3490452 0.3671536
## brown 0.2878002 0.3666877 0.3455121
@
\item Now, what is the value of the hidden state \texttt{h\_t} for the token \texttt{dog}?
\end{enumerate}

\newpage

\section{*Recurrent Neural Networks with Keras}

To get VG on this assignment, you can choose to do this task OR the task using BERT with PyTorch below. You are also free to do both, if you want. As long as you pass one of the VG assignments you will get a VG point. Although, please only write down the time it took to do the VG assignment for one of the tasks if you choose to do more than one.

\input{distinction_task.tex}


As a first step, we will try using Recurrent Neural Networks on a text classification task. Here Chapter 6 - 6.2 in \citet{chollet2018deep} will be the primary reference.

Start by downloading the IMBD dataset and preprocess it as follows.

<<eval=FALSE>>=
library(tensorflow)
library(keras)
imdb <- dataset_imdb(num_words = 10000)
c(c(input_train, y_train), c(input_test, y_test)) %<-% imdb
input_train <- pad_sequences(input_train, maxlen = maxlen)
input_test <- pad_sequences(input_test, maxlen = maxlen)
@

\emph{Note!} Running Keras can be computationally heavy, and I suggest not to run the code in \texttt{markdown}. Instead, run the code in R and copy the results as output (see the assignment template for an example).

\begin{enumerate}
\item Create a simple ordinary neural network for text classification in Keras using an embedding layer of dimension 16 and a hidden layer with 32 hidden states (and an output layer with one unit). Use a validation split of 20\% observations in the validation set and a batch size of 128. Train your network using rmsprop and use binary cross-entropy as the loss function. Print the model and return your validation accuracy after ten epochs. Most of these settings we will reuse later in the assignments.
\item Explain what the embedding layer is doing.
\item Now setup a simple Recurrent Neural Network with 32 hidden units based on the same embedding layer. Print the model and present your result.
\item The model you have implemented now. Which recurrent network does best describes the model you have now implemented out of Fig. 10.3 and 10.5 in \citet{Goodfellow-et-al-2016}? Motivate why.
\item Explain the role of the parameters $U$, $W$ in the recurrent net (see Goodfellow et al., 2017 Section 10.2). How large are these parameters in your model specified above, and where are they included in the Keras output?
\item Now, explain the role of the parameters $V$ (see Goodfellow et al., 2017 Section 10.2). How many parameters are included in $V$, and where are they included in the Keras output?
\item We are now going to implement an LSTM network. Implement a standard LSTM network with 32 hidden units using Keras. Print your model and your validation accuracy.
\item Extend your network in ways you think fit. Report at least two other extensions/modifications of your network and why you chose them. Report the Keras model output and validation accuracy.
\item Reason why we cannot get as good performance as with the MNIST dataset. Is the problem bias, variance, or the Bayes error? Why?
\end{enumerate}


\newpage

\section{*BERT for Swedish with PyTorch}

To get VG on this assignment, you can choose to do this task OR the task using Keras and PyTorch above. You are also free to do both if you want. You will get a VG point if you pass one of the VG assignments. Although, please only write down the time it took to do the VG assignment for one of the tasks if you choose to do more than one.

\input{distinction_task.tex}

Unfortunately, there is yet to be a simple way to run BERT directly from R. Although, using Google Colab, we can run one of the Swedish BERT models trained by the Swedish national library. The tutorial is in English, and without any Swedish understanding, it should be straightforward to follow.

Follow the tutorial on Google Colab \href{https://github.com/MansMeg/IntroML/blob/master/assignments/swedish_bert_classification.ipynb}{\color{blue}{here}}. Then do the following things.

\begin{enumerate}
\item Try to improve on the relatively low classification accuracy and get above 80\% overall accuracy. You are free to elaborate how much you want yourself, but some suggestions are:
\begin{enumerate}
\item Fine-tune a larger part of the BERT model (not just the classification head)
\item Run the model for more epochs
\item Try out other learning rates (scheduling)
\end{enumerate}
\emph{Note!} Training these large neural networks might take some time. It can take more than an hour at Google Colab to fine-tune the whole BERT model.
\item Write down the steps you take, the overall accuracy you get along the way, and the final accuracy of your fine-tuned BERT model, both overall and per party.
\item Reason based on your final confusion matrix on the difficulties for the model. Can the errors be reduced further or not? Why?
\end{enumerate}






%\begin{enumerate}
%\item Using reticulate in R. \href{https://blogs.rstudio.com/ai/posts/2019-09-30-bert-r/}{Here is a tutorial.}
%\item Run BERT using Python in Colab \href{https://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module05%20-%20NLP%20Applications/Project07E%20-%20Text%20Classification%20with%20BERT%20Deep%20Transfer%20Learning.ipynb}{here}.
%\end{enumerate}


\newpage
\bibliography{bibliography}

\end{document}

