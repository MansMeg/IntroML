\documentclass[11pt,a4paper,english]{article}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{listings}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Normal latex T1-fonts
\usepackage[T1]{fontenc}

% Figures
%\usepackage{graphicx,fancybox,subfigure}

 % AMS-stuff
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}

 % Misc
\usepackage{verbatim}
\usepackage{url}

\usepackage[round]{natbib}
\bibliographystyle{unsrtnat}

% Page size
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2cm}

% Paragraph
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% Horizontal line
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% No page numbers
\pagestyle{empty}

% New commands:
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\begin{document}

\begin{titlepage}

\center
\textsc{\LARGE Uppsala University}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.2]{files/uu_logo.png}\\[1cm]
\textsc{\Large Machine Learning}\\[0.5cm]
\textsc{\large }\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Assignment 2}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\vfill

\end{titlepage}


\HRule

\input{general_info.tex}

% General Knitr options
% (this cannot be input since the file runs knitr before LaTeX)
<<echo=FALSE,eval=TRUE>>=
options(continue="  ", prompt="> ")
knitr::opts_chunk$set(size = "small")
@

\HRule

\newpage

\tableofcontents

\newpage

\section{Decision Trees}

The dataset \texttt{Hitters} contain data on the salary of Baseball players and additional information on years in the baseball league (\texttt{Years}) and the number of hits (\texttt{Hits}). To access the data, use:

<<echo=TRUE, eval=TRUE>>=
library(uuml)
data("Hitters")
@

We are now going to build an algorithm to grow decision trees. The algorithm used below is based on the algorithm explained in Chapter 9.2.2 in \citet{hastie2009elements}. To split a tree, see Eq. (9.12-9.14).

\begin{enumerate}
\item Create a test set by setting aside the 30 first observations. Remove those observations from your training set.
<<echo=TRUE, eval=TRUE>>=
# Remove NA values
Hitters <- Hitters[complete.cases(Hitters),]

# Create test and training set
X_test <- Hitters[1:30, c("Years", "Hits")]
y_test <- Hitters[1:30, c("Salary")]
X_train <- Hitters[31:nrow(Hitters), c("Years", "Hits")]
y_train <- Hitters[31:nrow(Hitters), c("Salary")]
@
\item (3p) Implement an R function to split observations binary greedily (ignore \texttt{NA} values). Here we use the \citet[][Eq. 9.12-9.14]{hastie2009elements}, but with the addition that we will keep at least $l$ leafs. The function should take a design matrix \texttt{X}, an \texttt{y} variable and a minimal leaf size \texttt{l}. The function should return a list with the index set of the observations in \texttt{R1}, \texttt{R2}, the split value \texttt{s} and the covariate used for the split \texttt{j}. Below is a rough sketch how you can implement the function.

<<echo=TRUE, eval=FALSE>>=
tree_split <- function(X, y, l){
  checkmate::assert_matrix(X)
  checkmate::assert_numeric(y, len = nrow(X))
  checkmate::assert_int(l)

  # Store the sum of squares
  SS <- matrix(Inf, nrow = nrow(X), ncol = ncol(X))
  for(j in 1:ncol(X)){
    for(k in 1:nrow(X)){
      s <- X[k,j] # Choose split point
      R1 <- which(X[,j] <= s)
      R2 <- which(X[,j] > s)

      # Do if R1 or R2 is smaller than the leaf size l
      # Compute c1
      # Compute c2
      SS[k, j] <- # Compute the sum of squares
    }
  }

  # The final results for the min SS (arg min)
  j <- # Which column was used for the split-point
  s <- # What value of X was used as the split-point
  R1 <- which(X[,j] <= s)
  R2 <- which(X[,j] > s)
  list(j = j,
       s = s,
       R1 = R1,
       R2 = R2,
       SS = min(SS))
}
@

\emph{Hint!} You will need to handle ties (i.e. different observations with different values). This should not be a very large problem. In the example code, I choose the 'first' minima as:
<<echo=TRUE, eval=FALSE>>=
which(min(SS), arr.ind = TRUE)[1,]
@

See below for an example how the function should work.

<<echo = FALSE>>=
tree_split <- function(X,y,l){
  if(l == 5){
  list(j = 1,
       s = 5,
       R1 = c(1,  2,  3,  5, 6, 9, 13, 16, 18, 19),
       R2 = c(4,  7,  8, 10, 11, 12, 14, 15, 17, 20),
       SS = 1346633)
  } else if(l == 1){
      list(j = 2,
       s = 132,
       R1 = c(1,  2,  3,  4, 5,  6,  8,  9, 11, 12, 13, 14, 16, 17, 18, 19),
       R2 = c(7, 10, 15, 20),
       SS = 904383.4)

    }

}
@

<<echo=TRUE, eval=TRUE>>=
# Lets choose a small data to try out our algorithm with
X_check <- Hitters[31:50, c("Years", "Hits")]
y_check <- Hitters[31:50, c("Salary")]
# These are the names of the players we look at
rownames(Hitters)[31:50]
# This is how it should work
tree_split(X_check, y_check, l = 5)
# We can also make a split without any limit on leaf size
tree_split(X_check, y_check, l = 1)
@
\item (1p) What is the first split based on the whole training data \texttt{X\_train} and \texttt{y\_train}?
\item (1p) What is the Sum of Squares (SS) for this first split?
\item (3p) Use the function \texttt{tree\_split()} to create a function \texttt{grow\_tree()} that takes the arguments \texttt{X}, \texttt{y}, and \texttt{l} and then build a decision tree. The returned tree can be a \texttt{data.frame} that looks as below. This is just one example implementation, you are free to implement this however you want.

\emph{Hint 1!} Use a list in R to store the set of indecies you are splitting up.

\emph{Hint 2!} Remember that the indecies that \texttt{split\_tree()} returns are based on the data you put into that functions. Hence, you need to convert them to the full data indecies to simplify.

Here is an template how the function could be implemented, but you are free to implement it however you want.

<<echo = TRUE, eval=FALSE>>=
grow_tree <- function(X, y, l){
  checkmate::assert_matrix(X)
  checkmate::assert_numeric(y, len = nrow(X))
  checkmate::assert_int(l)

  # We do an initial split
  init <- tree_split(X, y, l)
  # We use S_m to store the set of observation indicies
  # This is used to keep track of the groups of
  # observations to be split
  S_m <- list(init$R1, init$R2)
  # Storing the results
  # j = column to use for split
  # s = split point
  # R1_i = pointer to row where to go next if in R1
  # R2_i = pointer to row where to go next if in R2
  # gamma = the gamma value of the leaf (if j and s are NA)
  results <- data.frame(j = init$j,
                        s = init$s,
                        R1_i = -1,
                        R2_i = -1,
                        gamma = NA)
  while (length(S_m) > 0) {
    # As long as not all parts of the tree has been handled
    # we will either split or compute lambda
    if(length(S_m[[1]]) >= 2*l){

      # Do stuff here to grow the tree
      # based on observations (indecies) in S_m[[1]]

      # Add split point
      new_results <- data.frame(j = j,
                                s = s,
                                R1_i = -1,
                                R2_i = -1,
                                gamma = NA)
      results <- rbind(results,
                       new_results)
      # Add R1 and R2 to S_m
      S_m <- c(S_m, R1, R2)
      # Remove the set we just handled
      S_m[[1]] <- NULL
    } else {

      # Do stuff here to when you cant grow the tree (i.e. compute gamma)

      # Add leaf
      new_results <- data.frame(j = NA,
                                s = NA,
                                R1_i = NA,
                                R2_i = NA,
                                gamma = gamma)
      results <- rbind(results,
                       new_results)

      # Remove the set we just handled
      S_m[[1]] <- NULL
    }

  }
  return(results)
}
@

Here is an example how it should work.

<<echo = FALSE>>=
grow_tree <- function(X,y,l){
  data.frame(j = c(1, 1, 2, NA, NA, NA, NA),
             s = c(5, 3, 101, NA, NA, NA, NA),
             R1_i = c(2, 4, 6, NA, NA, NA, NA),
             R2_i = c(3, 5, 7,  NA, NA, NA, NA),
             gamma = c(NA, NA, NA, 106.5, 244.5, 509.3, 946))

}
@

<<echo=TRUE, eval=TRUE>>=
tr <- grow_tree(X_check, y_check, l = 5)
tr
@
\item (3p) Finally implement a function \texttt{predict\_with\_tree(new\_data, tree)}. Here is a starting template for the function.
<<echo = TRUE, eval=FALSE>>=
predict_with_tree <- function(new_data, tree){
  checkmate::assert_matrix(new_data)

  predictions <- numeric(nrow(new_data))

  for(i in seq_along(predictions)){
    not_in_leaf <- TRUE
    while(not_in_leaf){

      # Check the split and go to next row
      # until we end up with a gamma

    }
    predictions[i] <- predicted_value
  }
  return(predictions)
}
@

To use the tree to predict new observations $X_{new}$ as follows.

<<echo = FALSE>>=
predict_with_tree <- function(new_data, tree){
  c(106.5, 244.5)
}
@

<<echo=TRUE,eval=TRUE>>=
X_new <- Hitters[51:52, c("Years", "Hits")]
X_new
y_new <- Hitters[51:52, c("Salary")]
y_new
pred_y <- predict_with_tree(new_data = X_new, tree = tr)
pred_y
@
\item (1p) The function \texttt{rmse(x,y)} in the \texttt{uuml} R package computes the root mean squared error between \texttt{x} and \texttt{y}.
<<echo = TRUE>>=
rmse
@

<<echo=TRUE,eval=TRUE>>=
rmse(x = c(75, 105, 33), y = c(102, 99, 43))
@
What is the root mean squared error (RMSE) on the test set for a tree trained on the whole training data \texttt{X\_train} and \texttt{y\_train}?
\end{enumerate}

\newpage

\section{Running standard tools for boosting and random forests}

The last part of the assignment consists of using Random Forest and XGBoost (Boosted regression trees). The data is taken in slightly different ways:

\begin{enumerate}
\item Use the \texttt{randomForest} R package to fit a random forest regression to the training data using the \texttt{randomForest} function.
<<echo=TRUE,eval=FALSE>>=
library(uuml)
data("Hitters")
Hitters <- Hitters[complete.cases(Hitters),]
dat_test <- Hitters[1:30, c("Salary","Years", "Hits")]
dat_train <- Hitters[31:nrow(Hitters), c("Salary","Years", "Hits")]
Hitters.rf <- randomForest(Salary ~ Years + Hits, data = dat_train)
@
\item (1p) How many variables are used at each split. Why do you get this number?
\item (1p) Use the trained random forest to predict using the \texttt{predict} function at test set and compute the RMSE of your predictions using the \texttt{rmse} function.
\item (1p) Next, use the \texttt{xgboost} R package to fit a boosted regression tree model to the training data using the \texttt{xgboost} function.
<<echo=TRUE,eval=FALSE>>=
X_test <- dat_test[, c("Years", "Hits")]
y_test <- dat_test[, c("Salary")]
X_train <- dat_train[, c("Years", "Hits")]
y_train <- dat_train[, c("Salary")]
xgb <- xgboost(as.matrix(X_train), as.matrix(y_train), nrounds = 200)
@
\item (1p) What is the RMSE of the predictions (again using the \texttt{rmse} function) using the boosted regression tree model?
\item (1p) How does that compare to the random forest regression model?
\end{enumerate}

\newpage

\section{*Bagged tree and random forest implementation}

\input{distinction_task.tex}

Now we can use our regression tree to do a Bagged Tree regression model. The main difference from the previous tree is that we now will draw $B$ bootstrap samples of size $N$ \emph{with replacement} from our dataset, then train $B$ trees, use them for a combined bagged prediction.

See Section 8.7 in \citet{hastie2009elements}, and especially Eq. (8.51) for details.

\begin{enumerate}
\item (1p) Implement a bagged tree regression model training function called \texttt{train\_bagged\_trees(X, y, l, B)} using the \texttt{grow\_tree()} function that return an R list of $B$ grown trees.
\item (1p) Use \texttt{predict\_with\_tree()} to create a function \texttt{predict\_with\_bagged\_trees()} that takes a list of grown trees and produce one prediction per observation.
\item (1p) Train your bagged tree regression model on the training data and predict on the test data. Try the values $B=\{1, 10, 100, 500, 1000\}$. What is the RMSE of your predictions on the test set? \emph{Hint!} See Figure 15.1 in \citet{hastie2009elements}.
\end{enumerate}


%  \section{*Random forest implementation}

% \input{distinction_task.tex}

Using our previous functions we can now build our own random forest regression model using our previous functions \texttt{grow\_tree()} and \texttt{predict\_with\_tree()}. Based on the bagged tree regression model, we only need to add the parameter $m$.

See Section 15.2 in \citet{hastie2009elements}, and especially Algorithm 15.1 for details.

\begin{enumerate}
\item (1p) Create a new function \texttt{grow\_tree\_m()} with the additional argument $m$. The function should grow a tree, but at every split, it should draw a sample of size $m$ of the covariates to make the split.
\item (1p) Implement a random forest regression model using \texttt{grow\_tree\_m()} and \texttt{predict\_with\_tree()}.
\item (1p) Train your random forest regression model on the training data with $m=1$, $B=100$ and predict on the test data. What is the RMSE of your predictions on the test set (using your \texttt{rmse} function)?

\emph{Note!} Since we only have just two covariates in the test and train data, the difference is small (if any) compared to the Bagging algorithm. Also, differences compared to the \texttt{randomForest} package probably comes from different way the \texttt{grow\_tree} functions are implemented in your code and the \texttt{randomForest} package.
\end{enumerate}

\newpage

\bibliography{bibliography}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
