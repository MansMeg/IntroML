% !Rnw weave = knitr

\documentclass[11pt,a4paper,english]{article}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{listings}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Normal latex T1-fonts
\usepackage[T1]{fontenc}

% Figures
%\usepackage{graphicx,fancybox,subfigure}

 % AMS-stuff
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}

 % Misc
\usepackage{verbatim}
\usepackage{url}

\usepackage[round]{natbib}
\bibliographystyle{unsrtnat}

% Page size
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2cm}

% Paragraph
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% Horizontal line
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% No page numbers
\pagestyle{empty}

% New commands:
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\begin{document}

\begin{titlepage}

\center
\textsc{\LARGE Uppsala University}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.2]{files/uu_logo.png}\\[1cm]
\textsc{\Large Introduction to Machine Learning, Big Data, and AI}\\[0.5cm]
\textsc{\large }\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Assignment 3}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\vfill

\end{titlepage}


\HRule

\input{general_info.tex}

% General Knitr options
% (this cannot be input since the file runs knitr before LaTeX)
<<echo=FALSE,eval=TRUE>>=
options(continue="  ", prompt="> ")
knitr::opts_chunk$set(size = "small")
@



\HRule

\newpage

\tableofcontents

\newpage

% \section{General Questions}

% \input{general_questions_info.tex}

% \begin{enumerate}
%\item Describe the output layer in a neural network. What does it do? (max 1 paragraph)
% \item Describe the input layer in a neural network. What does it do? (max 1 paragraph)
%\item What is a tensor? (max 1 paragraph)
%\item What is the benefit of using (negative) log-likelihood as loss functions according to \citet{Goodfellow-et-al-2016}? (max 1 paragraph)
%\item What are the benefits with deeper neural networks compared to shallow ones according to \citet{Goodfellow-et-al-2016}? (max 1 paragraph)
%\item Describe early stopping regularization (in 1 paragraph).
%\item Describe weight decay regularization (max 1 paragraph).
% \item Describe dropout regularization (max 1 paragraph).
%\item Describe the relationship between bagging and dropout regularization (max 1 paragraph).
% \end{enumerate}

\section{Feed-Forward Neural Network using Keras and TensorFlow}

As a first step, you must install \texttt{tensorflow} and \texttt{keras} on your local computer. You can find detailed information on how to install \texttt{tensorflow} and \texttt{keras} \textbf{\href{https://tensorflow.rstudio.com/installation/}{[here]}}. You can find details on different architecture components in the \texttt{keras} reference found \textbf{\href{https://keras.rstudio.com/index.html}{[here]}}.

\emph{Note!} Running Keras can be computationally heavy, and I suggest not running the code in \texttt{markdown}. Instead, run the code in R and copy the results as output (see the assignment template for an example).

If you have installed \texttt{keras}, you can load the MNIST dataset. This dataset contains data on handwritten digits that we want to classify. To access the data, we use \texttt{keras} as follows.

<<echo=TRUE,eval=FALSE>>=
library(keras)
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
@


\begin{enumerate}
\item As a first step, we visualize a couple of digits. Include the first eight you can find in the training dataset in the report as an image.
<<echo=TRUE,eval=FALSE>>=
idx <- 3
im <- mnist$train$x[idx,,]
# Transpose the image
im <- t(apply(im, 2, rev))
image(1:28, 1:28, im, col=gray((0:255)/255), xlab = "", ylab = "",
        xaxt='n', yaxt='n', main=paste(mnist$train$y[idx]))
@
\item How large is the training and test set?
\item Implement a simple feed-forward neural network in R using TensorFlow and Keras. See the introduction \href{https://tensorflow.rstudio.com/tutorials/beginners/}{\textbf{[here]}} or \href{https://cran.r-project.org/web/packages/keras/vignettes/index.html}{\textbf{[here]}}. Implement a neural network with one hidden layer with 16 units and the sigmoid as the activation function for the hidden layer. The output layer should be a softmax. Print your \texttt{keras} model description and include it in your report.
\begin{enumerate}
\item How many parameters do your model have in total?
\item How many parameters does the input layer have?
\item How many parameters does the output layer have?
\item What do you get as classification accuracy? Include the validation accuracy and error per epoch figure/plot that is automatically generated.

\emph{Hint!} The classification accuracy should be around 0.85.

\end{enumerate}

\item Now, we will play around with the essential concepts in feed-forward networks. Do the steps below, but feel free to tinker around and test other values. Try to understand the effect of the different choices. Report the \emph{validation} set accuracy/error and the validation accuracy/error per epoch for each step in a figure/plot together with the \texttt{keras} model description for the \emph{final} neural network with all changes. Start from the simple neural network above, then make the following changes.
\begin{enumerate}
\item Increase the number of hidden units to 128.
\item Change the activation function to reLU.
\item Change the optimizer to RMSprop.
\item Add a second layer with 128 hidden units.
\item Add dropout with 0.2 dropout probability for both of your hidden layers.
\item Add batch normalization for both of your hidden layers.
\end{enumerate}
\item If you would use early stopping regularization, how would you do that?

\emph{Note!} Only describe what you would do conceptually. You don't need to do it.
\item Now try to build the best possible network architecture for this data (but still only using a feed-forward network). How good can you get validation set accuracy (or how low is the error)? What is the architecture used for the best model you can get? Include the final \texttt{keras} code, and the \emph{validation} set accuracy figure. Feel free to use early stopping!

\emph{Hint!} The general idea is to find the optimal capacity of the model. Try to start with a network that overfits the data and then regularize that model to improve the generalization error on the validation set.

\item Identify two digits that your best network has classified incorrectly. Visualize them and include the digit number and what your network classified it as. Why do you think the network made the classification mistake?

\item Use your model to compute the accuracy and loss on the MNIST hold-out test set included with the data. If you get a lower accuracy (or higher error) than on the validation set (or higher error), why is this the case?

\end{enumerate}


\newpage

\section{* A simple neural network from scratch}

\input{distinction_task.tex}

We want to implement one of the most straightforward neural networks possible.

\begin{enumerate}
\item Implement the neural network in \citet[][Ch. 6.1]{Goodfellow-et-al-2016} as a function that takes as an input a $2 \times 2$ weight matrix \texttt{W}, a vector \texttt{c}, a vector \texttt{w} and the scalar $b$. It should work as in Ch. 6.1., here is an example.
<<echo = FALSE>>=
mini_net <- function(X, W, c, w, b){
    matrix(c(0,1,1,0))
}
@

<<echo=TRUE, eval=TRUE>>=
W <- matrix(1, nrow = 2, ncol = 2)
c <- matrix(c(0, -1), ncol = 2, nrow = 4, byrow = TRUE)
X <- matrix(c(0,0,1,1,0,1,0,1), ncol = 2)
w <- matrix(c(1, -2), ncol = 1)
b <- 0

mini_net(X, W, c, w, b)
@

<<echo = FALSE>>=
mini_net <- function(X, W, c, w, b){
    matrix(c(0,0.9,0.9,0.2))
}
@
<<echo=TRUE,eval = TRUE>>=
mini_net(X, W*0.9, c, w, b)
@

\item Now test changing the value $W_{1,1}$ to 0. What do you get for the results of the network?
\item What is the current output function? Is the network output function reasonable? Do you have any other output function that might be better?
\item Now implement a mean squared error loss function for the network. It should work as follows.
<<echo = FALSE>>=
mini_net_loss <- function(y, X, W, c, w, b){
    return(0)
}
@
<<eval = TRUE>>=
y <- c(0,1,1,0)
mini_net_loss(y, X, W, c, w, b)
@

<<echo = FALSE>>=
mini_net_loss <- function(y, X, W, c, w, b){
    return(0.015)
}
@
<<eval = TRUE>>=
mini_net_loss(y, X, 0.9*W, c, w, b)
@

\item Again, change the value $W_{1,1}$ to 0. What is the value of the loss function?
%\item Now we are going to train our mini network using gradient descent. The gradient for the ReLU function is
%\[
%\frac{\partial \text{ReLU}(x)}{\partial x} =
%\]
%Use this to implement the gradient for $\mathcal{W}$ (we keep the other parameters fixed). Remember the chain rule of calculus (see Chapter XX). You can derive this analytically for each of the four values of W.
%\item initialize values W using the distribution in XXX eq. 1. What is the initialization distribution.
%\item use batch/full gradient descent to train your mini network to find roughly the same parameter values as above. Visualize the loss per epoch/iteration as you did in assignment 1.
\end{enumerate}


\newpage
\bibliography{bibliography}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:


