% !Rnw weave = knitr

\documentclass[11pt,a4paper,english]{article}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{listings}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Normal latex T1-fonts
\usepackage[T1]{fontenc}

% Figures
%\usepackage{graphicx,fancybox,subfigure}

 % AMS-stuff
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}

 % Misc
\usepackage{verbatim}
\usepackage{url}

\usepackage[round]{natbib}
\bibliographystyle{unsrtnat}

% Page size
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2cm}

% Paragraph
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% Horizontal line
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% No page numbers
\pagestyle{empty}

% New commands:
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\begin{document}

\begin{titlepage}

\center
\textsc{\LARGE Uppsala University}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.2]{files/uu_logo.png}\\[1cm]
\textsc{\Large Introduction to Machine Learning, Big Data, and AI}\\[0.5cm]
\textsc{\large }\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Assignment 3}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\vfill

\end{titlepage}


\HRule

\input{general_info.tex}

% General Knitr options
% (this cannot be input since the file runs knitr before LaTeX)
<<echo=FALSE,eval=TRUE>>=
options(continue="  ", prompt="> ")
knitr::opts_chunk$set(size = "small")
@



\HRule

\newpage

\tableofcontents

\newpage

\section{General Questions}

\input{general_questions_info.tex}

\begin{enumerate}
\item Describe the output layer in a neural network. What does it do? (max 1 paragraph)
% \item Describe the input layer in a neural network. What does it do? (max 1 paragraph)
%\item What is a tensor? (max 1 paragraph)
\item What is the benefit of using (negative) log-likelihood as loss functions according to \citet{Goodfellow-et-al-2016}? (max 1 paragraph)
%\item What are the benefits with deeper neural networks compared to shallow ones according to \citet{Goodfellow-et-al-2016}? (max 1 paragraph)
\item Describe early stopping regularization (max 1 paragraph).
%\item Describe weight decay regularization (max 1 paragraph).
%\item Describe dropout regularization (max 1 paragraph).
\item Describe the relationship between bagging and dropout regularization (max 1 paragraph).
\end{enumerate}

\section{Feed-Forward Neural Network using Keras and TensorFlow}

As a first step, you need to install  \texttt{tensorflow} and \texttt{keras} on your local computer. You can find detailed information on how to install \texttt{tensorflow} and \texttt{keras} \textbf{\href{https://tensorflow.rstudio.com/installation/}{[here]}}. Details on different architecture components can be found in the \texttt{keras} reference found \textbf{\href{https://keras.rstudio.com/index.html}{[here]}}.

\emph{Note!} Running Keras can be computationally heavy, and I suggest not to run the code in \texttt{markdown}. Instead, run the code in R and copy the results as output (see the assignment template for an example).

If you have installed \texttt{keras}, you can load the MNIST dataset. This dataset contains data on handwritten digits that we want to classify. To access the data, we use \texttt{keras} as follows.

<<echo=TRUE,eval=FALSE>>=
library(keras)
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
@


\begin{enumerate}
\item As a first step, we visualize a couple of digits. Below is an example.
<<echo=TRUE,eval=FALSE>>=
idx <- 3
im <- mnist$train$x[idx,,]
# Transpose the image
im <- t(apply(im, 2, rev))
image(1:28, 1:28, im, col=gray((0:255)/255), xlab = "", ylab = "",
        xaxt='n', yaxt='n', main=paste(mnist$train$y[idx]))
@
\item How large is the training and test set?
\item Implement a feed-forward neural network in R using TensorFlow and Keras. Start by following the introduction \href{https://tensorflow.rstudio.com/tutorials/beginners/}{here}. Start with one hidden layer with 16 units and the sigmoid as the activation function. Do not use any regularization for now.
\begin{enumerate}
\item Print your \texttt{keras} model and include it in your report.
\item How many parameters do your model have?
\item What is the input layer in the model? How many parameters does the input layer have?
\item What is the output layer in the model? How many parameters does the output layer have?
\item What do you get as classification accuracy? Include the validation accuracy or error per epoch figure/plot that is automatically generated. \emph{Hint!} The classification accuracy should be around 0.85. \emph{Hint!} In multi-class classification, if the model classifies a digit to any other class than the valid class, it is a false negative or a false positive. I.e. we can still use the same definition for accuracy, precision and recall as in the binary case in multi-class classification.
\end{enumerate}
\item Now, we are going to play around with the essential concepts in feed-forward networks. Do the steps below, and report the \emph{validation} set accuracy/error and the validation accuracy/error per epoch for each step in a figure/plot. Please change this sequentially so you build a more and more complex network.
\begin{enumerate}
\item Increase the number of hidden units to 128.
\item Change the activation function to reLU.
\item Change the optimizer to RMSprop.
\item Try to run the net for ten epochs. If you would use early stopping for regularization, what would you do? \emph{Note!} You only need to describe what you would do, not do it.
\item Add a second layer with 128 hidden units. How many parameters do you have in the net right now?
\item Add dropout with 0.2 and 0.5 dropout probability. You can choose what layers you would like to introduce the dropout.
\item Add batch normalization. You can choose where you want to add it.
\end{enumerate}
\item Now try to improve the network architecture yourself (but still only use a feed-forward network). You can either start with a new network from scrach or build upon the network from the previous assignment. How good validation set accuracy (or how low error) can you get? What is the architecture used for the best model you can get?
\item Identify two digits that the in the best network in the previous task has classified incorrectly. Visualize them. Why do you think the network made the classification mistake?
\item Use your model to compute the accuracy, precision and recall on the hold-out test set included with the MNIST data. If you get a lower accuracy (or higher error) than on the validation set (or higher error), why is this the case?
\end{enumerate}

\section{* A simple neural network from scratch}

\input{distinction_task.tex}

We want to implement one of the most straightforward neural networks possible.

\begin{enumerate}
\item Implement the neural network in \citet[][Ch. 6.1]{Goodfellow-et-al-2016} as a function that takes as an input a $2 \times 2$ weight matrix \texttt{W}, a vector \texttt{c}, a vector \texttt{w} and the scalar $b$. It should work as in Ch. 6.1. Here is an example of how it should work.
<<echo = FALSE>>=
mini_net <- function(X, W, c, w, b){
    matrix(c(0,1,1,0))
}
@

<<echo=TRUE, eval=TRUE>>=
W <- matrix(1, nrow = 2, ncol = 2)
c <- matrix(c(0, -1), ncol = 2, nrow = 4, byrow = TRUE)
X <- matrix(c(0,0,1,1,0,1,0,1), ncol = 2)
w <- matrix(c(1, -2), ncol = 1)
b <- 0

mini_net(X, W, c, w, b)
@

<<echo = FALSE>>=
mini_net <- function(X, W, c, w, b){
    matrix(c(0,0.9,0.9,0.2))
}
@
<<echo=TRUE,eval = TRUE>>=
mini_net(X, W*0.9, c, w, b)
@

\item Now test changing the value $W_{1,1}$ to 0. What do you get for the results of the network?
\item What is the current output function? Is the network output function reasonable? Do you have any other output function that might be better?
\item Now implement a mean squared error loss function for the network. It should work as follows.
<<echo = FALSE>>=
mini_net_loss <- function(y, X, W, c, w, b){
    return(0)
}
@
<<eval = TRUE>>=
y <- c(0,1,1,0)
mini_net_loss(y, X, W, c, w, b)
@

<<echo = FALSE>>=
mini_net_loss <- function(y, X, W, c, w, b){
    return(0.015)
}
@
<<eval = TRUE>>=
mini_net_loss(y, X, 0.9*W, c, w, b)
@

\item Again, change the value $W_{1,1}$ to 0. What is the value of the loss function?
\end{enumerate}

\newpage
\bibliography{bibliography}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
