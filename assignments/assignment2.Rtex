\documentclass[11pt,a4paper,english]{article}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{listings}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Normal latex T1-fonts
\usepackage[T1]{fontenc}

% Figures
%\usepackage{graphicx,fancybox,subfigure}

 % AMS-stuff
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}

 % Misc
\usepackage{verbatim}
\usepackage{url}

% Page size
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2cm}

% Paragraph
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% Horizontal line
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% No page numbers
\pagestyle{empty}

% New commands:
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\begin{document}

\begin{titlepage}

\center
\textsc{\LARGE Uppsala University}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.2]{files/uu_logo.png}\\[1cm]
\textsc{\Large Introduction to Machine Learning, Big Data, and AI}\\[0.5cm]
\textsc{\large }\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Assignment 2}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\vfill

\end{titlepage}


\HRule

\input{general_info.tex}

% General Knitr options
% (this cannot be input since the file runs knitr before LaTeX)
<<echo=FALSE,eval=TRUE>>=
options(continue="  ", prompt="> ")
knitr::opts_chunk$set(size = "small")
@

\HRule

\newpage

\section{General Questions}

\begin{enumerate}
\item Describe bagging and boosting with your own words, whats the difference between the two approaches? (1-2 paragraphs)
\item When (and why) is random forests performing poorly with a small $m$? (max 1 paragraph)
\item What is a learning ensamble? (max 1 paragraph)
\end{enumerate}

\section{Decision Trees}

The dataset \texttt{Hitters} contain data on the salary of Baseball players and additional information on years in the baseball league (\texttt{Years}) and the number of hits (\texttt{Hits}). To access the data, use

<<echo=TRUE, eval=TRUE>>=
library(uuml)
data("Hitters")
@

We are now going to build an algorithm to grow decision trees. The algorithm used below is based on the algorithm explained in Chapter 9.2.2 in ESL. To split a tree, see Eq. (9.13) in ESL.

\begin{enumerate}
\item Create a test set by setting aside the 30 first observations. Remove those observations from your training set.
<<echo=TRUE, eval=TRUE>>=
# Remove NA values
Hitters <- Hitters[complete.cases(Hitters),]

# Create test and training set
X_test <- Hitters[1:30, c("Years", "Hits")]
y_test <- Hitters[1:30, c("Salary")]
X_train <- Hitters[31:nrow(Hitters), c("Years", "Hits")]
y_train <- Hitters[31:nrow(Hitters), c("Salary")]
@
\item Implement an R function to split observations binary greedily (ignore \texttt{NA} values). Here we use the algorithm in ESL. The function should take a design matrix \texttt{X}, an \texttt{y} variable and a minimal leaf size \texttt{l}. The function should return a list with the index set of the observations in \texttt{R1}, \texttt{R2}, the split value \texttt{s} and the covariate used for the split \texttt{j}. See below for an example.

<<echo = FALSE>>=
tree_split <- function(X,y,l){
  if(l == 5){
  list(j = 1,
       s = 5,
       R1 = c(1,  2,  3,  5,  9, 13, 16, 18, 19),
       R2 = c(4,  6, 7,  8, 10, 11, 12, 14, 15, 17, 20),
       SS = 1346633)
  } else if(l == 1){
      list(j = 2,
       s = 139,
       R1 = c(1,  2,  3,  4,  6,  8,  9, 11, 12, 13, 14, 16, 17, 18, 19),
       R2 = c(5, 7, 10, 15, 20),
       SS = 904383.4)
    }

}
@

<<echo=TRUE, eval=TRUE>>=
# Lets choose a small data to try out our algorithm with
X_check <- Hitters[31:50, c("Years", "Hits")]
y_check <- Hitters[31:50, c("Salary")]
# These are the names of the players we look at
rownames(Hitters)[31:50]
# This is how it should work
tree_split(X_check, y_check, l = 5)
# We can also make a split without any limit on leaf size
tree_split(X_check, y_check, l = 1)
@
\item What is the first split based on the whole training data \texttt{X\_train} and \texttt{y\_train}?
\item What is the Sum of Squares (SS) for this first split?
\item Use the function \texttt{tree\_split()} to create a function \texttt{grow\_tree()} that takes the arguments \texttt{X}, \texttt{y}, and \texttt{l} and then build a decision tree. The returned tree can be a \texttt{data.frame} that looks as follows. Note that \texttt{R1\_i} and \texttt{R2\_i} indicates the row of the \texttt{data.frame} where to go next. This is just one example implementation, you are free to implement this however you want. \emph{Hint!} Use a list in R to store the set of indecies you are splitting up.

<<echo = FALSE>>=
grow_tree <- function(X,y,l){
  data.frame(j = c(1, NA, 2, NA, NA),
             s = c(5, 0, 101, NA, NA),
             R1_i = c(2, NA, 4, NA, NA),
             R2_i = c(3, NA, 5,  NA, NA),
             gamma = c(NA, 167.5, NA, 558.125, 358.2143))

}
@

<<echo=TRUE, eval=TRUE>>=
tr <- grow_tree(X_check, y_check, l = 5)
tr
@
\item Finally implement a function \texttt{predict\_with\_tree(new\_data, tree)} to use the tree to predict new observations $X_{new}$ as follows:
<<echo = FALSE>>=
predict_with_tree <- function(new_data, tree){
  c(167.5, 167.5)
}
@

<<echo=TRUE,eval=TRUE>>=
X_new <- Hitters[51:52, c("Years", "Hits")]
X_new
y_new <- Hitters[51:52, c("Salary")]
y_new
pred_y <- predict_with_tree(new_data = X_new, tree = tr)
pred_y
@
\item Implement a function \texttt{rmse(x,y)} that computes the root mean squared error between \texttt{x} and \texttt{y}. It should work like the following
<<echo = FALSE>>=
rmse <- function(x, y){
  78.938
}
@

<<echo=TRUE,eval=TRUE>>=
rmse(y_new, pred_y)
@
\item What is the root mean squared error on the test set for a tree trained on the whole training data \texttt{X\_train} and \texttt{y\_train}?
\end{enumerate}


\section{Running standard tools for boosting and random forests}

The last part of the assignment consists of using Random Forest and XGBoost (Boosted regression trees). The data is taken in slightly different ways:

\begin{enumerate}
\item Use the \texttt{randomForest} R package to fit a random forest regression to the training data using the \texttt{randomForest} function.
<<echo=TRUE,eval=FALSE>>=
library(uuml)
data("Hitters")
Hitters <- Hitters[complete.cases(Hitters),]
dat_test <- Hitters[1:30, c("Salary","Years", "Hits")]
dat_train <- Hitters[31:nrow(Hitters), c("Salary","Years", "Hits")]
Hitters.rf <- randomForest(Salary ~ Years + Hits, data = dat_train)
@
\item How many variables are used at each split. Why do you get this number?
\item Use the trained random forest to predict using the \texttt{predict} function at test set and compute the RMSE of your predictions using your \texttt{rmse} function.
\item Next, use the \texttt{xgboost} R package to fit a boosted regression tree model to the training data using the \texttt{xgboost} function.
<<echo=TRUE,eval=FALSE>>=
X_test <- dat_test[1:30, c("Years", "Hits")]
y_test <- dat_test[1:30, c("Salary")]
X_train <- dat_train[31:nrow(Hitters), c("Years", "Hits")]
y_train <- dat_train[31:nrow(Hitters), c("Salary")]
xgb <- xgboost(as.matrix(X_train), as.matrix(y_train), nrounds = 200)
@
\item What is the RMSE of the predictions (again using your \texttt{rmse} function) using the boosted regression tree model?
\item How does that compare to the random forest regression model?
\end{enumerate}


\section{*Bagged tree implementation}

This assignment is a VG point assignment. If you do not want to get a VG-point, you can ignore this assignment.

Now we can use our regression tree to do a Bagged Tree regression model. The main difference from the previous tree is that we now will draw $B$ bootstrap samples of size $N$ \emph{with replacement} from our dataset, then train $B$ trees, use them for a combined bagged prediction.

See Section 8.7 in ESL, and especially Eq. 8.51 for details.

\begin{enumerate}
\item Implement a bagged tree regression model training function called \texttt{train\_bagged\_trees(X, y, l, B)} using the \texttt{grow\_tree()} function that return an R list of $B$ grown trees.
\item Use \texttt{predict\_with\_tree()} to create a function \texttt{predict\_with\_bagged\_trees()} that takes a list of grown trees and produce one prediction per observation.
\item Train your bagged tree regression model on the training data and predict on the test data. Try the values $B=\{100,500,1000\}$. What is the RMSE (using your \texttt{rmse} function) of your predictions on the test set?
\end{enumerate}


\section{*Random forest implementation}

This assignment is a VG point assignment. If you do not want to get a VG-point, you can ignore this assignment.

Using our previous functions we can now build our own random forest regression model using our previous functions \texttt{grow\_tree()} and \texttt{predict\_with\_tree()}. Based on the bagged tree regression model, we only need to add the parameter $m$.

See Section 15.2 in ESL, and especially Alg. 15.1 for details.

\begin{enumerate}
\item Create a new function \texttt{grow\_tree\_m()} with the additional argument $m$. The function should grow a tree, but at every split, it should draw a sample of size $m$ of the covariates to make the split.
\item Implement a random forest regression model using \texttt{grow\_tree\_m()} and \texttt{predict\_with\_tree()}.
\item Train your random forest regression model on the training data with $m=1$, $B=100$ and predict on the test data. What is the RMSE of your predictions on the test set (using your \texttt{rmse} function)? \emph{Note!} Since we only have just two covariates in the test and train data, the difference is small (if any).
\end{enumerate}




\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
