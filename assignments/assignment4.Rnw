% !Rnw weave = knitr

\documentclass[11pt,a4paper,english]{article}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{listings}

\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% Normal latex T1-fonts
\usepackage[T1]{fontenc}

% Figures
%\usepackage{graphicx,fancybox,subfigure}

 % AMS-stuff
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}

 % Misc
\usepackage{verbatim}
\usepackage{url}

\usepackage[round]{natbib}
\bibliographystyle{unsrtnat}

% Page size
\addtolength{\hoffset}{-1cm}
\addtolength{\textwidth}{2cm}
\addtolength{\voffset}{-1cm}
\addtolength{\textheight}{2cm}

% Paragraph
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}

% Horizontal line
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

% No page numbers
\pagestyle{empty}

% New commands:
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}

\begin{document}

\begin{titlepage}

\center
\textsc{\LARGE Uppsala University}\\[1.5cm] % Name of your university/college
\includegraphics[scale=.2]{files/uu_logo.png}\\[1cm]
\textsc{\Large Introduction to Machine Learning, Big Data, and AI}\\[0.5cm]
\textsc{\large }\\[0.5cm]

\HRule \\[0.4cm]
{ \huge \bfseries Assignment 4}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

\vfill

\end{titlepage}


\HRule

\input{general_info.tex}

% General Knitr options
% (this cannot be input since the file runs knitr before LaTeX)
<<echo=FALSE,eval=TRUE>>=
options(continue="  ", prompt="> ")
knitr::opts_chunk$set(size = "small")
@

\HRule

\newpage

\tableofcontents

\newpage

\section{General Questions}

\input{general_questions_info.tex}

\begin{enumerate}
\item What is a convolutional kernel/feature map? (max 1 paragraph)
%\item Describe how cross-correlation is related to a convolution with your own words. (max 1 paragraph)
%\item Describe the purpose of a pooling layer in a convolutional architecture. (max 1 paragraph)
\item Reason about when to gather more data in an ML application? (1-2 paragraphs)
%\item What hyperparameter does \citet{Goodfellow-et-al-2016} find to be the most important to tune, and why? (max 1 paragraph)
%\item What automatic hyperparameter search is recommended by \citet{Goodfellow-et-al-2016}, grid search or random search. Why? (max 1 paragraph)
\item Describe a good debugging test for an ML algorithm. (max 1 paragraph)
%\item Describe data augmentation in the context of image data and convnets. (max 1 paragraph)
\item Describe why we primarily focus on fine-tuning layers higher up in neural networks when fine-tuning large pre-trained neural networks. (1-2 paragraphs)
\end{enumerate}.

\section{Implementing a convolutional layer}

As a first step, we will implement a layer of a convolutional neural network with one filter. We will here not train the layer, only implement it to understand the inner workings. You are not allowed to use \texttt{convolve()} in R in the final solution, but you can use it to debug your code if you want to.

Some good material for this exercise is Figure 9.1 in \citet{Goodfellow-et-al-2016} and the video \citet{ng2017}.

Start by importing examples from the MNIST dataset as follows.

<<eval = TRUE>>=
library(uuml)
data("mnist_example")
@

To visualize an image use:

<<eval = FALSE>>=
im <- mnist_example[["5"]]
image(1:ncol(im), 1:nrow(im), im,
      xlab = "", ylab = "",
      xaxt='n', yaxt='n', main="4")
@

<<echo = FALSE>>=
convolution <- function(X,K){
  matrix(c(56, 490, 260,
           0,  438, 294,
           0,  341, 391), byrow = TRUE, ncol = 3)
}
convolutional_layer <- function(X,K,b,activation){
      matrix(c(52,  0,
               14, 14), byrow = TRUE, ncol = 2)
}
maxpool_layer <- function(X){
  matrix(c(250, 144,
           198, 241), byrow = TRUE, ncol = 2)
}
@


\begin{enumerate}
\item Visualize the MNIST example digit 4.
\item Implement a convolution function called \texttt{convolution(X, K)} that takes an input MNIST image (\texttt{X}), an arbitrary large square kernel (\texttt{K}) and returns a valid feature map. Below is an example of how it should work.
<<>>=
X <- mnist_example[["4"]][12:15,12:15]
X
K <- matrix(c(1, 1, 0, 0), nrow = 2)
K
convolution(X, K)
@
\item Visualize the feature map of MNIST example digit 4 using the above two by two kernel \texttt{K}.
\item Now implement all steps in a \texttt{convolutional\_layer(X, K, b, activation)} function that takes the kernel, bias and activation function. It should work as follows.
<<>>=
relu <- function(x) max(0, x)
X <- mnist_example[["4"]][12:15,12:15]
K <- matrix(c(1, 1, 1,
              0, 0, 0,
              0, 0, 0), nrow = 3, byrow = TRUE)
convolutional_layer(X, K, -370, relu)
@
\item Run your convolutional layer on MNIST example digit 4 with bias -400. Visualize the feature map as you visualize the original image. What does the filter seem to capture?
\item Now transpose your filter and run your convolutional layer on MNIST example digit 4 with bias -450. Visualize the feature map. What does that transposed filter seem to capture?
\item As the last step in our convolutional layer, implement a two by two, two stride max-pooling layer. It should work as follows.
<<>>=
X <- mnist_example[["4"]][12:15,12:15]
maxpool_layer(X)
@
\item Now put it all together and visualize the final output of your own convolutional layer. Visualize the feature map.
<<eval=FALSE>>=
X <- mnist_example[["4"]]
relu <- function(x) max(0, x)
K <- matrix(c(1, 1, 1,
              0, 0, 0,
              0, 0, 0), nrow = 3, byrow = TRUE)
output <- maxpool_layer(convolutional_layer(X, K, -370, relu))
@

\end{enumerate}




\section{Convolutional neural networks using Keras}

\emph{Note!} This assignment can be a little heavy computationally. If your own an old computer, you might want to run this task in the computer room in the Department.

We are now going to implement a convolutional neural network using Keras. Here Ch. 5.1-5.3 in \citet{chollet2018deep} and the \href{https://tensorflow.rstudio.com/tutorials/advanced/images/cnn/}{following tutorial} might be useful, especially for details on how to load the data. Remember to load the \texttt{tesorflow} R package before loading the \texttt{keras} R package.

\emph{Note!} Running Keras can be computationally heavy, and I suggest not to run the code in \texttt{markdown}. Instead, run the code in R and copy the results as output (see the assignment template for an example).


\begin{enumerate}
\item Implement a Convolutional Neural Network for the MNIST dataset. The network should have two convolutional layers as follows.
<<>>=
## _______________________________________________________________________
## Layer (type)                     Output Shape                Param #
## =======================================================================
## conv2d (Conv2D)                  (None, 26, 26, 32)          320
## _______________________________________________________________________
## max_pooling2d (MaxPooling2D)     (None, 13, 13, 32)          0
## _______________________________________________________________________
## conv2d (Conv2D)                  (None, 11, 11, 32)          9248
## _______________________________________________________________________
## flatten (Flatten)                (None, 3872)                0
## _______________________________________________________________________
## dense (Dense)                    (None, 64)                  247872
## _______________________________________________________________________
## dense (Dense)                    (None, 10)                  650
## =======================================================================
## Total params: 258,090
## Trainable params: 258,090
## Non-trainable params: 0
@
\item Explain why there are 320 parameters in the first layer. How many are kernel weights (and why), and how many biases?
\item Train the network using Keras. What is your loss and accuracy on the MNIST dataset?
\item As the next step, we will implement a similar network for the CIFAR-10 dataset using \texttt{dataset\_cifar10()}. See the \href{https://tensorflow.rstudio.com/tutorials/advanced/images/cnn/}{tutorial} for details on how to load data. Implement a similar CNN as in the tutorial. That is:
<<>>=
## Model: "sequential"
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #
## ===========================================================================
## conv2d (Conv2D)                  (None, 30, 30, 32)            896
## ___________________________________________________________________________
## max_pooling2d (MaxPooling2D)     (None, 15, 15, 32)            0
## ___________________________________________________________________________
## conv2d_1 (Conv2D)                (None, 13, 13, 64)            18496
## ___________________________________________________________________________
## max_pooling2d_1 (MaxPooling2D)   (None, 6, 6, 64)              0
## ___________________________________________________________________________
## conv2d_2 (Conv2D)                (None, 4, 4, 64)              36928
## ___________________________________________________________________________
## flatten (Flatten)                (None, 1024)                  0
## ___________________________________________________________________________
## dense (Dense)                    (None, 64)                    65600
## ___________________________________________________________________________
## dense_1 (Dense)                  (None, 10)                    650
## ===========================================================================
## Total params: 122,570
## Trainable params: 122,570
## Non-trainable params: 0
## ___________________________________________________________________________

@
\emph{Note!} This problem is more complex than the MNIST problem, so don't be surprised if you get lower accuracy than you got on the MNIST dataset.
\item Why do we now have 896 parameters in the first convolutional layer?
\item Try out at least three different networks, describe the networks, why you chose them, and the Keras model output. Reading Ch. 11-11.5 in \citet{Goodfellow-et-al-2016} might give some good inspiration and guidance. For example, you can add a \texttt{dropout\_layer} \citep[see ][, Ch. 5.3]{chollet2018deep}.
\item Can you improve over the previous network concerning accuracy? You can also use regularization techniques and batch normalization from the previous lab.
\end{enumerate}

\section{* Transfer learning using VGG16}

\input{distinction_task.tex}

\emph{Note!} This assignment can be a little heavy computationally. If your own an old computer, you might want to run this task in the computer room in the Department.

\emph{Note!} This problem is more complex than the MNIST problem, so don't be surprised if you get lower accuracy than you got on the MNIST dataset.

As the last step, we will look into using transfer learning as a quick way of improving the prediction accuracy on the cifar-10 dataset. Here Ch. 5.3 in \citet{chollet2018deep} or \href{https://blogs.rstudio.com/ai/posts/2017-12-14-image-classification-on-small-datasets/}{this tutorial} might be helpful.

You can find the current state-of-the-art neural networks for the cifar-10 dataset \href{https://benchmarks.ai/cifar-10}{here.} Feel free to read some of the papers and be inspired on how to improve your neural networks.

\begin{enumerate}
\item Using Keras, download the VGG16 convolutional neural network. Just download the convolutional base. We are going to use the network on the cifar-10 dataset so change the \texttt{input\_size} to \texttt{c(32, 32, 3)}.
\item Now add a dense layer with 64 hidden nodes (as in the previous exercise). Include the Keras model output in the assignment report. How many parameters do you have in the dense top layer (compared to the CNN in the previous part)?
\item Now, freeze the convolutional base and train the dense top layer of your network on the cifar-10 dataset. Run it for five epochs. \emph{Note!} This will take time. Don't be surprised if you need roughly 200s per epoch or more.
\item Report your final accuracy. How does this compare to the previous model for the cifar data?
\end{enumerate}

\newpage
\bibliography{bibliography}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
