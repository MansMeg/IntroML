%\documentclass[10pt,handout]{beamer}
\documentclass[10pt]{beamer}
\usepackage{babel} % Anpassa efter svenska. Ger svensk logga.
\usepackage[utf8]{inputenc} % Anpassa efter linux
\usepackage{graphicx}
\usepackage{../common/beamerthemeUppsala}
%\usecolortheme{UU} % Anpassa efter UU:s frger och logga
%\hypersetup{pdfpagemode=FullScreen} % Adobe Reader ska ppna fullskrm
\setbeamertemplate{itemize items}[circle]

% \usepackage{beamerthemesplit}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{epsfig}
% \usepackage[latin1]{inputenc}
 \usepackage{color}
% \usepackage{fancybox}
% \usepackage{psfrag}
% \usepackage[english]{babel}
 \setbeamertemplate{footline}{\hfill\insertframenumber/\inserttotalframenumber}


% Read in commands
\input{../common/commands.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\parskip}{3mm}
\title[]{{\color{black}Machine learning -- Block 6}}
\author[]{M{\aa}ns Magnusson\\Department of Statistics, Uppsala University}
\date{\currentsemester}


\begin{document}

\frame{\titlepage
% \thispagestyle{empty}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{This week's lectures}
\begin{itemize}
\item Introduction to unsupervised learning
\item k-means
\item Mixture of Gaussians
\item Expectation-Maximization
\item Probabilistic PCA
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Practicalities}

\begin{frame}{Practicalities}

\begin{itemize}
\item Last push this week!
\end{itemize}

\end{frame}


\section{Introduction to unsupervised learning}
\frame{\sectionpage}

\begin{frame}{Supervised and Unsupervised learning}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/iris_supervised.png}
\caption{The Supervised Problem}
\end{figure}

\end{frame}

\begin{frame}{Supervised and Unsupervised learning}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/iris_unsupervised.png}
\caption{The Unsupervised Problem}
\end{figure}

\end{frame}


\begin{frame}{Supervised and Unsupervised learning}

In {\color{uured} supervised} learning:
\begin{itemize}
\item We have \emph{training} data
\[
\mathcal{D}_{yx} = \{(y_i,x_i)\}_{i=1}^n
\]
\item We train a model $p(y|x)$ to {\color{uured} predict} $y$
\item We only care about the loss function during training
\end{itemize}
\pause
In {\color{uured} unsupervised}  learning:
\begin{itemize}
\item We have \emph{training} data
\[
\mathcal{D}_x = \{(x_i)\}_{i=1}^n
\]
\item We train a model $p(x)$ to {\color{uured} explain/model} $x$
\item Our loss function (or model) can be the goal
\end{itemize}
\end{frame}


\begin{frame}{Unsupervised learning}

{\color{uured} Goal}: Build a good (probabilistic) model $p(x)$ for $x$

\pause

Other names for $p(x)$:
\begin{itemize}
\item {\color{uured} Data} model\\ $p(x)$ is our \emph{data} generating mechanism
\item {\color{uured} Generative} model\\ We can \emph{generate} samples from $p(x)$.
\end{itemize}
\pause
Common use cases for unsupervised learning:
\begin{itemize}
\item Generate new observations from $p(x)$ % GAN/VAE
\item Study structure in large data % Topic models
\item Anomaly detection % Outbreak detection
\item Create representations for downstream tasks % Word embeddings
\end{itemize}

\end{frame}


\begin{frame}{The Learning Problem}

\begin{itemize}
\item {\color{uured} Goal}: A model that can "explain" the data well
\item Two main (basic) approaches:
\begin{itemize}
\item {\color{uured} Clustering}: Finding similar {\color{uured} observations} (rows)
\item {\color{uured} Dimensionality reduction}: Finding similar {\color{uured} variables} (columns)
\end{itemize}
\pause
\item Commonly, we use parametric probabilistic models\\ $p(x|\theta)$ where $\theta$ is unknown
\item {\color{uured} Learning problem}: Learn $\theta$ to explain the data as good as possible
\end{itemize}

\end{frame}


\begin{frame}{Example: Autoencoder}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/440px-Autoencoder_schema.png}
\caption{A Neural Autoencoder (Wikipedia)}
\end{figure}

\end{frame}

\begin{frame}{Loss functions and evaluation}
\begin{itemize}
\item For a deterministic autoencoder a common reconstruction loss is:
\[
L(x) = \|x - \hat{x}\|^2 = \|x - d(e(x))\|^2
\]
where $d(h)$ is the decoder and $e(x)$ is the encoder.\pause
\item In probabilistic models we can use the log-likelihood, $\mathcal{L}(x) = \log p(x)$, or {\color{uured} perplexity} (a function of the log-likelihood).
\begin{itemize}
\item {\color{uured} High} $\mathcal{L}(x)$: The observation is {\color{uured} well} explained by the model
\item {\color{uured} Low loss} $\mathcal{L}(x)$: The observation is {\color{uured} badly} explained by the model
\end{itemize}
\item Evaluate log-likelihood on a \uured{held-out validation set}
\end{itemize}

\end{frame}



\begin{frame}{Example: Bivariate Gaussian model}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/bi_data.png}
\caption{A Neural Autoencoder (Wikipedia)}
\end{figure}

\end{frame}

\begin{frame}{Example: Bivariate Gaussian model}

We assume a $p(x)$ is a Multivariate Gaussian model and estimate $\mu,\Sigma$ from data.
\pause

\[
\hat{\mu} = [3.19, 4.11]
\]

\[
\hat{\Sigma} = \begin{bmatrix} 1.95 & 2.05\\ 2.05 & 3.36 \end{bmatrix}
\]

We can now \uured{generate new data} from $\hat{p}(x)$ as MVN($\hat{\mu},\hat{\Sigma}$).

\end{frame}

\begin{frame}{Example: Bivariate Gaussian model}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/bi_model.png}
\caption{A Neural Autoencoder (Wikipedia)}
\end{figure}

\end{frame}




\subsection{Latent variables}

\begin{frame}{Latent variables}
\begin{itemize}
\item An {\color{uured} unobserved} or {\color{uured} hidden} variable or "factor"\pause
\item A parameter specific to some or a few observations or features\pause
\item Often these latent variables can be of \uured{main interest}
\end{itemize}

\end{frame}

\begin{frame}{Example: Hidden Markov Model}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/hmm.png}
\caption{A Hidden Markov Model (Wikipedia). }
\end{figure}

Here: $x$ is \uured{unobserved/latent} and $y$ is \uured{observed}.

\end{frame}


\begin{frame}{Example: Factor Analysis}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/FM.png}
\caption{A Factor Analysis Model (Eshima, Tabata and Borroni, 2018, edited).}
\end{figure}

\end{frame}



\section{Clustering}
\frame{\sectionpage}

\begin{frame}{Clustering}
\begin{itemize}
\item Separate observations $x_i$ into {\color{uured} groups} or {\color{uured} segments}\pause
\item What a cluster "is" depends on the {\color{uured} model/(dis)similarity}.
\item An example of (dis)similarity:
\[
D(x_i, x_j) = \sum_{k = 1}^P d_k(x_{i,k}, x_{j,k})
\]
(This assumes component-wise additive dissimilarity).
\pause
\item A common dissimilarity is the squared distance
\[
d_k(x_{i,k}, x_{j,k}) = (x_{i,k} - x_{j,k})^2
\]
\pause
\item Clustering can be divided into:
\begin{itemize}
\item {\color{uured} Hard} clustering%: An observation belong only to one cluster
\item {\color{uured} Soft} clustering%: An observation has a probability of belonging to all clusters (but it might be 0)
\end{itemize}
\pause
\item Clustering can also be divided into:
\begin{itemize}
\item {\color{uured} Hierarchical} clustering
\item {\color{uured} Flat} clustering
\end{itemize}
\pause
\item There is a ton of different algorithms and methods...
\end{itemize}

\end{frame}

\subsection{k-means}

\begin{frame}{k-means}
\begin{itemize}
\item Popular in practice and a classic in unsupervised machine learning\pause
\item Hard, flat clustering
\item Simple and effective\pause
\item {\color{uured} Model}: $x_i$ "is close to" one of $m_1,...,m_K$ vectors
\item {\color{uured} Loss function}:
\[
L(m) = \sum_{i=1}^n \min_{k \in \{1,\ldots,K\}} \| x_i - m_k \|^2
\]
\pause
\item {\color{uured} Hyperparameter}: $K$ (the number of clusters)
\item {\color{uured} Parameters}:  $\mathbf{m}$ (a $K \times P$ matrix).
\pause
\item The combinatorial assignment problem has $K^n$ possible clusterings, making global optimization infeasible.
\end{itemize}

\end{frame}


\begin{frame}{k-means algorithm}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/algo_10_1_kmeans.png}
\caption{The k-means cluster algorithm (Garreth et al, 2013, Alg. 10.1).}
\end{figure}


\end{frame}



\begin{frame}{k-means clustering}

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{fig/fig_10_6_kmeans_algo.png}
\caption{The k-means cluster algorithm (Garreth et al, 2013, Fig. 10.6).}
\end{figure}

\end{frame}


\begin{frame}{k-means clustering}

\begin{itemize}
\item k-means finds {\color{uured} local modes}
\item Re-run algorithm with many {\color{uured} different starting values}
\item Choose the best by the \uured{best loss}
\pause
\item There exists many developments
\begin{itemize}
\item scaling to large data
\pause
\item generalized loss
\end{itemize}
\pause
\item Choosing the number of clusters \(K\), common approaches:
\begin{itemize}
\item Elbow method (within-cluster variance)
\item Information criteria: AIC, BIC (mixture models)
\item Cross-validated log-likelihood
\item Bayesian nonparametrics (e.g.\ Dirichlet process mixtures)
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{k-means clustering}

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{fig/fig_10_7_kmeans_local_modes.png}
\caption{The k-means cluster algorithm (Garreth et al, 2013, Fig. 10.7).}
\end{figure}

\end{frame}


\begin{frame}{Problems with k-means}

\begin{itemize}
\item Clusters might
\begin{itemize}
\item overlap
\item have different forms
\end{itemize}
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{fig/mix_models.png}
\caption{Two clusters with different shapes.}
\end{figure}

\pause
We can solve these problems using {\color{uured} probabilistic models}

\end{frame}

\section{Mixture models}
\frame{\sectionpage}

\begin{frame}{Finite Mixture Models}

The {\color{uured} finite mixture model} density can be expressed as:
\[
p(y_i) = \sum_{k=1}^K \pi_k \, \phi(y_i \mid \theta_k),
\]
where $\pi_k \ge 0$, $\sum_{k=1}^K \pi_k = 1$, and $\phi(\cdot \mid \theta_k)$ is a density with parameters $\theta_k$.
\pause
Equivalent latent-variable representation:
\[
z_i \sim \text{Categorical}(\pi),
\]
\[
y_i \mid z_i=k \sim \phi(\cdot \mid \theta_k).
\]

\begin{itemize}
\item The parts of a (finite) mixture model:
\begin{itemize}
\item The number of components: $K$\pause
\item The proportions of observation from component $k$: $\pi_k$\pause
\item The density of component $k$: $\phi_k$\pause
\item The parameters of component $k$: $\theta_k$
\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}{Finite Mixture Models}

\begin{itemize}
\item Usually, we
\begin{itemize}
\item set $K$, and
\item use the same density for all $k$.
\end{itemize}
\pause
\item We can simulate data from the model as {\color{uured} compound probability distribution}:
\begin{enumerate}
\item Simulate cluster assignments for all $i$:
\[
z_i \sim \text{Categorical}(\pi)
\]
\pause
\item Generate $y_i$ conditioned on $z_i$:
\[
y_i \sim \phi_{z_i}(\theta_{z_i})
\]
\end{enumerate}
\pause
\item Cluster assignments $z_i$ are the {\color{uured} latent variables}
\end{itemize}

\end{frame}



\begin{frame}{Gaussian Mixture Models (GMM)}

\begin{itemize}
\item The (finite) Gaussian mixture model:
\[
p(y_i) = \sum_{k=1}^K \pi_k \mathcal{N}(\mu_k, \Sigma_k)\,,
\]
where $\mu_k$ and $\Sigma_k$ depend on the dimensionality of $y_i$.\pause
% \item GMM is a {\color{uured} universal approximator} of densities
% (Goodfellow et al. 2017, p. 67)
% A Gaussian mixture model is a universal approximator of densities, in the sense that any smooth density can be approximated with any speciﬁc nonzero amount of error by a Gaussian mixture model with enough components.
\end{itemize}

\end{frame}


\begin{frame}{GMM as Universal Approximators}

Let $p(x)$ be any continuous probability density on $\mathbb{R}^d$ with compact support.
For any $\varepsilon > 0$, there exists a Gaussian mixture model
\[
p_K(x) = \sum_{k=1}^K \pi_k \, \mathcal{N}(x \mid \mu_k, \sigma_k^2 \mathbf{I})
\]
such that
\[
\int |p(x) - p_K(x)| \, dx < \varepsilon.
\]
Hence, Gaussian mixture models are {\color{uured}\textbf{universal approximators}} of probability densities.

\vspace{2mm}

For an indepth proof, see Nguyen et. al (2020). \emph{Approximation by finite mixtures of continuous density functions that vanish at infinity} Cogent Mathematics and Statistics

\end{frame}

\begin{frame}{Example: Simulate data from a GMM}

\begin{enumerate}
\item Generate cluster assignments:
\[
z_i \sim \text{Categorical}(\pi = [0.4, 0.6])
\]
\item Generate observation conditioned on cluster assignment:
\[
y_i \sim \mathcal{N}(\mu_k, \Sigma_k)\,,
\]
where
\[
\mu_1 = [2, 2]\,, \mu_2 = [1, 1]\, \text{and}
\]

\[
\Sigma_1 = \begin{bmatrix} 3 & -2.7\\ -2.7 & 3 \end{bmatrix}\,,\Sigma_2 = \begin{bmatrix} 0.2 & 0\\ 0 & 0.2 \end{bmatrix}
\]

\end{enumerate}

\end{frame}


\begin{frame}{Simulated data from a GMM}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{fig/mix_models_z.png}
\caption{Simulated mixture data with the latent variable $z$.}
\end{figure}

\end{frame}

\begin{frame}{Simulated data from a GMM}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{fig/mix_models.png}
\caption{Simulated mixture data with the latent variable $z$.}
\end{figure}

\end{frame}

\begin{frame}{Label switching}

Finite mixture models are \emph{not identifiable} up to label permutation.

The likelihood is invariant under relabeling of components:
\[
(\pi_k, \theta_k)_{k=1}^K
\equiv
(\pi_{\sigma(k)}, \theta_{\sigma(k)})_{k=1}^K.
\]

This is known as \emph{label switching} and implies that cluster labels
have no intrinsic meaning.

\end{frame}

\begin{frame}{Mixtures of Multinomial distributions}

What {\color{uured} distribution} ($\phi$) should I use?\\
\pause
\vspace{2mm}
Depends on your {\color{uured} data} ($y$).
\pause

\[
p(y_i) = \sum_{k=1}^K \pi_k \text{Multinomial}(\mathbf{p}_k)
\]

\begin{figure}[h]
\centering
\includegraphics[width=0.25\textwidth]{fig/clu1.png}\includegraphics[width=0.25\textwidth]{fig/clu2.png}\\
\includegraphics[width=0.25\textwidth]{fig/clu3.png}\includegraphics[width=0.25\textwidth]{fig/clu4.png}
\caption{Mixture of Multinomials.}
\end{figure}

\end{frame}


\section{Expectation-Maximization}


\begin{frame}{Estimating Mixture Models}

\begin{itemize}
\item We are interested in estimating $\theta_k$ and $\pi_k$ for the model
\[
y_i = \sum_{k=1}^K \pi_k \phi(\theta_k)\,,
\]
\item If we add a cluster indicators $\uured{\mathbf{z}}$ it is simpler...
\pause
\item Two approaches:
  \begin{itemize}
  \item Gibbs sampler (Bayesian)
  \[
  p(\mathbf{z}, \theta, \pi|\mathbf{y})
  \]
  \pause
  \item Expectation-Maximization (Frequentist)
\end{itemize}
\end{itemize}


\end{frame}


\begin{frame}{Estimating Mixture Models}

\begin{itemize}
\item Hence we want to maximize the log-likelihood
\[
\mathcal{L}(\pi, \theta) = \sum^N_{i=1} \log \left( \sum_{k=1}^K \pi_k \phi(y_i|\theta_k)\right)
\]
\item This is difficult, although {\color{uured} if we only knew $\mathbf{z}$}...\pause
\begin{align*}
\mathcal{L}_{\text{full}}(\pi, \theta, \mathbf{z}) =& \sum^N_{i=1} \log \left( \sum_{k=1}^K I(z_i=k) \phi(y_i|\theta_k)\right) + \\
& \log(\pi_k^{I(z_i=k)})\\
=& \sum^N_{i=1} \sum_{k=1}^K I(z_i=k) \log \phi(y_i|\theta_k) + \\
& I(z_i=k) \log(\pi_k)
\end{align*}
\item So if we knew $\mathbf{z}$ it is essentially just maximizing $\mathcal{L}$ for each cluster separately.

\end{itemize}

\end{frame}


\begin{frame}{The Expectation}

\begin{itemize}
\item But, we dont know $\mathbf{z}$.\pause
\item Although, we could compute the {\color{uured} expected} cluster assignment
\[
\gamma^{(t)}_{i,k}
=
\Pr(z_i = k \mid y_i, \theta^{(t)})
=
\frac{
\pi_k^{(t)} \, \phi(y_i \mid \theta_k^{(t)})
}{
\sum_{j=1}^K \pi_j^{(t)} \, \phi(y_i \mid \theta_j^{(t)})
}
\]
\pause
\item $\gamma_{i}$ can be seen as observation $i$s {\color{uured} weights} or probability for each cluster
\pause
\item $\gamma_{i}$ is sometimes referred to as the {\color{uured} responsibility}.
\end{itemize}

\end{frame}


\begin{frame}{The Maximization}

\begin{itemize}
\item Now, given $\gamma$ we can (hopefully) easier maximize $(\pi,\theta)$.\pause
\[
\theta^{(t+1)}
=
\arg\max_(\pi,\theta)
\;
\mathbb{E}_{z \mid y, \pi^{(t)}, \theta^{(t)}}
\left[
\log p(y, z \mid \theta, \pi)
\right]
\]
\item We usually choose $\phi$ (the density) so the maximization
\begin{itemize}
\item is a nice analytical expression.
\item end up with a weighted MLE.
\end{itemize}

\uured{Note!} The E-step computes posterior probabilities of the latent variables. The M-step maximizes an expected log-likelihood using these probabilities as weights.

\end{itemize}



\end{frame}

\begin{frame}{Example: EM for a Gaussian Mixture}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/algo_8_1_em_gaussian.png}
\caption{The EM algorithm for a two component Gaussian mixture (Hastie et al 2008, Alg. 10.1)}
\end{figure}

\end{frame}

% TODO: Add an example of the EM algorithm

\begin{frame}{The EM algorithm}

\begin{itemize}
\item Properties of the EM algorithm:
\begin{itemize}
\item The EM-algorithm will converge to a {\color{uured} local mode}\pause
\item Each iteration will {\color{uured} always} increase the likelihood
\begin{itemize}
\item Can be proven straightforward using Jensen’s inequality% (see optional assignment)
\end{itemize}
\pause
\item We can interpret the final $\gamma_i$ as the {\color{uured} expected cluster}\\
Hence, the EM algorithm is a {\color{uured} soft clustering} approach.
\end{itemize}
\pause
\item Expanding the likelihood with latent variables ($z$) is called {\color{uured} data augmentation}.
\begin{block}{Clarification}
In EM, \uured{data augmentation} refers to introducing latent variables
(e.g., $z_i$) to make optimization easier.
This is unrelated to "data augmentation" in deep learning, where
new training examples are synthetically generated.
\end{block}
\end{itemize}

\end{frame}


\begin{frame}{Connections to other approaches}

\begin{itemize}
\item k-means can be derived as a limiting case of GMM:
Gaussian mixture with equal, spherical covariances
\[
\Sigma_k = \sigma^2 I
\]
where \(\sigma^2 \to 0\).
Then EM assignments become hard clustering, recovering \uured{k-means}.
\item Hence, if we set $z_i = \text{argmax}(\gamma_i)$: {\color{uured} k-means}\pause
\pause
\item Similarly, if we sample $z_i$ according to $\gamma$: {\color{uured} stochastic EM}\pause
\item If we sample $z_i$ conditional on $\theta$: {\color{uured} Gibbs sampling}

\end{itemize}

\end{frame}

\section{Probabilistic PCA}
\frame{\sectionpage}

\begin{frame}{Dimensionality reduction}

\begin{itemize}
\item So far focus has been on (clustering) {\color{uured} observations}
\item Now, we will address the other large area of UL: {\color{uured} dimensionality reduction}\pause
\item The starting point is {\color{uured} Principal Component Analysis} (PCA)
\item PCA can be used for
\begin{itemize}
\item {\color{uured} Reduce the dimensionality} of our data
\item {\color{uured} Produce lower-dimensional features} in a prediction model
\item {\color{uured} Discover underlying latent variables} (factors)
\end{itemize}
\pause
\item More details in the \uured{multivariate course}.
\end{itemize}

\end{frame}

\begin{frame}{Principal Component Analysis}

\begin{itemize}
\item {\color{uured} Basic idea}: We can summarize our data using $K$ principal components (PC)
\item The PCA "{\color{uured} model}" can be expressed as
\[
X \approx b +  W H^T\,,
\]
where $H \in \mathbb{R}^{n \times k}$, $W \in \mathbb{R}^{p \times k}$, $b \in \mathbb{R}^{p}$ and $X \in \mathbb{R}^{n \times p}$.
\item $H$ can be seen as {\color{uured} latent factors}
\pause
\item $W$ can be seen as {\color{uured} factor loadings}
\pause
\item We assume that $W$ is orthogonal: $W^T W = I$
\end{itemize}

\end{frame}

\begin{frame}{Principal Component Analysis}

\begin{itemize}
\item The PCA model
\[
X \approx b +  W H^T\,,
\]
\item The loss function, also called {\color{uured} reconstruction error}:
\[
J(b, W, H) = \sum_{i=1}^n \|x_i - (b + W h_i)\|^2.
\]
\pause
\item This can be minimized using {\color{uured} Singular Value Decomposition}
\end{itemize}

\end{frame}

\begin{frame}{PCA: Conceptual depiction}

\scriptsize

\begin{figure}
\begin{centering}
\[
\begin{array}{ccccc}
\\
\left[\begin{array}{ccc}
 & \text{ }\\
\\
\text{ } & X & \text{ }\\
 & (n \times p)\text{ }\\
\\
\end{array}\right] & \approx & \left[\begin{array}{c}
\\
\\
W\\
(p\times k)\\
\\
\end{array}\right] & \times & \left[\begin{array}{ccccc}
 &  & \text{ }\\
\text{ } & \text{ } & H^T & \text{ } & \text{ }\\
 &  & (k\times n)
\end{array}\right]\\
\\
\end{array}
\]
\end{centering}
\caption{Conceptual depiction of PCA.}
\label{matrix_decomposition_view}
\end{figure}

\end{frame}

% TODO Add latent Semantic Analysis as an example

\begin{frame}{Probabilistic PCA (pPCA)}

\begin{itemize}
\item PCA is not a {\color{uured} probabilistic model}\pause
\item Probabilistic PCA
\[
x_i = b +  W h_i^T + \epsilon_i
\]
where $\epsilon \sim N(\mathbf{0}, \Psi)$
\item In pPCA, we assume $\Psi = \sigma^2 \mathbf{I}$
\item We also assume that $h_i \sim N(0, I)$\pause
\item We can integrate out $H$ and get the model
\[
x_i \sim N(b, W W^T + \Psi)
\]
\end{itemize}

\end{frame}


\begin{frame}{Probabilistic PCA}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/pPCA_h.png}
\caption{Data from a pPCA model with $W = (-1, 3)^T$, $b = (0.5, 2)$ and $\sigma^2 = 1$}
\end{figure}

\end{frame}


\begin{frame}{Probabilistic PCA}

\begin{itemize}
\item Probabilistic PCA
\[
x_i = b +  W h_i^T + \epsilon_i
\]
\pause
\item We can now {\color{uured} estimate our parameters} using EM \\ (or Bayesian methods)
\item Enables us to {\color{uured} combine with other models} \\ (e.g. mixture of pPCA)
\pause
\item And as we will see next week, is the {\color{uured} basic building block} for many high-dimensional problems
\end{itemize}

\end{frame}


\begin{frame}{Connections to PCA and Factor Analysis}

\begin{itemize}
\item Probabilistic PCA
\[
x_i = b +  W h_i^T + \epsilon_i
\]
where $\epsilon \sim N(\mathbf{0}, \Psi)$
\item pPCA is closely connected to PCA and Factor Analysis:
\begin{itemize}
\item As $\sigma^2 \to 0$, noise vanishes and the latent-variable model
reduces to finding \emph{directions of maximal variance}, i.e. \uured{PCA}.
\item $\Psi = \text{diag}(\sigma_1, ..., \sigma_p, ..., \sigma_P)$: pPCA $\rightarrow$ {\color{uured} Factor Analysis},
where
\begin{itemize}
\item $H$ can be seen as latent factors
\item $W$ can be seen as factor loadings
\end{itemize}
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Latent-variable models (general form)}
A latent-variable model assumes:
\[
z_i \sim p(z), \qquad x_i \mid z_i \sim p(x \mid z_i).
\]

Examples:
\begin{itemize}
\item Mixture models: \(z_i\) = cluster
\item PCA / pPCA: \(z_i\) = latent factor
\item HMMs / State-space models: \(z_t\) = hidden state
\end{itemize}
\end{frame}

\end{document}
