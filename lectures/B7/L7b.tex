%\documentclass[10pt,handout]{beamer}
\documentclass[10pt]{beamer}
\usepackage{babel} % Anpassa efter svenska. Ger svensk logga.
\usepackage[utf8]{inputenc} % Anpassa efter linux
\usepackage{graphicx}
\usepackage{../common/beamerthemeUppsala}
%\usecolortheme{UU} % Anpassa efter UU:s frger och logga
%\hypersetup{pdfpagemode=FullScreen} % Adobe Reader ska ppna fullskrm
\setbeamertemplate{itemize items}[circle]

% \usepackage{beamerthemesplit}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{epsfig}
% \usepackage[latin1]{inputenc}
 \usepackage{color}
% \usepackage{fancybox}
% \usepackage{psfrag}
% \usepackage[english]{babel}
 \setbeamertemplate{footline}{\hfill\insertframenumber/\inserttotalframenumber}


% Read in commands
\input{../common/commands.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\parskip}{3mm}
\title[]{{\color{black}Machine learning -- Large Language Models}}
\author[]{M{\aa}ns Magnusson\\Department of Statistics, Uppsala University}
\date{\currentsemester}


\begin{document}

\frame{\titlepage
% \thispagestyle{empty}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Introduction} % to Large Language Models

\begin{frame}{What is a Large Language Model?}

\begin{itemize}
  \item Large Language Models (LLM) are commonly defined as:
  \begin{itemize}
      \item large natural language models
      \pause
      \item generative and autoregressive:
      \\predicting a token at a time, based on previous \uured{context}
      \pause
      \item having some ability to achieve \uured{general-purpose language "understanding"}
      \item show \uured{"emergent"} abilities to solve other more complex tasks
      \item such as few-shot learning and \uured{incontext} learning
      \pause
      \item pre-trained on very large data
  \end{itemize}
  \item Large Language Models (LLM) are commonly also:
  \begin{itemize}
    \item usually decoder-type models
    \pause
    \item based on transformer architectures
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Comparing to pre-trained models (such as BERT)}

\begin{itemize}
  \item Compared to pretrained language models (PLM), LLMs are:
  \begin{itemize}
      \item larger (billions or trillions, rather than millions of parameters), in practice only possible to train by a few persons
      \item possible to use for in-context learning
      \item usually interacted with through the prompt
      \item generate textual responses
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Historical development}

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/fig2_zhao_2023}
\caption{The development of LLMs (Figure 2, Zhao et al., 2023)}
\end{figure}

\begin{itemize}
    \item Milestones (subjective): GPT-3, GPT-4, chatGPT, Llama 2
\end{itemize}

\end{frame}

\begin{frame}{A subset of current LLMs}

\begin{figure}[h]
\centering
\includegraphics[width=0.82\textwidth]{fig/tab1_zhao_2023}
\caption{Statistics of LLMs (Table 1, Zhao et al., 2023)}
\end{figure}

\end{frame}

\begin{frame}{Examples of LLM prompting}

Examples:
\begin{enumerate}
\item Can you please add 113329 and 719292? (true is 832621)
\item Who is Olof Palme? Please respond both in English and Swedish.
\end{enumerate}

\centering

\vspace{5mm}

\href{https://www.llama2.ai/}{\texttt{Llama 2}}

\vspace{10mm}

\href{https://chat.openai.com/}{\texttt{chatGPT}}

\end{frame}


\begin{frame}{Why is this working now?}

\begin{itemize}
\item Scaling of pretrained models:
\begin{itemize}
\item More computation thanks to the Transformer architecture
\item Larger models (GPT3 175B, PaLM 540B)
\item Larger datasets
\item All three are important (Zhou et al., 2023)\\
\uured{Can we connect this back to the standard ML framework?}
% TODO: Fix citations
% Scaling laws has shown that all three matters and is of importance.
\end{itemize}
\pause
\item Fine-tuning for tuning models to follow instructions (InstructGPT)
\end{itemize}

\end{frame}


%\begin{frame}{Scaling laws}

%Extensive research has shown that scaling can largely improve the model capacity of LLMs

% Scale Matters/ Scaling laws p 4 in Zhao
% model performance with respective to three major factors
% namely model size (N), dataset size (D),

%\end{frame}

\subsection{"Emergent" abilities}

\begin{frame}{"Emergent" abilities}
% Emergent abilities
% p 4 in Zhao
\begin{itemize}
\item "the abilities that are not present in small models but arise in large models" (Zhao et al, 2023)
\item One of the main differences between LLMs and PLMs
\item Examples:
\begin{itemize}
\item In-context learning (basis for prompting)
\item Instruction following % "LaMDA-PT started to significantly outperform the untuned one on unseen tasks when the model size reached 68B, but not for 8B or smaller model sizes."
\item Step-by-step reasoning
\end{itemize}
\end{itemize}

\end{frame}

\subsection{In-context learning (ICL)}

\begin{frame}{In-context learning (ICL)}

"In context learning is a paradigm that allows language models to learn tasks given only a few examples in the form of demonstrations." (Dong et al, 2023)

\begin{itemize}
\item Learning tasks in the actual \uured{context} (previous tokens).
\pause
\item No updates of the weight - only using existing weights.
\pause
\item We demonstrate what to do with a \uured{few examples}
\pause
\item The model "learn" what to do \uured{in context}.
\pause
\item We "prompt" LLMs for utilization
\item Let data be
\[
D_k = \left(f(x_1, y_1),...,f(x_k, y_k) \right)
\]
then
\[
\hat{y} = \text{LLM}(I, \underbrace{\left(f(x_1, y_1),...,f(x_k, y_k)\right)}_{demonstrations}, \underbrace{f(x_{k+1}}_{input} , ...)
\]
where $I$ are the general instructions.

\end{itemize}

% TODO: A comprehensive review of ICL has been presented in the survey paper [50], and we suggest the readers refer- ring to it for a more general, detailed discussion on this topic.
% In-context learning
% 6.1 Zhao

\end{frame}


\begin{frame}{In-context learning (ICL)}

\begin{itemize}
\item The problem: How to design the instructions ($I$) in a good way? Prompt engineering.
\pause
\item What happens under the hood? \uured{We position the model in embeddings space.}
\item What is good can be unintuitive?
\begin{itemize}
\item "Take a deep breath and think hard." %TODO: Fix citation
\item Beeing nice. %TODO: Fix citation
\end{itemize}
\pause
\item The effectiveness of ICL is highly affected by the \uured{design of demonstrations} (Zhou et al., 2023) %[350, 370, 371]
\end{itemize}

\end{frame}

% TODO Add section slides

\begin{frame}{Demonstration}
% Zhao see 6.1.2.
% the selected demon- stration examples in ICL should contain sufficient informa- tion about the task to solve as well as be relevant to the test query, for the above two selection approaches.

\begin{itemize}
\item Demonstration selection
\begin{itemize}
\item diversity-based selection strategies: choose the most representative set of examples
\item k-NN
\item LLM-based: e.g. let an LLM itself generate examples
\end{itemize}
\pause
\item Demonstration format/instruction
\begin{itemize}
\item "Lets think step by step", "Take a deep breath and think."
\end{itemize}
\pause
\item Demonstration order
\begin{itemize}
\item Indications of recency bias
% (they are prone to repeat answers that are near the end of demonstrations)
\end{itemize}
\end{itemize}


\end{frame}




\begin{frame}{Chain of thought (CoT) prompting}

% Zhao Section 6.2

\begin{itemize}
\item Prompting strategy to improve performance in \uured{"reasoning"}
\pause
\item Incorporates \uured{intermediate reasoning steps}
\item Instead of (input, output), we use (input, chain-of-thought, output)
\pause
\item Zero-shot CoT: "Lets think step by step"
\end{itemize}

\end{frame}


\begin{frame}{Chain of thought (CoT) prompting}

% Zhao Section 6.2

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/zhao_2023_fig12}
\caption{ICL vs CoT (Figure 12, Zhao et al., 2023)}
\end{figure}

\end{frame}


\subsection{Hallucinations}

\begin{frame}{Hallucinations: What is it?}
% What is it?
"Generated text that is fluent and natural but unfaithful"

\begin{itemize}
\item LLMs are prone to generate untruthful information, often called \uured{hallicinations} that
\begin{itemize}
\item logically contradicts the source content (intrinsic hallucination)
\item cannot be verified by the available source (extrinsic hallucination)
\end{itemize}
\item This can be a major problem.
\item See Huang et al (2023) for a survey on hallucination %https://arxiv.org/pdf/2311.05232.pdf
\end{itemize}

\end{frame}


\begin{frame}{Hallucinations: What is it?}
% What is it?
"Generated text that is fluent and natural but unfaithful"

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/zhou_2023_fig14}
\caption{Hallucinations (Zhao et al., 2023, Figure 14)}
\end{figure}

% TODO: Fix image (remove yellow)

\end{frame}

% intrinsic and extrinsic hallucinations [241]
% . In the former, the generated text logically contradicts the source content. In the latter, we cannot verify the output correctness from the provided source; the source content does not provide enough information to assess the out- put, which is, therefore, under-determined.

%In generating factual texts, a challenging issue is hallucination generations [518], where the generated information is either in conflict with the existing source (intrinsic hallucination) or cannot be verified by the avail- able source (extrinsic hallucination),

% LLMs are prone to generate untruthful informa- tion that either conflicts with the existing source or cannot be verified by the available source. Even the most powerful LLMs such as ChatGPT face great challenges in migrating the halluci- nations the generated texts. This issue can be partially alleviated by special approaches such as alignment tuning and tool utilization.

% Zhao
% Hallucinations
% Kaddour (2023): 2.8, Fig 7


\begin{frame}{Hallucinations: Example}

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/hall_llama7B}
\caption{Hallucinations with Llama 2 (7B, from 2023-11-25)}
\end{figure}

\end{frame}

\begin{frame}{Hallucinations: Example}


\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/hall_llama70B}
\caption{Hallucinations with Llama 2 (70B, from 2023-11-25)}
\end{figure}

\end{frame}

\begin{frame}{Hallucinations: Example}

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/hall_gpt35}
\caption{Hallucinations with GPT3.5 (from 2023-11-25)}
\end{figure}

\end{frame}

\begin{frame}{Hallucinations: Causes}
% What is it?
\begin{itemize}
\item LLMs are not trained for factual correctness
\item LLMs are statistical models
\item Knowledge changes over time - pretraining takes a lot of time. %The parametric knowledge of LLMs is hard to be updated in a timely manner. Augmenting LLMs with external knowledge sources is a practical approach to tackling the issue. However, how to effectively update knowledge within LLMs remains an open research problem.
\end{itemize}
\pause
\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/hall2_gpt35}
\caption{Hallucinations with GPT3.5, trained up until january 2022 (from 2023-11-26)}
\end{figure}
\begin{itemize}
\item What can be the cause for this hallucination?
\end{itemize}

\end{frame}

\begin{frame}{Hallucinations: Mitigations}
% What is it?
\begin{itemize}
\item Decoding strategies % Dziri et al. [136] observe a positive correlation between increased diversity in response generation and hallucinations.
\item Retrieval-augmented generation (RAG) % For (i), a retriever module retrieves the top-k relevant documents (or passages) for a query from a large corpus of text. Then, for (ii), we feed these retrieved documents to the language model together with the initial prompt. In theory, using an external data source may also make it eas- ier to interpret which knowledge is retrieved and update it without tediously fine-tuning the model.
\item Instruction-tuning/alignment
\item Quality of training data
\end{itemize}

\end{frame}


\section{Architectures}

\begin{frame}{The transformer (Vaswani et al., 2017)}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{fig/Vaswani_1_transformer}
\caption{The transformer (Vaswani et al., 2017)}
\end{figure}

\end{frame}


\begin{frame}{Architecture}
% TODO: Add images from the Illustrated GPT-2

\begin{itemize}
\item LLMs are usually based on the transformer \uured{decoder}
\item "Decoder-only architectures"
\item Why autoregressive decoders? Why predicting the next word?
\item Here, \uured{generative pre-trained transformer} (GPT) 2 is a running example.
\item
\end{itemize}

\end{frame}

\begin{frame}{Decoder vs. Encoder models}
% TODO: Add images from the Illustrated GPT-2

\begin{figure}[h]
\centering
\only<1>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig1}}
\caption{Alammar, J (2019). The Illustrated GPT2 [Blog post]. Retrieved from \href{https://jalammar.github.io/illustrated-gpt2/}{jalammar.github.io/illustrated-gpt2/}
}
\end{figure}

\end{frame}


\begin{frame}{The GPT Architecture}
% TODO: Add images from the Illustrated GPT-2

\begin{figure}[h]
\centering
\only<1>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig2}}
\only<2>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig3}}
%\only<3>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig4}}
\only<3>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig5}}
\only<4>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig6}}
\only<5>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig7}}
\caption{Alammar, J (2019). The Illustrated GPT2 [Blog post]. Retrieved from \href{https://jalammar.github.io/illustrated-gpt2/}{jalammar.github.io/illustrated-gpt2/}
}
\end{figure}

\end{frame}

\begin{frame}{The GPT input and output}
% TODO: Add images from the Illustrated GPT-2

\begin{figure}[h]
\centering
\only<1>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig8a}}
\only<2>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig9}}
\only<3>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig10}}
\only<4>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig11}}
\only<5>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig12}}
\caption{Alammar, J (2019). The Illustrated GPT2 [Blog post]. Retrieved from \href{https://jalammar.github.io/illustrated-gpt2/}{jalammar.github.io/illustrated-gpt2/}
}
\end{figure}

\end{frame}


\begin{frame}{The (masked) self attention}
% TODO: Add images from the Illustrated GPT-2

\begin{figure}[h]
\centering
\only<1>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig15saa}}
\only<2>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig15sab}}
\only<3>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig15sac}}
\only<4>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig15sad}}
\only<5>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig15sae}}
\only<6>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig16a}}
\only<7>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig17a}}
\only<8>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig17b}}
\only<9>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig17c}}

\caption{Alammar, J (2019). The Illustrated GPT2 [Blog post]. Retrieved from \href{https://jalammar.github.io/illustrated-gpt2/}{jalammar.github.io/illustrated-gpt2/}
}
\end{figure}

\end{frame}


\begin{frame}{The GPT output}
% TODO: Add images from the Illustrated GPT-2

\begin{figure}[h]
\centering
\only<1>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig18a}}
\only<2>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig18b}}
\only<3>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig19a}}
\only<4>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig19b}}
\only<5>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig19c}}
\only<6>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig19d}}
\only<7>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig19e}}
\only<8>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig19f}}
\only<9>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig19g}}
\only<10>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig19h}}
\only<11>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig19i}}
\only<12>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig19j}}
\only<13>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig19k}}
\only<14>{\includegraphics[width=0.99\textwidth]{fig/alammar2019_fig18a}}
\caption{Alammar, J (2019). The Illustrated GPT2 [Blog post]. Retrieved from \href{https://jalammar.github.io/illustrated-gpt2/}{jalammar.github.io/illustrated-gpt2/}
}
\end{figure}

\end{frame}

\begin{frame}{The transformer}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{fig/Vaswani_1_transformer}
\caption{The transformer (Vaswani et al., 2017)}
\end{figure}

\end{frame}

\begin{frame}{Architecture configurations}

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/zhou_2023_tab4}
\caption{Common configurations (Zhou et al., 2023)}
\end{figure}
where "sublayer" indicates the previous layer in the network.
% TODO: Show sublayer mathematically

\end{frame}


\begin{frame}{Common models and their configuration}

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/zhou_2023_tab3}
\caption{Common configurations (Zhou et al., 2023, Table 3)}
\end{figure}

\end{frame}


\begin{frame}{Decoding (word generation)}

\begin{itemize}
\item LLMs are trained to predict the next word (called decoding), i.e the discret probability model (pmf)
\[
p_{x,i} = P(x_i|\mathbf{x}_{j<i})
\]
\pause
\item Two approaches to generate/decode:
\begin{description}
\item[greedy] $x_i = \arg_x \max P(x|\mathbf{x}_{j<i})$
\item[sampling] $x_i \sim P(x|\mathbf{x}_{j<i})$
\end{description}
\pause
\item \uured{Temperature sampling}:
\[
\tilde{P}(x_i|\mathbf{x}_{j<i}) = \frac{\exp(\log(p_{x,i})/t)}{\sum_{i'} \exp(\log(p_{x,i'})/t)}
\]
\[
x_i \sim \tilde{P}(x_i|\mathbf{x}_{j<i})
\]
As $t\rightarrow 0$: greedy, and $t = 1$: ordinary sampling
\pause
\item Top-$k$ sampling: sample proportionally from from top $k$ most probable words
\end{itemize}

\end{frame}



\section{Training}

\begin{frame}{(Pre-training) Data}

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/zhou_2023_tab2}
\caption{Common datasets (Zhou et al., 2023, Table 2)}
\end{figure}

% 1. Data Collection
% use Table 2 Zhao p 11 and part 4.1
% Use Figure 5 here
% Quality filtering: p 14

\end{frame}

\begin{frame}{(Pre-training) Data}

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/zhou_2023_fig5}
\caption{Common datasets (Zhou et al., 2023, Figure 5)}
\end{figure}

% 1. Data Collection
% use Table 2 Zhao p 11 and part 4.1
% Use Figure 5 here
% Quality filtering: p 14

\end{frame}

\begin{frame}{Pre-training Data Processing Pipeline}

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/zhou_2023_fig6}
\caption{Pre-training data processing (Zhou et al., 2023, Figure 6)}
\end{figure}

\end{frame}



\begin{frame}{Tokenization}
% See p. 15
\begin{itemize}
\item Simple tokenization: Word based
\pause
\begin{itemize}
\item Difficult in some languages (e.g. Chinese)
\pause
\item Generate large vocabularies with many different words
\pause
\item Out-of-vocabulary problem
\pause
\end{itemize}
\item \uured{Byte-pair encoding (BPE)}: \\Combine most common characters until predefined vocabulary is reached
\pause
\item \uured{Wordpiece encoding (WE)}: \\Simplary to BPE, but uses a language model to score which to merge
\pause
\item Unigram tokenizer: \\Like WE, but start with a large vocabulary and trim it down.
\end{itemize}

\end{frame}

\begin{frame}{Pretraining task/Loss}

\begin{itemize}
\item The most common task is language modeling(LM), i.e. maximizing
\[
L_{LM}(\mathbf{x}) = \sum_i^n \log P(x_i | \mathbf{x}_{<i})
\]
\item Why is this good/result in good models?\pause
\begin{itemize}
\item To better and better predict text we need to increase the \uured{understanding} of the model. \\ e.g. "Biden and Xi and a meeting on the role of ..." (truth: artificial intelligence)
\end{itemize}
\end{itemize}


\end{frame}

\begin{frame}{Optimization/Training}

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/zhou_2023_tab5}
\caption{Optimization during pre-training (Zhao et al, 2023, Table 5)}
\end{figure}

\end{frame}




\section{Adaptation and Alignment}

\begin{frame}{Instruction tuning/following}

% instruction tuning (discussed in Section 5.1)
\begin{itemize}
\item We have a very large, pre-trained model
\item We want it \uured{perform tasks} (e.g. ICL): \uured{instruction tuning}
\item We want it \uured{to be safe}: \uured{alignment}
\end{itemize}
\end{frame}


\subsection{Instruction tuning}

\begin{frame}{Instruction tuning/following}

% instruction tuning (discussed in Section 5.1)
\begin{itemize}
\item \uured{Instruction tuning}: fine-tuning pre-trained LLMs on a collection of formatted instances
\pause
\item Effective approach to adapting existing general LLMs to be domain-specific experts\\
e.g. fine-tune Flan-PaLM using medical datasets to create Med-PaLM
\pause
\item \uured{Effect of instruction tuning}: Performance improvement, a general approach to enhancing the abilities of existing language models
\end{itemize}

\end{frame}

% TODO: Add example of instruction tuning from instructGPT paper

\begin{frame}{Instruction tuning/following}

% instruction tuning (discussed in Section 5.1)
\begin{itemize}
\item An instruction-formatted instance consists of:
\begin{itemize}
\item a task description (called an instruction),
\item an optional input,
\item the corresponding output,
\item and a small number of demonstrations (optional).
\end{itemize}
\item Two kinds of important instruction data,
\begin{itemize}
\item task-formatted instructions
\item chat instructions.
\end{itemize}
\item Key aspects for instruction data:
\begin{itemize}
\item Scaling, more data is better (to a point) % - a large number of instances, but the effect is descreasing
\item Diversity and quality of instructions more important than the number of instances
\item Including things to (1) avoid, (2) reasons, and (3) suggestions may have a negligible or adverse effect
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Instruction tuning training}

% instruction tuning (discussed in Section 5.1)
\begin{itemize}
\item Unlike pre-training, instruction tuning is often \uured{more efficient}.
\item Usually, \uured{only a moderate number of instances} are used for training.

\pause
\item The training objective (i.e., usually sequence-to-sequence loss with teacher forcing)
% TODO: Elaborate on teacher forcing and seq-2-seq loss
\pause
\item Smaller batch size and learning rate
\pause
\item Balance the proportion of different tasks during fine-tuning
\pause
%A widely used method is the examples-proportional mixing strategy
\item Pre-training data during instruction tuning as regularization% which can be regarded as regularization for model tuning
\end{itemize}

\end{frame}

% TODO: Add slide on LoRA here.

\begin{frame}{Fine-tuning Llama}

\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{fig/zhao_2023_tab7}
\caption{Training time for fine-tuning Llama}
\end{figure}

\end{frame}

% Im here

\subsection{Alignment}

\begin{frame}{Alignment tuning}

% Aligning p 4 in Zhao

\begin{itemize}
\item \uured{Problems} with LLMs:
\begin{itemize}
\item fabricating false information (hallucinations)
\item producing harmful, misleading, and biased output
\end{itemize}
% LLM may sometimes exhibit unintended behaviors, e.g., fabricating false information, pursuing inaccurate objectives, and producing harmful, misleading, and biased expressions [61, 293].
\pause
\item \uured{Alignment} of LLMs: Fine-tuning (like instruction tuning) but different criteria:
\begin{itemize}
\item helpfulness
\item honesty
\item harmlessness
\end{itemize}
% However, unlike the original pre-training and adaptation tuning (e.g., instruction tuning), such an alignment requires considering very different crite- ria (e.g., helpfulness, honesty, and harmlessness).
\pause
\item Alignment might harm the general performance of LLMs : \uured{alignment tax}%, which is called alignment tax in related literature [295].
\item \uured{Red teaming}:
\begin{enumerate}
\item probe the model in an adversarial way to generate harmful outputs
\item updates LLMs to prevent such outputs
\end{enumerate}
\end{itemize}
\end{frame}


\begin{frame}{Alignment tuning: Criterias}

% Aligning p 4 in Zhao
% RLHF

\begin{itemize}
\item \uured{Helpfulness}
\begin{itemize}
\item Demonstrate a clear attempt to assist users in solving their tasks
\item Answering questions in a concise and efficient manner
\end{itemize}
\pause
\item \uured{Honesty}
\begin{itemize}
\item Present accurate content to users instead of fabricating information
\item Convey appropriate degrees of uncertainty in its output
%iin order to avoid any form of deception or misrepresentation of information. This requires the model to know about its capabilities and levels of knowledge (e.g., “know unknowns”).
\end{itemize}
\pause
\item \uured{Harmlessness}
\begin{itemize}
\item No offensive or discriminatory language
\item Detect covert endeavors aimed for malicious purposes.
\end{itemize}
\end{itemize}

% TODO: Find examples

\end{frame}



\begin{frame}{Alignment tuning: Human feedback}

\begin{itemize}
\item High-quality human feedback is crucial for aligning LLMs
\pause
\item \uured{Human Labeler Selection}: Usually native speakers.
\pause
\item Three ways to collect feedback (from humans):
\begin{enumerate}
\item Ranking based (choosing the best out of many suggestions)
\pause
\item Answering question about the model output (multiple choice)
\pause
\item Collect data about if the LLM is "breaking the rules" %helpful, correct, and harmless (as scores)
\end{enumerate}
\end{itemize}

\end{frame}

\subsection{RLHF}

\begin{frame}{Reinforcement learning with human feedback}
% (discussed in Section 5.2.3)
%The RLHF system mainly comprises three key components: a pre-trained LM to be aligned, a reward model learning from human feedback, and a RL algorithm training the LM.

\begin{itemize}
\item Method to align a model (e.g. in chatGPT)
\item Three components:
\begin{enumerate}
\item Pre-trained LLM
\item Reward Model (RM) learned from human feedback
\item Reinforcement learning algorithm aligning the LLM
\end{enumerate}
\pause
\item \uured{Idea:} The reward model (RM) learn human preferences
\item RM: A fine-tuned smaller LM or a LM trained from scratch using human preference data.
\pause
\item The reinforcement learning algorithm:
\begin{itemize}
\item the (LLM) agent  will perform an action by generate text
\item the (RM) will give the agent a reward signal
\end{itemize}
\end{itemize}


%\begin{enumerate}
%\item Supervised finetuning %To make the LM initially perform desired behaviors, it usually needs to collect a supervised dataset containing input prompts (instruction) and desired outputs for fine-tuning the LM.
%\item Learn reward model % : OpenAI uses 6B GPT-3 and DeepMind uses 7B Gopher, but bigger is usually better
%\item Fine-tune the model with the reward model
%\end{enumerate}


\end{frame}


\begin{frame}{RLHF: Three steps}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/zhao_2023_fig10}
\caption{RLHF (Zhao, 2023, Figure 10)}
\end{figure}

\end{frame}



\begin{frame}{The values of LLMs}

\begin{itemize}
\item What are the values of an LLM?
\item Atari et al (2023) asked ChatGPT World Value Survey questions:
\end{itemize}
\pause
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/atari_2023_fig2}
\caption{Value dimensions (Atari et al., 2023, Figure 2)}
\end{figure}


\end{frame}


\section{Usage}

\begin{frame}{Usage of LLM}
\begin{itemize}
\item Many areas of applications
\item Probably just in the starting phase
\item Example applications:
\begin{itemize}
\item ChatBots/Question-answering in domains (Medical, Law etc)
\item Virtual Assistants
\item Text summarization
\item Code Generation
\item Information retrieval
\end{itemize}
\item Currently, still BERT is competetive in many large scale text analysis tasks (eg see Ollion et al, 2023, "Mind the hype")
\end{itemize}

\end{frame}


\subsection{Prompt engineering}

\begin{frame}{Prompt engineering}

\begin{itemize}
\item prompt engineering: Designing prompts to produce good results with \uured{in context learning}
\item In practice: Moving into the right initial position in embedding space.
\item four different areas affecting the performance (Zhou et al (2023):
\begin{itemize}
\item Task description: Describe the task clearly.
\item Input data: Describe the example data for the LLM so it can be used.
\item Contextual information: Background information on the task
\item Propt style:
\begin{itemize}
\item OpenAI suggest to sepratate examples with hashtags.
\item Most pre-training data is in English, so using english improves performance
\item "Think through this step by step", an unintuitive "Take a deep breadth and think it through".
\end{itemize}
\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}{How to prompt: Design Principles}

\begin{itemize}
\item See Zhou et al. (2023), Table 14.
\item Design principles:
\begin{itemize}
\item Express your goal clearly, e.g. make the prompt self-contained (avoid propositions like they, it etc)
\item Decompose into detailed, easy-to-solve sub-tasks, e.g. "Check if the above solution is correct"
\item Provide few-shot examples
\item Use model friendly formats, e.g. use qutation marks and hashtags to separate examples
\end{itemize}
\end{itemize}

\end{frame}


% \section{Safety and Risks}

%\begin{frame}{Risks}

%\begin{figure}[h]
%\centering
%\includegraphics[width=0.99\textwidth]{fig/cederberg_2023}
%\caption{The 1177 debacle (Cederberg, 2023)}
%\end{figure}

%\end{frame}


\end{document}
