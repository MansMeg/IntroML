%\documentclass[10pt,handout]{beamer}
\documentclass[10pt]{beamer}
\usepackage{babel} % Anpassa efter svenska. Ger svensk logga.
\usepackage[utf8]{inputenc} % Anpassa efter linux
\usepackage{graphicx}
\usepackage{../common/beamerthemeUppsala}
%\usecolortheme{UU} % Anpassa efter UU:s frger och logga
%\hypersetup{pdfpagemode=FullScreen} % Adobe Reader ska ppna fullskrm
\setbeamertemplate{itemize items}[circle]

% \usepackage{beamerthemesplit}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{epsfig}
% \usepackage[latin1]{inputenc}
 \usepackage{color}
% \usepackage{fancybox}
% \usepackage{psfrag}
% \usepackage[english]{babel}
 \setbeamertemplate{footline}{\hfill\insertframenumber/\inserttotalframenumber}

%library(tinytex)
%tlmgr_install('booktabs')
\usepackage{booktabs}

%library(tinytex)
%tlmgr_install('csquotes')
%\usepackage{csquotes}


% Read in commands
\input{../common/commands.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\parskip}{3mm}
\title[]{{\color{black}Machine learning -- Block 7}}
\author[]{M{\aa}ns Magnusson\\Department of Statistics, Uppsala University}
\date{\currentsemester}


\begin{document}

\frame{\titlepage
% \thispagestyle{empty}
}


\section{Practicalities}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{This week's lectures}
\begin{itemize}
\item Variational autoencoders
\item Probabilistic Topic Models
\item (Diffusion models)
\item Large Language Models
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\frame{\sectionpage}

\begin{frame}{Why variational autoencoders and topic models?}
\begin{itemize}
\item Popular approaches in \uured{industry and academia}
\item \uured{Probabilistic} methods for unsupervised learning\pause
\item \uured{Aim} of this lecture:
\begin{itemize}
\item Describe the models
\item How to estimate these models
\item Explain what they are used for
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Use Cases}

\begin{itemize}
\item Variational autoencoders: Unsupervised modeling of \uured{images}
\item Topic models: Unsupervised modeling of \uured{documents}
\end{itemize}

\pause

\begin{itemize}
\item Used for:
\begin{itemize}
\item Identify "\uured{closeness}" in high-dimensional data\pause
\item \uured{Visualize/analyze} data\pause
\item \uured{Compression}\pause
\item \uured{Feature construction}\pause
\item Analyze underlying \uured{patterns}
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Use Cases: Examples}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/MNIST_VAE_latent.png}
\caption{The latent state of MNIST using an Variational Autoencoder}
\end{figure}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Autoencoders}
\frame{\sectionpage}

\begin{frame}{Autoencoder}

\begin{itemize}
\item An autoencoder is a neural network (e.g. feed-forward) that takes an input $x$ and predict (the same) $x$ ($r$, reconstruction). \pause
\item Three parts:
\begin{itemize}
\item \uured{encoder} $f(x)$ (or $e(x)$)
\item \uured{code} $h$
\item \uured{decoder} $g(h)$ (or $d(h)$)
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{fig/DL_14_1_ae}
\caption{A Neural Autoencoder (Goodfellow et al, 2018)}
\end{figure}

\end{itemize}

\end{frame}

\begin{frame}{Autoencoder loss (Reconstruction error)}
For a deterministic autoencoder:
\[
h = e_{\phi}(x), \qquad \hat{x} = d_{\theta}(h)
\]

A common reconstruction loss is:
\[
L(\theta, \phi) = \| x - \hat{x} \|_2^2
     = \| x - d_{\theta}(e_{\phi}(x)) \|_2^2
\]


Minimizing squared error is equivalent (up to constants) to maximizing the log-likelihood of a Gaussian decoder with fixed variance:
\[
x \mid h \sim \mathcal{N}(d_{\theta}(h), \sigma^2 I).
\]
\end{frame}



\begin{frame}{The Undercomplete Autoencoder}

\begin{itemize}
\item More interesting: an \uured{undercomplete} autoencoder:\\Dimension of code is \uured{lower} than that of $x$
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{fig/440px-Autoencoder_schema.png}
\caption{A Neural Autoencoder (Wikipedia)}
\end{figure}

\end{frame}


\begin{frame}{PCA and autoencoders}

\begin{itemize}
\item A linear autoencoder: $e_\phi(x) = W_\phi$ x, and $d_\theta(h) = W_\phi$ h
\item We want to minimize the loss (ignoring $b$/the mean):
\[
L(\theta,\phi)
= \sum_{i=1}^N \left\| x_i - W_{\theta} W_{\phi} x_i \right\|_2^2
\]
\pause
\item Remember \uured{PCA loss}:
\[
L(P) = \sum_{i=1}^N (x_i - P_q P_q^T x_i)^2\,,
\]
where $P$ is an orthogonal matrix of rank $q$.
\pause
\item \uured{Hence}: PCA can be seen as an linear autoencoder (under squared loss and orthonormal constraints)
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{fig/Rocca_PCA_as_autoencoder2.png}
\caption{PCA as autoencoder (Rocca, 2019)}
\end{figure}

\end{frame}


\begin{frame}{Deep Autoencoders}
\begin{itemize}
\item \uured{Deep} Autoencoder: An autoencoder with \uured{multilayer neural networks} as encoder and decoder
\begin{itemize}
\item can be seen as a \uured{non-linear PCA}
\item learn \uured{nonlinear representations}
%\item project data onto a nonlinear manifold
%(manifold is a topological space that locally resembles Euclidean space near each point)
\end{itemize}
\pause
\item Problem: Deep autoencoders need to be \uured{regularized} to not \uured{overfit} the latent state
\end{itemize}

\end{frame}


\begin{frame}{Probabilistic PCA as a decoder}

\begin{itemize}
\item Problem: Autoencoders (as PCA) are not probabilistic models:
\begin{itemize}
\item cannot \uured{generate} data.
\item no notion of \uured{uncertainty}
\end{itemize}
\pause
\item We would like something like probabilistic PCA for deep autoencoders
\end{itemize}

\end{frame}

\begin{frame}{probabilistic PCA as an decoder}

\begin{itemize}
\item Remember the pPCA model (with $z$ as latent variable):
\[
x_i \sim N(\uured{b +  W} z_i, \sigma^2 I)
\]
\pause
\item Now, swap the simple parameters with a neural network
\[
x_i \sim N(\uured{\text{NeuralNetwork}}_\theta (z_i), \sigma I)
\]
\pause
\item This is an example of a \uured{Deep Latent Variable model} (DLVM)%(a probabilistic decoder)
\pause
\item One example of a DLVM is the \uured{Variational Autoencoder}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Variational Autoencoder}
\frame{\sectionpage}

\begin{frame}{The Variational Autoencoder (VAE)}
\begin{itemize}
\item The variational autoencoder (VAE) is a \uured{deep probabilistic autoencoder}
\item Used for unsupervised learning of \uured{images}\pause
\item Consists of:
\begin{enumerate}
\item The (probabilistic) encoder $q(z|\phi, x)$: \uured{inference model}\pause
\item Sample $z$ from encoded $x$: the \uured{latent state}\pause
\item The (probabilistic) decoder $p(x|\theta, z)$: \uured{observation model}
\end{enumerate}
\pause
\item Encoding the \uured{latent state as a distribution} forces the space to be "reasonable"/reduces overfitting
\pause
\item VAEs get their name from \uured{variational inference} (used in training)
\end{itemize}

\end{frame}


\begin{frame}{The Variational Autoencoder}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/Rocca_AE_vs_VAE.png}
\caption{Autoencoder vs. the Variational Autoencoder (Rocca, 2019)}
\end{figure}

\end{frame}


\begin{frame}{The Variational Autoencoder}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{fig/Kingma_Welling_2018_Fig_2_1.png}
\caption{The Variational Autoencoder (Kingma and Welling, 2018, Fig. 2.1)}
\end{figure}

\end{frame}


\subsection{The probabilistic decoder}

\begin{frame}{The Prior over the Latent Space}

In a variational autoencoder, the latent variable is assumed to follow a simple prior:
\[
z \sim p(z) = \mathcal{N}(0, I).
\]

This prior defines where encoded data points are allowed to live in latent space.

\begin{itemize}
  \item Prevents arbitrary or irregular latent encodings
  \item Encourages continuity and smooth interpolation
  \item Enables easy generation by sampling from $p(z)$
\end{itemize}

\end{frame}

\begin{frame}{The probabilistic decoder}
\begin{itemize}
\item The probabilistic decoder $p(x|\theta, z)$ (\uured{ observation model})
\item Usually a Normal distribution:
\[
x_i \sim N(\text{NeuralNetwork}(z,\theta), \sigma_\theta^2 I)
\]
\item $x_i$ for observation $i$ depends non-linearly on the latent state $z_i$
\pause
\item A probabilistic linear decoder: \uured{ pPCA}
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/Rocca_VAE_decoder.png}
\caption{The Decoder (Rocca, 2019)}
\end{figure}

\end{frame}


\subsection{The encoder}

\begin{frame}{The probabilistic encoder}

\begin{itemize}
\item The probabilistic encoder $q(z|x, \phi)$ (\uured{inference model})
\pause
\item We assume that $q_\phi(z|x)$ follows a specific distribution. Commonly:
\[
z \sim N(\mu, \Sigma_\phi)
\]
where $\Sigma_\phi = \operatorname{diag}\!\left(\sigma_{1,\phi}^2, \ldots, \sigma_{d,\phi}^2\right)$ (diagonal covariance) in practice.
\pause
\item A neural network learns the parameters $\mu$ and $\Sigma_\phi$
\[
\mu(x) = \text{NeuralNetwork}(x,\phi_\mu)\,,
\]
and
\[
\log \sigma_{d,\phi}^2(x) = \text{NeuralNetwork}(x,\phi_\Sigma)\,.
\]
\pause
\item \uured{ Result}: $z_i$ for observation $i$ depends non-linearly on $x_i$
\end{itemize}

\uured{Note}: the decoder variance is typically fixed (e.g. $\sigma_\theta^2 I$), while the encoder learns a diagonal covariance.

\end{frame}

\begin{frame}{The probabilistic encoder}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/Rocca_VAE_encoder.png}
\caption{The Encoder (Rocca, 2019)}
\end{figure}

\end{frame}



\begin{frame}{The Variational Autoencoder}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/Rocca_VAE2.png}
\caption{The Variational Autoencoder (Rocca, 2019)}
\end{figure}

\end{frame}


\begin{frame}{The Variational Autoencoder}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{fig/Kingma_Welling_2018_Fig_2_1.png}
\caption{The Variational Autoencoder (Kingma and Welling, 2018, Fig. 2.1)}
\end{figure}
\end{frame}

\subsection{Training a variational autoencoder}

\begin{frame}{Training a VAE}
\begin{itemize}
\item \uured{Goal}: estimating $\phi$, $\theta$ (and $z_i$)
\pause
\item The encoder and decoder are (usually) complicated \\ (no close form solution)
\item Estimate $\phi$ and $\theta$ %using \uured{gradient ascent}
\pause
\item Target: \item Maximize $\log p(x)$ (i.e. Explain the data as well as possible)

%\item \uured{Optimization target}: \\ Maximize the variational lower bound or \uured{evidence lower bound (ELBO)}
\end{itemize}
\end{frame}


%\begin{frame}{The marginal log-likelihood}

%\begin{align*}
%\log p_\theta(x)&=\uured{\int q_\phi(z|x)} \log p_\theta(x) \uured{ dz}&  \\
%&=\uured{\mathbb{E}_{q_\phi(z|x)}}[\log p_\theta(x)] \\
%&=\mathbb{E}_{q_\phi(z|x)}\left[\log \left(\uured{\frac{p_\theta(x,z)}{p_\theta(z|x)}}\right)\right]\,, \text{using}\, p(z|x)= \frac{p(x,z)}{p(x)} \\
%&=\mathbb{E}_{q_\phi(z|x)}\left[\log \left(\frac{p_\theta(x,z)}{\uured{q_\phi(z|x)}}\frac{\uured{q_\phi(z|x)}}{p_\theta(z|x)}\right)\right] \\
%&=\mathbb{E}_{q_\phi(z|x)}\left[\log \left(\frac{p_\theta(x,z)}{q_\phi(z|x)}\right)\right] + \mathbb{E}_{q_\phi(z|x)}\left[\log \left(\frac{q_\phi(z|x)}{p_\theta(z|x)}\right)\right] \\
%&=\underbrace{\mathcal{L}_{\theta,\phi}(x)}_{\text{ELBO}} + D_{KL}(q_\phi(z|x)||p_\theta(z|x))\\
%\end{align*}
%\pause
%\begin{align*}
%\underbrace{\mathcal{L}_{\theta,\phi}(x)}_{\text{ELBO}} = \log p_\theta(x) - D_{KL}(q_\phi(z|x)||p_\theta(z|x))
%\end{align*}

%\end{frame}

\begin{frame}{The Kullback--Leibler (KL) Divergence}

\uured{What is it?}

A measure of how well one probability distribution approximates another.

%\vspace{0.3cm}

\uured{Definition}

For two distributions \( p \) and \( q \):
\[
\mathrm{D}_{\mathrm{KL}}(p \,\|\, q)
=
\mathbb{E}_{p}
\left[
\log \frac{p(X)}{q(X)}
\right]
\]

%\vspace{0.3cm}

\uured{Key properties}

\begin{itemize}
  \item \( \mathrm{D}_{\mathrm{KL}}(p \,\|\, q) \ge 0 \)
  \item \( \mathrm{D}_{\mathrm{KL}}(p \,\|\, q) = 0 \) iff \( p = q \)
  \item Not symmetric:
  \( \mathrm{D}_{\mathrm{KL}}(p \,\|\, q) \neq \mathrm{D}_{\mathrm{KL}}(q \,\|\, p) \)
\end{itemize}

% \vspace{0.3cm}

\uured{Interpretation}

\begin{itemize}
  \item Measures information lost when using \( q \) to approximate \( p \)
  \item Direction matters: \emph{which distribution is the truth}
\end{itemize}


\end{frame}


\begin{frame}{From the marginal likelihood to the ELBO}

\uured{Goal:} Compute or maximize the marginal log-likelihood
\[
\log p_{\theta}(x)
\]

%\vspace{0.3cm}

\uured{Step 1: Introduce the latent variable}
\[
\log p_{\theta}(x)
=
\log \int p_{\theta}(x,z)\,dz
\]

% {\small (This integral is usually intractable.)}

% \vspace{0.3cm}

\uured{Step 2: Introduce a variational distribution}
\[
\log p_{\theta}(x)
=
\log \int q_{\phi}(z \mid x)
\frac{p_{\theta}(x,z)}{q_{\phi}(z \mid x)}\,dz = \log \mathbb{E}_{q_{\phi}(z \mid x)}
\left[ \frac{p_{\theta}(x,z)}{q_{\phi}(z \mid x)}\right]
\]

{\small (This equality always holds as long as \( q_{\phi}(z \mid x) > 0 \).)}

%\vspace{0.3cm}

\uured{Step 3: Apply Jensen's inequality} \\$(\text{For a concave function } f,\quad f\bigl(\mathbb{E}[X]\bigr) \ge \mathbb{E}\bigl[f(X)\bigr])$
\[
\log p_{\theta}(x)
\ge
\mathbb{E}_{q_{\phi}(z \mid x)}
\left[
\log \frac{p_{\theta}(x,z)}{q_{\phi}(z \mid x)}
\right]
\]

\end{frame}



\begin{frame}{The Evidence Lower Bound (ELBO)}

\uured{Definition:} The Evidence Lower Bound (ELBO) is
\[
\mathcal{L}_{\theta,\phi}(x)
=
\mathbb{E}_{q_{\phi}(z \mid x)}
\left[
\log \frac{p_{\theta}(x,z)}{q_{\phi}(z \mid x)}
\right]
\]

%\vspace{0.4cm}

\uured{Key relationship:}
\[
\mathcal{L}_{\theta,\phi}(x)
=
\log p_{\theta}(x)
-
\mathrm{D}_{\mathrm{KL}}
\bigl(
q_{\phi}(z \mid x)
\;\|\;
p_{\theta}(z \mid x)
\bigr)
\]

%\vspace{0.3cm}

{\small (KL divergence is always non-negative.)}

%\vspace{0.3cm}

\uured{Interpretation:}
\begin{itemize}
  \item Maximizing the ELBO \emph{increases} \( \log p_{\theta}(x) \)
  \item Minimizes the gap between \( q_{\phi}(z \mid x) \) and \( p_{\theta}(z \mid x) \)
  \item The KL term regularizes the latent space by pushing $q_\phi(z \mid x)$ toward the prior $\mathcal{N}(0, I)$.
\end{itemize}

%\vspace{0.3cm}

\uured{Important:}
\[
\mathcal{L}_{\theta,\phi}(x) \le \log p_{\theta}(x),
\quad
\text{with equality iff }
q_{\phi}(z \mid x) = p_{\theta}(z \mid x)
\]

\end{frame}

%\begin{frame}{The Kullback–Leibler divergence}

%\begin{itemize}
%\item \uured{The Kullback–Leibler divergence}: a way of measuring the "distance" between probability distributions (however not a metric)
%\[
%D_{KL}(q_\phi(z|x)||p_\theta(z|x)) = \mathbb{E}_{q_\phi(z|x)}\left[\log \left(\frac{q_\phi(z|x)}{p_\theta(z|x)}\right)\right]
%\]

%\[
%D_{KL}(q_\phi(z|x)||p_\theta(z|x)) \geq 0
%\]
%\end{itemize}

%\end{frame}


%\begin{frame}{Training target}

%\begin{itemize}
%\item \uured{Optimization target}: Maximize the ELBO
%\begin{align*}
%\mathcal{L}_{\theta,\phi}(x) = \log p_\theta(x) - D_{KL}(q_\phi(z|x)||p_\theta(z|x))
%\end{align*}
%\item ELBO is a lower bound for the marginal log-likelihood%\\ (similar to the EM algorithm)
%\pause
%\item Maximizing the ELBO will do two things:
%\begin{itemize}
%\item Maximize the marginal log-likelihood $\log p_\theta(x)$:\\Better generative model/decoder
%\item Minimize the KL-divergence between $q_\phi(z|x)$ and $p_\theta(z|x)$:\\Better approximation of the latent space/encoder
%\end{itemize}
%\end{itemize}

%\end{frame}

\begin{frame}{Optimizing the ELBO}

\uured{Objective}

We want to maximize the ELBO with respect to encoder and decoder parameters:
\[
\max_{\theta,\phi}\;
\mathcal{L}_{\theta,\phi}(x)
=
\mathbb{E}_{q_{\phi}(z \mid x)}
\left[
\log p_{\theta}(x,z)
-
\log q_{\phi}(z \mid x)
\right]
\]

%\vspace{0.4cm}
\pause
\uured{Problem}

\begin{itemize}
  \item The expectation is over \( q_{\phi}(z \mid x) \)
  \item We need gradients with respect to \( \phi \) and \( \theta \)
  \item Direct differentiation through sampling is not possible
\end{itemize}

% \vspace{0.3cm}
\pause

\uured{Solution}

\begin{itemize}
  \item Approximate the expectation using Monte Carlo
  \item Rewrite the sampling step so gradients can flow (reparameterization trick)
\end{itemize}

\end{frame}

\begin{frame}{The Reparameterization Trick}

\uured{Assumption}

The variational posterior is Gaussian:
\[
q_{\phi}(z \mid x) = \mathcal{N}\bigl(\mu_{\phi}(x), \sigma^2_{\phi}(x) I\bigr)
\]

%\vspace{0.3cm}

\uured{Key idea}

Instead of sampling \( z \sim q_{\phi}(z \mid x) \), write:
\[
z = g(\phi, x, \epsilon) = \mu_{\phi}(x) + \sigma_{\phi}(x) \odot \epsilon,
\quad
\epsilon \sim \mathcal{N}(0,I)
\]

%\vspace{0.3cm}

\uured{Why this helps}

\begin{itemize}
  \item Randomness is now isolated in \( \epsilon \)
  \item \( z = g(\phi, x, \epsilon)\) is a differentiable function of \( \phi \)
  \item \uured{Enables backpropagation through the sampling step}
\end{itemize}

\end{frame}

\begin{frame}{Monte Carlo Optimization of the ELBO}

\uured{ELBO rewritten}

\[
\mathcal{L}_{\theta,\phi}(x)
=
\mathbb{E}_{p(\epsilon)}
\left[
\log p_{\theta}(x, g(\phi,x,\epsilon))
-
\log q_{\phi}(g(\phi,x,\epsilon) \mid x)
\right]
\]

% \vspace{0.3cm}

\uured{Monte Carlo approximation}

Using one sample \( \epsilon^{(1)} \sim \mathcal{N}(0,1) \):
\[
\mathcal{L}_{\theta,\phi}(x)
\approx
\log p_{\theta}(x, g(\phi,x,\epsilon^{(1)}) )
-
\log q_{\phi}(g(\phi,x,\epsilon^{(1)}) \mid x)
\]

% \vspace{0.3cm}

\uured{Optimization}

\begin{itemize}
  \item Use stochastic gradient ascent (or descent on \(-\)ELBO)
  \item Gradients flow through \( \mu_{\phi}(x) \) and \( \sigma_{\phi}(x) \) and we can compute $ \nabla \mathcal{L}_{\theta,\phi}(x)$
  \item One or few samples $\epsilon$ per datapoint is usually sufficient
\end{itemize}

\end{frame}


%\begin{frame}{Optimizing the ELBO}

%\begin{itemize}
%\item \uured{Stochastic Gradient Ascent} to maximize:
%\begin{align*}
%\mathcal{L}_{\theta,\phi}(x) =& \sum_i^N \mathcal{L}_{\theta,\phi}(x_i)\\
%=& \sum_i^N \mathbb{E}_{q_\phi(z_i|x_i)}\left[\log \left(p_\theta(x_i,z_i)\right) - \log(q_\phi(z_i|x_i))\right]
%\end{align*}
%\pause
%\item Two problems:
%\begin{enumerate}
%\item How do we compute the expectation? \\ Solution: \uured{ Monte Carlo Approximation}\pause
%\item How compute the gradient wrt $\phi$?\\ Solution: \uured{Change of variables}: $z = g(\epsilon, \phi, x)$\\This is called \uured{the reparametrization trick}
%\end{enumerate}
%\end{itemize}

%\end{frame}

%\begin{frame}{Optimizing the ELBO}
% Im here. TODO Clarify the steps
%\begin{itemize}
%\item Using the reparametrization trick and Monte Carlo approximation, we get:
%\begin{align*}
%\mathcal{L}_{\theta,\phi}(x) =& \mathbb{E}_{q_\phi(z|x)}\left[\log \left(p_\theta(x,z)\right)- \log(q_\phi(z|x))\right]\\
% =& \mathbb{E}_{p(\epsilon)}\left[\log \left(p_\theta(x,g(\epsilon, \phi, x))\right)- \log(q_\phi(g(\epsilon, \phi, x)|x))\right]\\
% \approx & \log \left(p_\theta(x,g(\epsilon, \phi, x))\right)- \log(q_\phi(g(\epsilon, \phi, x)|x))
%\end{align*}
%\pause
%\item A common approach: do the MC approximation with only \uured{one sample per datapoint $x_i$}.\pause
%\item unbiasedness
%\item We approximate both $\mathcal{L}_{\theta,\phi}(x)$ and $\nabla\mathcal{L}_{\theta,\phi}(x)$
%\pause
%\item Sometimes called a \uured{doubly stochastic} algorithm.
%\end{itemize}

%\end{frame}


\begin{frame}{The Autoencoding Variational Bayes Algorithm}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/Kingma_Welling_2018_Algo_1.png}
\caption{The Autoencoding Variational Bayes Algorithm (Kingma and Welling, 2018, Algo. 1)}
\end{figure}

\end{frame}


\begin{frame}{The Autoencoding Variational Bayes Algorithm}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/Rocca_VAE_full_reparmetr.png}
\caption{The Autoencoding Variational Bayes Algorithm (Rocca, 2019)}
\end{figure}

\end{frame}

\begin{frame}{Summary}

\begin{itemize}
\item Benefits of VAE:
\begin{itemize}
\item Get a more \uured{interpretable} latent state
\item We can estimate \uured{uncertainty} (but its usually bad)
\item \uured{Regularize} the latent state
\item We can inject knowledge in our latent state
\end{itemize}
\pause
\item Still much ongoing research
% \item Diffusion models is getting increased attention as a rival method
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/Vahdat_Kautz_NVEA_2020.png}
\caption{Examples of images generated with a deep hierarchical Variational Autoencoder (Vahdat and Kautz, 2020)}
\end{figure}

\end{frame}

\begin{frame}{VAE vs Diffusion Models}

\uured{Variational Autoencoders (VAEs)}
\begin{itemize}
  \item Latent variable model: \( z \sim p(z) \)
  \item Trained by maximizing the ELBO
  \item Fast sampling (one forward pass)
  \item Compact, interpretable latent space
  \item Often produce blurrier samples
\end{itemize}

\uured{Diffusion Models}
\begin{itemize}
  \item No explicit low-dimensional latent space
  \item Learn to reverse a noise process
  \item Trained via denoising objectives
  \item Very high-quality samples
  \item Slow sampling (many steps)
\end{itemize}


%\vspace{0.3cm}

\uured{Takeaway:}
VAEs prioritize efficient inference and representation learning,
while diffusion models prioritize sample quality.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probabilistic Topic Models}
\frame{\sectionpage}

\begin{frame}{Probabilistic Topic Models}

\begin{itemize}
    \item Unsupervised method for \uured{ textual data}\pause
    \item Popular in industry and academia to \uured{ analyze large corpora}\pause
    \item The most common model: \uured{ Latent Dirichlet Allocation}
    \item A \uured{ mixed membership} model (a mixture of multinomial mixtures model)\pause
    \item Topic model builds on the \uured{distributional hypothesis}\pause
    \item Use cases:
    \begin{itemize}
        \item \uured{Explore} document collections\pause
        \item \uured{Analyzing large corpora} using statistical methods\pause
    \end{itemize}
    \item Example: \href{https://en.allears.ai/}{\uured{All ears}} media monitoring of speech data
\end{itemize}
\end{frame}



\begin{frame}{The Dirichlet Distribution}

\begin{itemize}
    \item Probability distribution over the simplex with $K$ categories:
\[
f({\boldsymbol {x} }| {\boldsymbol {\alpha }}) = {\frac {1}{\mathrm {B} ({\boldsymbol {\alpha }})}} \prod _{i=1}^{K}x_{i}^{\alpha _{i}-1}\,, \text{ where }
{\displaystyle \mathrm {B} ({\boldsymbol {\alpha }})={\frac {\prod _{i=1}^{K}\Gamma (\alpha _{i})}{\Gamma {\bigl (}\sum _{i=1}^{K}\alpha _{i}{\bigr )}}}}\,,
\]
and  ${\displaystyle {\boldsymbol {\alpha }}=(\alpha _{1},\ldots ,\alpha _{K})}$
\pause
\item The probability distribution has the support on the simplex, that is
\[
    \sum_{i=1}^{K}x_{i}=1{\text{ and }}x_{i}\geq 0{\text{ for all }} i\in [1,K]
\]
\pause
\item The parameters ${\boldsymbol {\alpha }}$ can be seen as \uured{ pseudo-counts} and
\[
\mathbb{E}[x_i] = \frac{\alpha_i}{\sum_{j=1}^K \alpha_j}
\]
\end{itemize}
\end{frame}


\begin{frame}{The Dirichlet Distribution}


\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/dirichlet.jpg}
\caption{The Dirichlet Distribution (Wikipedia)}
\end{figure}

% Interactive plot \href{https://chart-studio.plotly.com/~david_avakian/14.embed}{\uured{here}}.

\end{frame}



\begin{frame}{The distributional hypothesis}

\begin{itemize}
    \item Harris (1954) and Firth (1957): \\ ``Word is characterized by the company it keeps''
    \pause
    \item Semantics (broadly defined) is captured by \uured{ context}\pause
    \item Rough definition: \uured{ word windows} of different sizes\pause
    \item Different window sizes, different \uured{ semantic} content:
    \begin{itemize}
      \item Word embeddings (context: word windows)
      \item Topic models (context: documents)
    \end{itemize}
\end{itemize}

\begin{example}
\begin{enumerate}
    \item ``A friend in need is a friend indeed.''
    \item ``She is my friend indeed.''
\end{enumerate}
\end{example}
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Latent Dirichlet Allocation}

\begin{frame}{Latent Dirichlet Allocation}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/lda_model.png}
\caption{The Latent Dirichlet Allocation Model}
\end{figure}
where $\phi_k$ is the $k$th row in $\Phi$ (of dimension $K \times V$) and $\theta_d$ is the $d$th row in $\Theta$ (of dimension $D \times K$).

\end{frame}

\begin{frame}
\frametitle{Generative model for LDA}


Relies on the \uured{ bag-of-word} assumption
% Not much better with BERT embeddings


\begin{enumerate}
    \item For each component $k$ to $K$:
    \begin{enumerate}
        \item $\phi_k \sim$ Dirichlet($\beta$)
    \end{enumerate}
    \item For each document $d$:
    \begin{enumerate}
        \item $\theta_d \sim$ Dirichlet($\alpha$)
        \item For each token $i$:
        \begin{enumerate}
            \item $z_{id} \sim$  Categorical($\theta_d$)
            \item $w_{id} \sim$  Categorical($\phi_{z_{id}}$)
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

\end{frame}


\begin{frame}{Example of parameters  $\mathbf{z}$, $\Theta$ and $\Phi$}

\[
\begin{array}{ccccccc}
\mathbf{w}_{1} &  & \mbox{boat} & \mbox{shore} & \mbox{bank}\\
\mathbf{z}_{1} &  & 1 & 1 & 1\\
\mathbf{w}_{2} &  & \mbox{Zlatan} & \mbox{boat} & \mbox{shore} & \mbox{money} & \mbox{bank}\\
\mathbf{z}_{2} &  & 2 & 1 & 1 & 3 & 3\\
\mathbf{w}_{3} &  & \mbox{money} & \mbox{bank} & \mbox{soccer} & \mbox{money}\\
\mathbf{z}_{3} &  & 3 & 3 & 2 & 3
\end{array}
\]
\pause
\[
\Phi=\begin{array}{ccccccc}
 & \mbox{boat} & \mbox{shore} & \mbox{soccer} & \mbox{Zlatan} & \mbox{bank} & \mbox{money}\\
\text{Topic 1} & 0.35 & 0.35 & 0.05 & 0.05 & 0.15 & 0.05\\
\text{Topic 2} & 0.025 & 0.025 & 0.45 & 0.45 & 0.025 & 0.025\\
\text{Topic 3} & 0.025 & 0.025 & 0.025 & 0.025 & 0.45 & 0.45
\end{array}
\]
\pause
\[
\Theta=\begin{array}{cccc}
 & \text{Topic 1} & \text{Topic 2} & \text{Topic 3}\\
\text{doc 1} & 0.96 & 0.02 & 0.02\\
\text{doc 2} & 0.3 & 0.2 & 0.5\\
\text{doc 3} & 0.05 & 0.35 & 0.6
\end{array}
\]


\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% EXAMPLE OF LDA


\begin{frame}
\frametitle{}

%\begin{displayquote}
Closing arguments were heard yesterday in the Federal bankruptcy fraud trial of Stephen J. Sabbeth, whose legal problems have raised doubts about his ability to continue as leader of the Nassau County Democratic Party.

\medskip

Mr. Sabbeth is charged with trying to conceal \$750,000 from his bank creditors by hiding the money in a secret account in his wife's maiden name, rather than use it to pay creditors when his lumber business went into bankruptcy 10 years ago.

%\end{displayquote}

\hspace{10mm} -- The New York Times 25th of Febuary 1999

\end{frame}

\begin{frame}
\frametitle{The estimated topic proportion $(\hat{\theta_d})$}

\begin{center}
\includegraphics[width=0.7\textwidth]{fig/topic_prop.png}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Topic top words}

\input{tables/word_type_topics.tex}

\end{frame}

\begin{frame}
\frametitle{The Latent Dirichlet Allocation Model}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\textwidth]{fig/Blei2012_fig_1.png}
\caption{The Latent Dirichlet Allocation Model (Blei 2012, Fig. 1)}
\end{center}
\end{figure}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimating the LDA model}

\begin{frame}
\frametitle{Inference}

\begin{itemize}
\item Common inference approaches

\begin{enumerate}
\item Variational inference %\cite{blei2003latent}
\item Markov Chain Monte Carlo (MCMC) %\cite{griffiths2004finding}
\end{enumerate}
\pause
\item The Gibbs sampler is usually prefered
\pause
\item Similar to (Stochastic) EM
\end{itemize}

\end{frame}



\begin{frame}{Gibbs sampler for LDA}

The basic Gibbs sampler:
\begin{enumerate}
\item We want to estimate $z, \Phi, \Theta$:
\pause
\item Sample topic indicators (latent variable)
\[
p(z=k|\Phi,\Theta)\propto\phi_{v,k}\theta_{k,d}
\]
\pause
\item Sample model parameters
\[
\theta_{d}|\mathbf{z}\sim Dir(\mathbf{n}^{(d)}+\alpha)
\]
\[
\phi_{k}|\mathbf{z}\sim Dir(\mathbf{n}^{(v)}+\beta)
\]
\end{enumerate}

where $\mathbf{n}^{(d)}$ is the number of tokens by topic in document
$d$ and $\mathbf{n}^{(v)}$ is the number of tokens by topic for word
type $v$.
\end{frame}

\begin{frame}{Gibbs sampler for LDA}

Integrating out (collapsing) $\Theta$ and $\Phi$ %(\citet{Griffiths2004}):
\begin{eqnarray*}
p(\mathbf{z}|\mathbf{w}) & = & \int\int p(\mathbf{z},\Theta,\Phi|\mathbf{w})\cdot p(\mathbf{z},\Theta,\Phi)d\mbox{\ensuremath{\Phi}}d\mbox{\ensuremath{\Theta}}
\end{eqnarray*}
will result in the following Gibbs sampler:

\[
p(z_{i}=k|w_{i},\mathbf{z}_{\lnot i})\propto\underbrace{\frac{n_{k}^{(v)}+\beta}{n_{k}^{(v)}+V\beta}}_{type-topic\mbox{ }\mbox{(\ensuremath{\Phi})}}\cdot\underbrace{(n_{k}^{(d)}+\alpha)}_{topic-doc\mbox{ }\mbox{(\ensuremath{\Theta})}}
\]
where $n^{(v)}$ and $n^{(d)}$ are count matrices of size $D\times K$
and $K\times V$.
\end{frame}


\begin{frame}{Example of $n^{(v)}$ and $n^{(d)}$}

\[
\begin{array}{ccccccc}
\mathbf{w}_{1} &  & \mbox{boat} & \mbox{shore} & \mbox{bank}\\
\mathbf{z}_{1} &  & 1 & 1 & 1\\
\mathbf{w}_{2} &  & \mbox{Zlatan} & \mbox{boat} & \mbox{shore} & \mbox{money} & \mbox{bank}\\
\mathbf{z}_{2} &  & 2 & 1 & 1 & 3 & 3\\
\mathbf{w}_{3} &  & \mbox{money} & \mbox{bank} & \mbox{soccer} & \mbox{money}\\
\mathbf{z}_{3} &  & 3 & 3 & 2 & 3
\end{array}
\]


\pause{}

\[
n^{(v)}=\begin{array}{cccccc}
\mbox{boat} & \mbox{shore} & \mbox{soccer} & \mbox{Zlatan} & \mbox{bank} & \mbox{money}\\
2 & 2 & 0 & 0 & 1 & 0\\
0 & 0 & 1 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 2 & 2
\end{array}
\]


\pause{}

\[
n^{(d)}=\left[\begin{array}{ccc}
3 & 0 & 0\\
2 & 1 & 3\\
0 & 2 & 3
\end{array}\right]
\]

\end{frame}



% \begin{frame}{Topic Models as non-negative matrix factorization}
%\Small
%\begin{figure}
% \begin{centering}
% \[
% \begin{array}{ccccc}
% \\
% \left[\begin{array}{ccc}
%  & \text{ }\\
% \\
% \text{ } & n_{dv} & \text{ }\\
%  & (D\times V)\text{ }\\
% \\
% \end{array}\right] & \approx & \left[\begin{array}{c}
% \\
% \\
% \Theta\\
% (D\times K)\\
% \\
% \end{array}\right] & \times & \left[\begin{array}{ccccc}
%  &  & \text{ }\\
% \text{ } & \text{ } & \Phi & \text{ } & \text{ }\\
%  &  & (K\times V)
% \end{array}\right]\\
% \\
% \end{array}
% \]
% \end{centering}
%\caption{Conceptual depiction of LDA as a matrix decomposition.}
%\label{matrix_decomposition_view}
%\end{figure}

% \end{frame}

\begin{frame}{Practicalities}

\begin{itemize}
    \item Setting $K$, $\alpha$ and $\beta$ \pause
    \item \uured{ Reducing the vocabulary}: stopwords, rare words, stemming \pause
    \item "Junk" topics\pause
    \item We can analyze the topic indicators $z$ directly
\end{itemize}
\end{frame}




\begin{frame}{Research Example: Swedish Immigration Discourse}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\textwidth]{fig/weekly_yearly_tot_outliers.png}
\caption{The Immigration topic in Swedish Newspapers (Hurtado Bodell et al, 2024)}
\end{center}
\end{figure}

\end{frame}


\begin{frame}{Summary: Topic Models}

\begin{itemize}
    \item Topic models are \uured{unsupervised} models for textual data\pause
    \item The \uured{Latent Dirichlet Allocation} is a popular model\pause
    \item A \uured{mixed membership model} (a mixture of multinomial mixtures model)\pause
    \item Use Gibbs samplers for estimation
\end{itemize}
\end{frame}

\end{document}
