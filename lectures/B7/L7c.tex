%\documentclass[10pt,handout]{beamer}
\documentclass[10pt]{beamer}
\usepackage{babel} % Anpassa efter svenska. Ger svensk logga.
\usepackage[utf8]{inputenc} % Anpassa efter linux
\usepackage{graphicx}
\usepackage{../common/beamerthemeUppsala}
%\usecolortheme{UU} % Anpassa efter UU:s frger och logga
%\hypersetup{pdfpagemode=FullScreen} % Adobe Reader ska ppna fullskrm
\setbeamertemplate{itemize items}[circle]

% \usepackage{beamerthemesplit}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{epsfig}
% \usepackage[latin1]{inputenc}
 \usepackage{color}
% \usepackage{fancybox}
% \usepackage{psfrag}
% \usepackage[english]{babel}
 \setbeamertemplate{footline}{\hfill\insertframenumber/\inserttotalframenumber}

%library(tinytex)
%tlmgr_install('booktabs')
\usepackage{booktabs}

%library(tinytex)
%tlmgr_install('csquotes')
%\usepackage{csquotes}


% Read in commands
\input{../common/commands.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\parskip}{3mm}
\title[]{{\color{black}Machine learning -- Block 7c}}
\author[]{M{\aa}ns Magnusson\\Department of Statistics, Uppsala University}
\date{\currentsemester}


\begin{document}

\frame{\titlepage
% \thispagestyle{empty}
}

%========================================================
\section{Diffusion Models}
%========================================================

\begin{frame}
\frametitle{Diffusion Models}

\begin{itemize}
\item A new class of deep probabilistic generative models \pause
\item State-of-the-art for image generation \pause
\item Reading: Bishop \& Bishop, Chapter 20. \pause
\item Closely related to variational autoencoders
\end{itemize}

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{Generative Models Recap}

\begin{itemize}
\item (Deep) latent variable models:
\begin{itemize}
    \item Variational Autoencoders (VAEs)
%    \item Generative Adversarial Networks (GANs)
%    \item Normalizing Flows
    \item \alert{Diffusion Models}
\end{itemize}
\item All transform a simple (latent) distribution into complex data \pause
\item \uured{Main difference}: how this transformation is learned
\end{itemize}

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{Why Diffusion Models?}

\begin{itemize}
\item Stable training
\item Excellent sample quality
\item Main drawback: \uured{slow sampling}
\end{itemize}

Demo: DALLÂ·E 3: \href{https://openai.com/dall-e-3}{https://openai.com/dall-e-3}

\end{frame}

%========================================================
\subsection{Core Idea}
%========================================================

\begin{frame}
\frametitle{High-Level Idea}

\begin{itemize}
\item Two processes:
\begin{enumerate}
    \item \textbf{Forward process:} gradually add noise to data
    \item \textbf{Reverse process:} learn to remove noise
\end{enumerate}
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{fig/Fig_20_1_Bishop2.png}
\caption{Figure 20.1 from Bishop \& Bishop (2024).}
\end{figure}

\pause
\begin{itemize}
\item Generation starts from pure noise
\item Data is generated by iteratively denoising
\end{itemize}

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{Diffusion Models and VAEs}

\begin{itemize}
\item Diffusion models can be viewed as hierarchical VAEs
\item Key differences:
\begin{itemize}
    \item Encoder is \uured{fixed} (noise process)
    \pause
    \item Decoder is learned (denoising network)
    \pause
    \item Many latent variables instead of a few
\end{itemize}
\end{itemize}

\end{frame}

%========================================================
\subsection{Forward Process (Encoder)}
%========================================================

\begin{frame}
\frametitle{Forward Diffusion Process}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{fig/Fig_20_2_Bishop2.png}
\caption{Figure 20.2 from Bishop \& Bishop (2024).}
\end{figure}

\begin{itemize}
\item Start with data point $x$
\item Add small Gaussian noise repeatedly
\item After many steps: result is Gaussian noise
\end{itemize}



\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{Forward Process: Single Step}

For $t = 1, \dots, T$:
\[
z_t = \sqrt{1 - \beta_t} z_{t-1} + \sqrt{\beta_t}\,\varepsilon_t,
\quad \varepsilon_t \sim \mathcal{N}(0, I)
\]

\begin{itemize}
\item $\beta_t$ controls noise level
\item Noise increases with $t$
\item Defines a Markov chain
\end{itemize}

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{Probabilistic Encoder}

\begin{itemize}
\item The forward process defines:
\[
q(z_t \mid z_{t-1}) = \mathcal{N}\left(
\sqrt{1-\beta_t}\,z_{t-1}, \beta_t I
\right)
\]
\item This plays the role of an \textbf{encoder}
\item Unlike VAEs: encoder is fixed
\end{itemize}

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{Closed-Form Noising}

Key result:
\[
z_t = \sqrt{\alpha_t} x + \sqrt{1-\alpha_t}\,\varepsilon,
\quad \varepsilon \sim \mathcal{N}(0, I)
\]

\begin{itemize}
\item $\alpha_t = \prod_{\tau=1}^t (1 - \beta_\tau)$
\item Allows direct sampling of $z_t$ from $x$
\item Very important for training
\end{itemize}

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{End of Forward Process}

\begin{itemize}
\item As $t \rightarrow T$:
\[
z_T \sim \mathcal{N}(0, I)
\]
\item All information about $x$ is lost
\item This distribution is easy to sample from
\end{itemize}

\end{frame}

%========================================================
\subsection{Reverse Process (Decoder)}
%========================================================

\begin{frame}
\frametitle{Learning the Reverse Process}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{fig/Fig_20_2_Bishop2.png}
\caption{Figure 20.2 from Bishop \& Bishop (2024).}
\end{figure}

\begin{itemize}
\item Goal: reverse the noise process
\[
p(z_{t-1} \mid z_t)
\]
\item Exact reverse distribution is intractable
\item We learn an approximation using a neural network
\end{itemize}

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{Probabilistic Decoder}

We model:
\[
p(z_{t-1} \mid z_t, w) =
\mathcal{N}(\mu(z_t, t), \beta_t I)
\]

\begin{itemize}
\item Mean predicted by neural network
\item Variance often fixed
\item Decoder shared across all steps
\end{itemize}

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{Sampling from the Model}

\begin{enumerate}
\item Sample $z_T \sim \mathcal{N}(0, I)$
\item For $t = T, \dots, 1$:
\begin{itemize}
    \item Sample $z_{t-1} \sim p(z_{t-1} \mid z_t)$
\end{itemize}
\item Final sample corresponds to data point $x$
\end{enumerate}

\end{frame}


\begin{frame}
\frametitle{Sampling from the Model}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{fig/Algo_20_2_generation.png}
\caption{Algorithm 20.2 from Bishop \& Bishop (2024).}
\end{figure}

\end{frame}





%========================================================
\subsection{Training}
%========================================================

\begin{frame}
\frametitle{Training Objective}

\begin{itemize}
\item Exact likelihood is intractable
\item Maximize an Evidence Lower Bound (ELBO)
\item Very similar to VAE training
\end{itemize}

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{ELBO Interpretation}

\begin{itemize}
\item Reconstruction term
\item Consistency terms between forward and reverse processes
\item Encoder distribution is fixed
\item Only decoder parameters are learned
\end{itemize}

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{Predicting Noise Instead of Data}

Key idea:
\begin{itemize}
\item Instead of predicting $z_{t-1}$
\item Predict the noise $\varepsilon$ added to $x$
\end{itemize}

\[
z_t = \sqrt{\alpha_t}x + \sqrt{1-\alpha_t}\varepsilon
\]

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{Final Training Loss}

The loss simplifies to:
\[
\mathbb{E}_{x,t,\varepsilon}
\left[
\| \varepsilon - g(z_t, t) \|^2
\right]
\]

\begin{itemize}
\item Simple squared error loss
\item Very stable optimization
\item Central result of diffusion models
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Training a Denoising diffusion model}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{fig/Algo_20_1_training.png}
\caption{Algorithm 20.1 from Bishop \& Bishop (2024).}
\end{figure}

\end{frame}

%========================================================
\subsection{Summary}
%========================================================

\begin{frame}
\frametitle{Diffusion Models vs VAEs}

\begin{center}
\begin{tabular}{lcc}
\hline
 & VAE & Diffusion \\
\hline
Encoder & Learned & Fixed \\
Latent dim. & Low & High \\
Decoder & One step & Many steps \\
Training & ELBO & ELBO \\
Sampling & Fast & Slow \\
\hline
\end{tabular}
\end{center}

\end{frame}

%--------------------------------------------------------


%========================================================
\subsection{Conditional Diffusion Models}
%========================================================

\begin{frame}
\frametitle{Conditional Diffusion Models}

\begin{itemize}
\item So far: unconditional generation
\begin{itemize}
    \item Sample images from $p(x)$
\end{itemize}
\item Often we want \alert{conditional generation}
\begin{itemize}
    \item Generate images given text
    \item Generate images given class labels
\end{itemize}
\item Goal: model $p(x \mid c)$, where $c$ is conditioning information
\end{itemize}

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{Conditioning on Text}

\begin{itemize}
\item Let $c$ be a text description
\item Text is encoded using a language model
\[
c \;\rightarrow\; \text{embedding } e(c)
\]
\item Diffusion model is conditioned on this embedding
\item Reverse process becomes:
\[
p(z_{t-1} \mid z_t, c)
\]
\end{itemize}

\end{frame}

%--------------------------------------------------------

\begin{frame}
\frametitle{How Conditioning Is Used}

\begin{itemize}
\item Conditioning information is provided to the neural network
\item Noise prediction network becomes:
\[
g(z_t, t, c)
\]
\item Intuition:
\begin{itemize}
    \item Noise removal is guided by the text
    \item Different texts lead to different denoising trajectories
\end{itemize}
\item Used in systems such as:
\begin{itemize}
    \item Text-to-image generation
    \item Image editing and inpainting
\end{itemize}
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{fig/Fig_20_8_Bishop2.png}
\caption{Figure 20.8 from Bishop \& Bishop (2024).}
\end{figure}

\end{frame}


\begin{frame}
\frametitle{How Conditioning Is Used}

\begin{itemize}
\item Used in systems such as:
\begin{itemize}
    \item Text-to-image generation
    \item Image editing and inpainting
\end{itemize}
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/Fig_20_8_Bishop2.png}
\caption{Figure 20.8 from Bishop \& Bishop (2024).}
\end{figure}

\end{frame}


%========================================================
\begin{frame}
\frametitle{Diffusion Ideas Beyond Images}

\begin{itemize}
\item Diffusion models popularized a key idea:
\begin{itemize}
    \item Learn complex objects by \uured{iterative denoising}
\end{itemize}

\pause

\item \uured{AlphaFold~2} applies a related principle to protein structures:
\begin{itemize}
    \item Protein geometry is refined step-by-step
    \item Each step reduces uncertainty and inconsistency
\end{itemize}

\item Important distinction:
\begin{itemize}
    \item AlphaFold~2 is \uured{not} a Denoising Diffusion Probabilistic Model
    \item But it uses a diffusion-\emph{like} denoising process
\end{itemize}

\pause

\item New generative biology models now use \uured{explicit diffusion models}
\begin{itemize}
    \item Protein backbone generation (Watson et al, 2023)
    \item Molecular design (Hoogeboom et al, 2022)
\end{itemize}

\end{itemize}

\end{frame}
%========================================================

\begin{frame}
\frametitle{Key Takeaways}

\begin{itemize}
\item Diffusion models are deep probabilistic models
\item Closely related to VAEs
\item Built around denoising Gaussian noise
\item Foundation for modern image generation systems conditional on textual input
\end{itemize}

\end{frame}

\end{document}
