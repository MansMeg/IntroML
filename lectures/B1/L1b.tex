%\documentclass[10pt,handout]{beamer}
\documentclass[10pt]{beamer}
\usepackage[english]{babel} % Anpassa efter svenska. Ger svensk logga.
\usepackage[utf8]{inputenc} % Anpassa efter linux
\usepackage{graphicx}
\usepackage{../common/beamerthemeUppsala}
%\usetheme{Uppsala}
%\usecolortheme{UU} % Anpassa efter UU:s frger och logga
%\hypersetup{pdfpagemode=FullScreen} % Adobe Reader ska ppna fullskrm
\setbeamertemplate{itemize items}[circle]

% \usepackage{beamerthemesplit}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{epsfig}
% \usepackage[latin1]{inputenc}
 \usepackage{color}
% \usepackage{fancybox}
% \usepackage{psfrag}
% \usepackage[english]{babel}

\usepackage{array}
%\usepackage{multirow}

 \setbeamertemplate{footline}{\hfill\insertframenumber/\inserttotalframenumber}

% Read in commands
\input{../common/commands.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\parskip}{3mm}
\title[]{{\color{black}Machine learning -- Block 1(b)}}
\author[]{M{\aa}ns Magnusson\\Department of Statistics, Uppsala University}
\date{\currentsemester}


\begin{document}

\frame{\titlepage
% \thispagestyle{empty}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{This weeks lectures}
\begin{itemize}
\item Regularization
\item Model Selection and Assement
\item Cross-Validation
\item Evaluate classification models
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Predictive Performance}
% What is predictive performance?
\frame{\sectionpage}

\begin{frame}{Previous Model Evaluation}
\begin{itemize}
\item In the past, tools for assessing models, e.g.:
\begin{itemize}
%\item Various plots
\item Residuals
\item Leverage, Cook's distance
\item p-values
\item $R^2$
\item AIC
\end{itemize}
\item Model diagnoses and \uured{how well the model fits the data}.
\item In statistics: focus on \uured{estimation} and \uured{attribution}.
\end{itemize}
\end{frame}


\begin{frame}{Predictive Performance}

% We are looking at predictive performance here

\begin{itemize}
\item Supervised learning: focus on \uured{predictive performance}:
\item How well our model $\hat{f}$ trained on
\[
\mathcal{T}=\{(y_i, x_i), i = 1, ..., n\}
\]
work when predicting a new observation $y_0$ from the data generating process $P_{y,x}$.
\[
\E(L(Y_0,X_0)|\mathcal{T}) = \int L(Y_0, \hat{f}(X_0)) P_{y,x} d(Y_0,X_0) |\mathcal{T}
\]
%\pause
\item ability to perform well on previously unobserved inputs is called \uured{generalization}

%\begin{exampleblock}{}
%  {\large ``The generalization performance of a learning method relates to its prediction capability on \emph{independent} test data.''}
%  \vskip5mm
%  \hspace*\fill{\small--- Hastie et al., (2017, p. 219)}
%\end{exampleblock}

%\pause

\item Models can be overly \uured{optimistic}\footnote{See e.g. Picard, R.R., Cook, R.D. (1984). Cross-validation of regression models. \emph{Journal of the American Statistical Association}, \textbf{79(387)}, 575--583.}:
\begin{itemize}
\item explain training data well
\item poor generalizability
\end{itemize}
\item a phenomenon known as \uured{overfitting}.
\end{itemize}

\end{frame}

\section{Measuring Performance}
\frame{\sectionpage}
% How do we measure predictive performance?

\begin{frame}{Loss Functions}

% We are looking at predictive performance here


\begin{itemize}

\item To assess the performance we use the loss function for a new unseen observation $y_0$ and the prediction of that observation $\hat{f}(x_0)$
\[
L(y_0,\hat{f}(x_0))
\]
\pause
\item This is quite general and we choose based $L$ based on what we want the model performe well on.\pause
\item Examples:
\begin{itemize}
\item Regression problems (squared loss/error):
\[
    L(y_0,\hat{f}(x_0)) = (y_0 - \hat{f}(x_0))^2
\]
\pause
\item Classification (0-1 loss)
\[
    L(y_0,\hat{f}(x_0)) = I(y_0 = \hat{f}(x_0))
\]
%\pause
%\item Language models: Perplexity
\end{itemize}


\end{itemize}

\end{frame}


\begin{frame}{Cross-Entropy Loss}

\begin{itemize}
\item When we predict probabilities $\hat{f}(x_0)=\hat{p}$:
\[
L(y_0, \hat{p}) = - (y_0 \log{\hat{p}}) + ((1 - y_0) \log{(1- \hat{p})})
\]
\uured{Question:} Do you recognize the (cross-entropy) loss function?
% Bernoulli negative log-likelihood?
\pause
\item Maximizing the likelihood is the same as minimizing the cross-entropy. \pause
\item Multi class cross-entroy over $M$ classes
\[
L(\mathbf{y}_0, \hat{\mathbf{p}}) = - \sum^M_{j=1} y_{0,j} \log{\hat{p}_j}
\]
\end{itemize}

\end{frame}


\begin{frame}{The Confusion Matrix}

\begin{itemize}
\item A common way to present performance in classification is \uured{the confusion matrix}:
\end{itemize}
\centering
\begin{tabular}{lcc}
  & Prediction &  \\
  Actual & Positive & Negative \\
  Positive & True Positive (TP) & False Negative (FN) \\
  Negative & False Positive (FP) & True Negative ((TN)
\end{tabular}


\end{frame}

\begin{frame}{The Confusion Matrix: Multi-class}

\centering
\begin{tabular}{lccc}
  & Prediction &  \\
  Actual & a & b & c \\
  a & $T_a$ & $F_{ab}$ & $F_{ac}$ \\
  b & $F_{ba}$ & $T_b$ & $F_{bc}$ \\
  c & $F_{ca}$ & $F_{cb}$ & $T_c$ \\
\end{tabular}
and
\centering
\begin{tabular}{lccc}
  & Prediction &  \\
  Actual & TP & FP & FN \\
  a & $T_a$ & $F_{ba} + F_{ca}$ & $F_{ab} + F_{ac}$ \\
  ... & ... & ... & ...
\end{tabular}



\end{frame}


\begin{frame}{Accuracy}

% We are looking at predictive performance here

\[
\text{Accuracy} = \frac{(\text{TP+TN})}{\text{(TP+FP+FN+TN)}}
\]

or

\[
\text{Accuracy} = \frac{T_a + T_b + T_c}{N}
\]

\pause


\uured{Question}: What is the problem with Accuracy?
% Unbalanced samples

\end{frame}


\begin{frame}{Precision}

Of all the predicted positives, how many are \uured{actually positive}?

% Want to use when want to be sure about a prediction
\[
\text{Precision} = \frac{(\text{TP})}{\text{(TP+FP)}}
\]
or
\[
\text{Precision}_a = \frac{(T_a)}{T_a + F_{ba} + F_{ca})}
\]

All \uured{predicted} $a$: $T_a + F_{ba} + F_{ca}$

If we want one precision estimate for all classes:
\begin{enumerate}
\item Macro-average ($\text{Precision}_a, ..., \text{Precision}_c$)
\item Micro-average (use Table 2)
\end{enumerate}

\end{frame}



\begin{frame}{Recall}

% Want to use when want to be sure about a prediction
Of all \uured{positives}, how many are predicted correctly (recalled)?\pause

\[
\text{Recall} = \frac{(\text{TP})}{\text{(TP+FN)}}
\]
and
\[
\text{Recall}_a =  \frac{(T_a)}{T_a + F_{ab} + F_{ac})}
\]

All \uured{true/actual} $a$: $T_a + F_{ab} + F_{ac}$

If we want one precision estimate for all classes:
\begin{enumerate}
\item Macro-average ($\text{Recall}_a, ..., \text{Recall}_c$)
\item Micro-average (use Table 2)
\end{enumerate}



\end{frame}

\begin{frame}{Sensitivity and specificity}

\[
\text{Sensitivity} = \text{Recall of \uured{positive} class} =  \frac{\text{TP}}{\text{TP+FN}}
\]
and
\[
\text{Specificity} = \text{Recall of \uured{negative} class} =\frac{\text{TN}}{\text{TN+FP}}
\]

\end{frame}


\begin{frame}{F1-score}

% Want to use when want to be sure about a prediction
Harmonic mean of Precision and Recall.

% F1: Both good precision and recall
\[
\text{F}_1 = \frac{2}{\text{Precision}^{-1} + \text{Recall}^{-1}} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{\text{Precision} + \text{Recall}}}
\]

Very common performance measurement in practice.

If we want one precision estimate for all classes:
\begin{enumerate}
\item Macro-average ($\text{F}_{1a}, ..., \text{F}_{1c}$)
\item Micro-average (use Table 2)
\end{enumerate}

\end{frame}



\begin{frame}{Example}

Say that we want to classify spam vs. ham. \\[3mm]\pause
%TODO: Images here

\begin{center}
\begin{tabular}{ l | c | c }
  & $\hat{f}(x)=1$ & $\hat{f}(x)=0$\\
  \hline
  $y=1$ & 515 & 91 \\
  $y=0$ & 85 & 569 \\
  \hline
\end{tabular}
\end{center}
The cell counts yield us estimates of
\begin{enumerate}
\item Accuracy: $\frac{515+569}{515+91+85+569}\approx 0.86$
\item Precision: $\frac{515}{515+85}\approx 0.86$
\item Recall: $\frac{515}{515+91}\approx 0.85$
\item $F_1$: $\frac{2 \cdot 0.85 \cdot 0.86}{0.85 + 0.86}\approx 0.855$
\end{enumerate}

\pause

In this example, we let $\hat{y}_i=1$ whenever $\hat{\pi}_i>0.5$.

What if we choose another cut-off level $\hat{\pi}_i>\alpha$ instead?

\end{frame}


\begin{frame}{Classification tables}

\begin{center}
\begin{tabular}{ l | c | c }
 {\color{uured}$\alpha=0.5$} & $\hat{f}(x)=1$ & $\hat{f}(x)=0$\\
  \hline
  $y=0$ & 515 & 91 \\
  $y=1$ & 85 & 569 \\
  \hline
\end{tabular}
\end{center}
\pause
Now let $\alpha=0.3$ instead, so that we are more prone to say that $\hat{y}=1$:
\begin{center}
\begin{tabular}{ l | c | c }
  {\color{uured}$\alpha=0.3$} & $\hat{f}(x)=1$ & $\hat{f}(x)=0$\\
  \hline
  $y=1$ & 462 & 144 \\
  $y=0$ & 38 & 616 \\
  \hline
\end{tabular}
\end{center}
The cell counts yield us estimates of
\begin{enumerate}
\item Accuracy: $\frac{462+616}{462+38+144+616}\approx 0.86$
\item Precision: $\frac{462}{462+38}\approx 0.92$
\item Recall: $\frac{462}{462+144}\approx 0.76$
\item $F_1$: $\frac{2 \cdot 0.92 \cdot 0.76}{0.92 + 0.76}\approx 0.83$
\end{enumerate}

The Precision has increased, but the Recall has decreased...

\end{frame}



\begin{frame}{A more problematic example}
A highly unbalanced example. 1001 ham and 17 spam.

Our new classifier: Everything is ham!\pause

\begin{center}
\begin{tabular}{ l | c | c }
  & $\hat{f}(x)=1$ & $\hat{f}(x)=0$\\
  \hline
  $y=1$ & 1001 & 0 \\
  $y=0$ & 17 & 0 \\
  \hline
\end{tabular}
\end{center}
\pause
The cell counts yield us estimates of
\begin{enumerate}
\item Accuracy: $\frac{1001}{1001+17}\approx 0.99$
\item Precision: $\frac{1001}{1001+0}\approx 1.0$
\item Recall: $\frac{1001}{1001+17}\approx 0.99$
\item $F_1$: $\frac{2 \cdot 1 \cdot 0.99}{0.99 + 1}\approx 0.99$
\item But Specificity is 0!
\end{enumerate}

\end{frame}




\begin{frame}<handout:0>{Questions?}
Questions?
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Test and training error}
\frame{\sectionpage}

\begin{frame}{Test Error}

\begin{itemize}
\item The main error of interest - \emph{generalization error}
\item Conditional Test Error \\(Model performance for the \uured{actual} training data):
\[
\text{Err}_\mathcal{T} = \E_{Y,X}(L(Y_0,\hat{f}(X_0) | \mathcal{T})
\]
\item Expected Test Error \\(Model performance over \uured{different} training datasets):
\[
\text{Err} = \E_\mathcal{T}(\E_{Y,X}(L(Y_0,\hat{f}(X_0)))
\]
\item Sometimes refered to as \uured{generalization} error.

\end{itemize}

\end{frame}


\begin{frame}{Training Error}

\begin{itemize}
\item The loss function the algorithm try to \uured{minimize}
\item The Error in the training data:
\[
\overline{\text{err}} = \frac{1}{N} \sum_{i=1}^N L(y_i,\hat{f}(x_i))
\]

\end{itemize}

\end{frame}


\begin{frame}{Model complexity/capacity}

\begin{itemize}
\item Model complexity/capacity: \uured{The flexibility of the model.}
\item \uured{Underfitting}: Too \uured{low} capacity of model
\item \uured{Overfitting}: Too \uured{high} capacity of model
\item Example: Polynomial regression with higher order terms
\end{itemize}

\begin{figure}[h]
\caption{Model complexity (Goodfellow et al, 2017, Figure 5.2)}
\centering
\includegraphics[width=0.9\textwidth]{figs/Dl_5_2}
\end{figure}

\end{frame}


\begin{frame}{Training, test, and complexity}


\begin{figure}[h]
\caption{Test, training, and model complexity (Goodfellow et al, 2017, Figure 5.3)}
\centering
\includegraphics[width=0.8\textwidth]{figs/Dl_5_3.png}
\end{figure}

\end{frame}


\section{Estimating the test error}

\begin{frame}{How to estimate the Test Error (Model Assesment)}

\begin{itemize}
\item We set aside a \emph{test set} from the data
\item Use as the last step to \emph{estimate} the test error
\item Should only be used \uured{ONCE}\pause
\item Size of testset:
\begin{itemize}
\item Common suggestion 10\%
\item A statistical estimation problem (choice of samling size)
\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}{Multiple Use of Test Set for Model Assesment}

\begin{itemize}
% TODO: Fix
%\item Say that we have $\hat{L}(\mathcal{T})$ an estimate of the loss on the test set given a training set
%\item Lets say that we have $i \in \{1,...,M\}$ be models trained on $M$ independent training sets $\mathcal{T}_i$ but they all have the same underlying error $L^*$
%\item Then we can assume that
%\[
%\hat{L}_i(\mathcal{T}_i) \sim N(L^*, \sigma)
%\]
\item What happens if we use the test set to pick the model?
\end{itemize}

\end{frame}

\begin{frame}<handout:0>{Questions?}
Questions?
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\section{Model Selection}
%\frame{\sectionpage}


%\begin{frame}{Model Selection}
%\begin{itemize}
%\item We want to select a model based on performance.
%\item Important when using hyperparameters (as in regularization)
%\end{itemize}

%\end{frame}


\section{Bias and Variance}
\frame{\sectionpage}

\begin{frame}{Bias and Variance}

Assume we have the following data generating process:
\[
Y = f(X) + \epsilon\,,
\]
where $\E(\epsilon)=0$ and $V(\epsilon)=\sigma_\epsilon$.

\begin{align*}
\text{Err}(x_0) &= \E\{(Y-\hat{f}(x_0))^2 | X = x_0\} \\
  &= \sigma^2_\epsilon + \{\E(\hat{f}(x_0)) - f(x_0)\}^2 + \E\{\hat{f}(x_0) - \E(\hat{f}(x_0))\}^2 \\
  &= \sigma^2_\epsilon + \text{Bias}^2(\hat{f}(x_0)) + V(\hat{f}(x_0))
\end{align*}

\begin{itemize}
\item \emph{Bias}: How close can $\hat{f}$ get to the true model $f$
\item \emph{Variance}: The variability of the predictions from $\hat{f}$
\item \emph{Irreducible/Bayes error}: The minimum possible error
\end{itemize}

\end{frame}


\begin{frame}{Bias and Variance: Linear regression}

In linear regression we have:
\[
\hat{f}(x_i) = \hat{\beta} x_i
\]

This give us the following error decomposition:

\begin{align*}
\frac{1}{N}\sum^N_i \text{Err}(x_i) &= \sigma^2_\epsilon + \frac{1}{N}\sum^N_i (f(x_i) - E(\hat{f}(x_i))^2 + \frac{p}{N} \sigma^2_\epsilon
\end{align*}

\end{frame}


\begin{frame}{Bias and Variance}

\begin{figure}[h]
\caption{Test, training, and model complexity (Hastie et al, 2009, Figure 7)}
\centering
\includegraphics[width=0.65\textwidth]{figs/ESL_7_1.png}
\end{figure}

\begin{itemize}
\item High Bias: \uured{Underfit}
\item High Variance: \uured{Overfit}
\item High Irreducible error: No model is good
\end{itemize}

\end{frame}


\begin{frame}{Bias and Variance}

\begin{figure}[h]
\caption{Bias and variance (Goodfellow et al., 2017, Figure 5.6)}
\centering
\includegraphics[width=0.9\textwidth]{figs/Dl_5_6.png}
\end{figure}

\end{frame}

%\subsection{Optimism of Training Error}

\begin{frame}{Optimism}

The in-sample \uured{test} error:
\[
\text{Err}_\text{in} = \frac{1}{N}\sum^N_{i=1} \E_{Y^0}\{L(Y^0_i,\hat{f}(x_i))|\mathcal{T}\}\,,
\]
where $Y_{0,i}$ is a \uured{new variable conditioned on $x_i$}.\\[3mm]

\pause

We have that
\[
\E_\mathbf{y} (\text{Err}_\text{in}) = \E_\mathbf{y}(\overline{\text{err}}) + \underbrace{\frac{2}{N}\sum^N_{i=1} \text{Cov}(\hat{f}(x_i),y_i)}_{\text{optimism}}\,,
\]
where $\overline{\text{err}}$ is the training error.

\uured{Question}: How could we create an optimistic classifier for the training data?

\end{frame}

\begin{frame}{Estimating Optimism}

\begin{itemize}
\item Under certain conditions we can estimate this optimism.
\item AIC is an example of this -- asymptotic predictive performance.
\item Find the optimism
\end{itemize}

% Answer the diff between the red and blue line

\end{frame}



\begin{frame}{The double descent of large models}

\begin{figure}[h]
\caption{The double descent of large models (Nakkiran et al., 2019, Figure 1)}
\centering
\includegraphics[width=0.9\textwidth]{figs/Nakkiran_et_al_2019.png}
\end{figure}



\end{frame}


\begin{frame}<handout:0>{Questions?}
Questions?
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Cross-validation}
\frame{\sectionpage}


\begin{frame}{Cross-Valdidation}






\begin{itemize}
\item We want to estimate \uured{$\text{Err}$} for different models and to \uured{choose the best model} where
\begin{align*}
\text{Err} & = \E_\mathcal{T} (\text{Err}_\mathcal{T}) \\
 & = \E_\mathcal{T} (\E(L(Y_0,X_0)|\mathcal{T})) \\
& = \int (\int L(Y_0, \hat{f}(X_0)) P_{y,x} d(Y_0,X_0) |\mathcal{T}) d\mathcal{T}
\end{align*}
\item Cross-Validation is probably the most popular approach to estimate \uured{$\text{Err}$} and choose between models because it is
\begin{enumerate}
\item Conceptually easy to understand
\item Easy to implement
\item No need for rules-of-thumbs to verify that it is applicable
\item Equally useful for many different type of models
\item Flexible for the use case at hand
\end{enumerate}
\pause
\item Common approach to \uured{learn hyper parameters}\\(that is a model choice)
\end{itemize}

\end{frame}


\begin{frame}{Cross-Valdidation Algorithm}

\begin{figure}[h]
\caption{Cross-Validation (Hastie et al, 2009, p. 222, 242)}
\centering
\includegraphics[width=0.6\textwidth]{figs/ESL_test_train_val.png}
\includegraphics[width=0.6\textwidth]{figs/ESL_cross_val.png}
\end{figure}

\begin{enumerate}
\item Split data in $K$ folds
\item For each fold $k=1,2,...,K$
\begin{enumerate}
\item Use all samples except those in $k$ to build $\hat{f}(x)$
\item Use the model and predict the observations in fold $k$
\end{enumerate}
\end{enumerate}

\[
\text{Err}_{CV}(\hat{f}) = \frac{1}{N}\sum^N_{i=1} L(y_i,\hat{f}_{-\kappa(i)}(x))
\]

\end{frame}

\begin{frame}{The Bias of Cross-Valdidaton}

\begin{itemize}
\item Cross-validation estimation of \uured{$\text{Err}$} will be biased
\item The training data size is smaller than the full data
\end{itemize}

\begin{figure}[h]
\caption{Cross-Validation Bias (Hastie et al, 2009, Fig. 7.8)}
\centering
\includegraphics[width=0.7\textwidth]{figs/ESL_7_8.png}
\end{figure}

\end{frame}


\begin{frame}{K-fold Cross Validation}

\begin{itemize}
\item Common $K$ are: $K=\{2,5,10\}$
\item Smaller $K$ gives larger bias
\item Larger $K$ is computationally more costly
\item $K=10$ is a common approach
\end{itemize}

\end{frame}


\begin{frame}{Leave-One-Out Cross Validation}

\begin{itemize}
\item When $K=N$
\item Benefits
\begin{itemize}
\item Almost unbiased estimate of $\text{Err}$
\item Sometimes we only need to train our model once
\end{itemize}
\item Drawbacks
\begin{itemize}
\item Higher Variance in estimate of $\text{Err}$
\item Can be more computationally very costly (naive implementation)
\item Can be unstable/less robust in some settings
\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}{Leave-One-Out Cross Validation}

\begin{figure}[h]
\caption{Cross-Validation Bias (Hastie et al, 2009, Fig. 7.14)}
\centering
\includegraphics[width=0.7\textwidth]{figs/ESL_7_14.png}
\end{figure}

\end{frame}


% TODO: Add?

%\begin{frame}{LOO-CV for large data}

%\[
%CV(\hat{f},\alpha) = \frac{1}{N}\sum^N_{i=1} L(y_i,\hat{y}_{-\kappa(i)}(x_i,\alpha)
%\]

%Lets use sampling theory!\\[3mm]\pause

%Difference estimator

%Good approximations of the optimism.

%\end{frame}

\begin{frame}{The role of the data generating process}

\begin{itemize}
\item we assume that testset and train set are different observations from the same data generating process
\[
\mathbf{d} = \{(y_i, \mathbf{x}_i), i = 1, ..., n\} \,\sim P_{y,x}
\]
\item The (naive) assumption: independence
\item Things that can go wrong:
\begin{itemize}
\item temporal leak/concept drift
\item duplicated observations
\item non-randomized data
\end{itemize}
\pause
%\item Example: Evaluating prediction models for Elections
% TODO: Add examples from the bachelor theses
\end{itemize}


\end{frame}


\begin{frame}{Example: Election prediction}

\begin{itemize}
\item We want to predict the next election
\item We know that there are "\uured{concept drift}"
\item Solution in Frölander and Uddhammar (2021) and Olsson and Ölfvingsson (2021)
\begin{enumerate}
\item LOO-CV on the elections 1973-2014
\item The elections 2018 as the final validation set
\end{enumerate}


\end{itemize}

\begin{figure}[h]
\caption{Predictive distr. (Olsson and Ölfvingsson, 2021, Fig. 6)}
\centering
\includegraphics[width=1\textwidth]{figs/elections}
\end{figure}

\end{frame}

\begin{frame}<handout:0>{Questions?}
Questions?
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Regularisation}
\frame{\sectionpage}

\begin{frame}{Regression and GLM}
\begin{itemize}
\item Linear regression and logistic regression are examples of {\color{uured}generalised linear models}, GLMs.\\[3mm]\pause
\item Both use maximum likelihood estimation for fitting the model, where the likelihood function $L(\beta)$ is maximised.
\end{itemize}
\end{frame}

\begin{frame}{Regularised regression models}
\begin{itemize}
\item In some situations, for instance when the predictors are highly collinear, when there are too many predictors or when there is complete separation in the data, maximum likelihood estimation is unstable.\pause
\begin{itemize}
\item Either the solution is not unique, or minuscule changes in the data can change the solution completely.\pause
\item Such datasets are increasingly common in e.g. genomics, finance, astronomy and image analysis.\\[3mm]\pause
\end{itemize}

\item In such cases, {\color{uured}regularisation/shrinkage methods} can be used instead.\\[3mm]\pause
\item In a regularized GLM, it is not the likelihood $L(\beta)$ that is maximized, but a {\color{uured}regularised} function $L(\beta) \cdot p(\beta)$, where $p$ is a penalty function that typically forces the resulting estimates to be closer to 0, which leads to a stable solution.
\end{itemize}
\end{frame}

\begin{frame}{Regularised regression models}
Regularised linear regression models increase the {\color{uured}bias} of the estimates, but lowers their {\color{uured}variance}, thereby potentially decreasing the MSE.
\end{frame}

\begin{frame}{Connection to Bayesian estimation}
In Bayesian estimation, a {\color{uured}prior distribution} $p(\beta)$ for the parameters $\beta_i$ is chosen.\\[3mm]\pause
The estimates are then computed from the conditional distribution of the $\beta_i$ given the data, called the {\color{uured}posterior distribution}.\\[3mm]\pause
Using Bayes' theorem, we find that
$$P(\beta|\mathbf{x})\propto L(\beta)\cdot p(\beta),$$
i.e. that the posterior distribution is proportional to the likelihood times the prior.\\[3mm]
A special type of Bayesian estimator is the {\color{uured}maximum a posteriori (MAP)} estimator, which is found by maximizing the above expression (i.e. finding the mode of the posterior).\\[3mm]
This is equivalent to the estimates from a regularised frequentist model with penalty function $p(\beta)$!
\end{frame}

\begin{frame}{Inference and invariance}
\begin{itemize}
\item Regularised regression models are not invariant under linear rescaling of the predictors.\pause
\begin{itemize}
\item If a predictor is multiplied by a scalar $a\neq 0$, this can change the entire model.\pause
\item A model with measurements in inches might yield completely different results from a model with measurements in cm.\\[3mm]\pause
\end{itemize}
\item For this reason, it is widely agreed that the predictors should be standardized to have mean 0 and variance 1 before a regularised model is fitted.\pause
\begin{itemize}
\item With this approach we choose a particular (natural?) scaling, among all possible scalings.\pause
\item All predictors are on the same scale and are therefore treated equally by the penalty function.\pause
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Inference and invariance}
\begin{itemize}
\item Hypothesis tests are available (e.g. Lockhart et al. (2014), A significance test for the lasso, \emph{Annals of Statistics}), but I advise against using them.\\[3mm]\pause
\item Note that the hypothesis tests will be conditioned on the choice of scaling.\pause
\begin{itemize}
\item Because of this, regularised models are not appropriate for hypothesis testing -- the p-values could change completely if we rescaled the data!\\[3mm]\pause
\end{itemize}
\item Regularised regression models are however very useful for {\color{uured}predictive modelling}.
\end{itemize}
\end{frame}

\begin{frame}{$L_q$-penalties}
The most popular penalty terms correspond to common {\color{uured}$L_q$-norms}. \pause On a log-scale, the function to be maximized is
$$\ell(\beta)+\lambda\sum_{i=1}^p|\beta_i|^q,$$
where $\ell(\beta)$ is the loglikelihood of $\beta$ and $\sum_{i=1}^p|\beta_i|^q$ is the $L_q$-norm, with $q\geq 0$.\\[3mm]\pause
This is equivalent to maximizing $\ell(\beta)$ under the constraint that $\sum_{i=1}^p|\beta_i|^q\leq\frac{1}{h(\lambda)}$, for some increasing positive function $h$.\pause
\begin{itemize}
\item Relies on the {\color{uured}sparsity} assumption that most $\beta$ are 0.\\[3mm]\pause
\end{itemize}
$\lambda\geq 0$ is a {\color{uured}smoothing parameter}:
\begin{itemize}
\item When $\lambda=0$, we are back at the standard ML-estimate.
\item The $\hat{\beta}$ are forced to be closer to 0 when $\lambda$ increases.
\item $\lambda$ is usually chosen using cross-validation.
\end{itemize}
\end{frame}


\begin{frame}{Ridge regression}
When the $L_2$ penalty is used, the regularised model is called {\color{uured}ridge regression}, for which we maximize
$$\ell(\beta)+\lambda\sum_{i=1}^p\beta_i^2.$$
\begin{itemize}
\item Invented and reinvented by several authors, from the 1940's onwards.\\[3mm]\pause
\item In a linear model, the OLS estimate is $\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$, whereas the ridge estimate is $\hat{\beta}=(\mathbf{X}^T\mathbf{X}+\lambda \mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}$. The $\lambda \mathbf{I}$ is the 'ridge'.\\[3mm]\pause
\item The $\beta_i$ can become very small, but are never pushed all the way down to 0.\\[3mm]\pause
\item In a Bayesian context, this corresponds to putting a standard normal prior on the $\beta_i$.
\end{itemize}
\end{frame}

\begin{frame}{Lasso}
When the $L_1$ penalty is used, the regularised model is called the {\color{uured}lasso} (Least Absolute Shrinkage and Selection Operator), for which we maximize
$$\ell(\beta)+\lambda\sum_{i=1}^p|\beta_i|.$$
\begin{itemize}
\item Introduced by Robert Tibshirani in 1996.\\[3mm]\pause
\item As $\lambda$ increases, more and more $\beta_i$ become 0.\pause
\begin{itemize}
\item Simultaneously performs estimation and variable selection!\\[3mm]\pause
\end{itemize}
\item In a Bayesian context, this corresponds to putting a standard Laplace prior on the $\beta_i$.
\end{itemize}
\end{frame}

\begin{frame}{Examples in R}
Functions for regularised generalized linear models (linear, logistic, Poisson, multinomial, and more) are available e.g. in the \texttt{glmnet} package for R.\\[3mm]\pause
The syntax used is somewhat different from that for \texttt{glm} and \texttt{lm}.\\[3mm]
\end{frame}


\begin{frame}{Generalizations}
Regularised models have been a hot research topics in the last 20 years. Some additional important models are:\pause
\begin{itemize}
\item {\color{uured}Elastic net:} a compromise between ridge and lasso, in which $$\ell(\beta)+\lambda_1\sum_{i=1}^p|\beta_i|+\lambda_2\sum_{i=1}^p\beta_i^2$$ is maximized.\pause
\begin{itemize}
\item Introduced by Zou and Hastie in 2005.\pause
\item Is better than the lasso at handling correlated predictors.\pause
\item Has two smoothing parameters that we need to choose.\pause
\item Available in the \texttt{glmnet} package.\\[3mm]\pause
\end{itemize}
\item {\color{uured}Group lasso:} a version of the lasso in which variables can be grouped before fitting the model. The group lasso then selects groups of variables rather than individual variables.\pause
\begin{itemize}
\item Introduced by Yuan and Lin in 2006.\pause
\item Useful e.g. when we have dummies for categorical variables (in contrast, the lasso may choose to only include the dummies for some of the categories).
\end{itemize}
\end{itemize}
\end{frame}






\end{document}
