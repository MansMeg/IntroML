%\documentclass[10pt,handout]{beamer}
\documentclass[10pt]{beamer}
\usepackage{babel} % Anpassa efter svenska. Ger svensk logga.
\usepackage[utf8]{inputenc} % Anpassa efter linux
\usepackage{graphicx}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\usepackage{../common/beamerthemeUppsala}
%\usetheme{Uppsala}
%\usecolortheme{UU} % Anpassa efter UU:s frger och logga
%\hypersetup{pdfpagemode=FullScreen} % Adobe Reader ska ppna fullskrm
\setbeamertemplate{itemize items}[circle]

% \usepackage{beamerthemesplit}
\usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{epsfig}
% \usepackage[latin1]{inputenc}
 \usepackage{color}
% \usepackage{fancybox}
% \usepackage{psfrag}
% \usepackage[english]{babel}
 \setbeamertemplate{footline}{\hfill\insertframenumber/\inserttotalframenumber}

% Read in commands
\input{../common/commands.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\parskip}{3mm}
\title[]{{\color{black}Machine learning -- Block 1(a)}}
\author[]{M{\aa}ns Magnusson\\Department of Statistics, Uppsala University}
\date{\currentsemester}


\begin{document}

\frame{\titlepage
% \thispagestyle{empty}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{This block}
\begin{itemize}
\item What is AI and Machine Learning?
\item Course Information and Practicalities
\item Introduction to Supervised Learning
\item (Stochastic) Gradient Descent
\item Regularization
\end{itemize}
\end{frame}

\section{What is AI and ML?}
\frame{\sectionpage}

\begin{frame}{What exactly is machine learning and artificial intelligence?}
The word "AI" is often used quite loosely:
   \includegraphics[width=0.9\textwidth]{figs/AI-example.jpg}
\end{frame}


\begin{frame}{What is Artificial Intelligence?}
Artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals. -- Wikipedia
\pause
Artificial intelligence (AI), the ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings. -- Encyclopedia Brittanica
\end{frame}


\begin{frame}{What is Artificial General Intelligence?}

Artificial general intelligence (AGI) is the hypothetical intelligence of a machine that has the capacity to understand or learn any intellectual task that a human being can. -- Wikipedia

Also called:
\begin{enumerate}
\item Strong AI
\item General AI
\item Full AI
\end{enumerate}

\pause
Artificial super intelligence (ASI) is "any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest" -- Nick Bostrom

\end{frame}



\begin{frame}{What is Machine Learning?}

Machine Learning is the field of study that gives the computer the ability to learn without being explicitly programmed. -- Arthur Samuel (1959)

A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. -- Tom Mitchell (1998)\pause

Learning from data. -- Hastie, Tibshirani, Friedman (2009)
\end{frame}


\begin{frame}{What is Machine Learning?}

\begin{figure}[h]
\caption{ML, AI and DL (Chollet, 2018, Figure 1.1)}
\centering
\includegraphics[width=0.8\textwidth]{figs/fig1_1_chollet.png}
\end{figure}

\end{frame}


\begin{frame}{Computer Science and Machine Learning}

\begin{figure}[h]
\caption{A new paradigm? (Chollet, 2018, Figure 1.2)}
\centering
\includegraphics[width=0.8\textwidth]{figs/fig1_2_chollet.png}
\end{figure}

\end{frame}


\begin{frame}{Statistics and Machine Learning}

\begin{figure}[h]
\caption{Regression vs. Pure Predictions (Efron, 2020, Table 5)}
\centering
\includegraphics[width=0.8\textwidth]{figs/table5.png}
\end{figure}

\end{frame}

\begin{frame}{Different names for the same things}

\begin{itemize}
\item Machine learning has developed in parallel with Statistics
\item Common with \uured{different names for the same thing}:
\begin{enumerate}
\item Time series classification (ML) vs. Functional data classification (Stats)\pause
\item Time series regression (ML) vs. Scalar-on-function
regression (Stats)\pause
\item Learning (ML) vs. Estimation (Stats)\pause
\item Weights (ML) vs. Parameters (Stats)\pause
\item Features (ML) vs. Covariates (Stats)
\end{enumerate}

\end{itemize}

\end{frame}


\begin{frame}{Different flavors of ML}

\begin{itemize}
\item Supervised learning
\item Unsupervised learning
\begin{itemize}
\item Self-(un)supervised learning
\end{itemize}
\item Reinforcement learning
\end{itemize}

\end{frame}


\begin{frame}<handout:0>{Questions?}
Questions?
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Course information}
\frame{\sectionpage}

\begin{frame}{Course information}
The aims of this course are that you should:\\[3mm]\pause
\begin{enumerate}
\item get a good knowledge of a large number of machine learning models,
\item become able to use methods for evaluating and improving predictive models,
\item become able to handle big data,
\item become able to train and use machine learning models in R,
\item become able to train and use neural networks using Keras/TensorFlow.
\item become able to describe and discuss ethical aspects of big data and black box-models,
\end{enumerate}

\end{frame}


\begin{frame}{Course Outline}
Two main parts:
\begin{itemize}
\item Core Content (8 lecture blocks):
\begin{itemize}
\item Supervised learning (5 blocks)
\begin{itemize}
\item Introduction, statistical learning (1 block)
\item Tree-based methods (1 block)
\item Neural Networks (3 block)
\end{itemize}
\item Unsupervised learning (2 blocks)
\item Reinforcement learning (1 block)\pause
\end{itemize}
\item Assignments (8 individual assignments)\pause
\item Mini-project on a supervised project (2-3 students)\pause
\end{itemize}
Exact dates and details; see the course page.
\end{frame}

\begin{frame}{Core Content}

\begin{itemize}
\item Each block consist of:
\begin{itemize}
\item Online video material (optional)
\item Reading assignments (approx. 2-4h, 50-90 pages a week)
\item One-two Lecture(s) (optional)
\item An individual computer assignment (approx. 14-16h).
\item Three Zoom computer lab sessions (optional)\pause
\end{itemize}
\item Reading: Mandatory and optional (overlap)\pause
\item Recommended workflow for each block
\begin{itemize}
\item Do the reading assignments
\item Watch the videos (optional)
\item Attend the lecture (optional) \uured{to ask questions}.
\item Do the computer assignment
\item Attend the zoom lab session (optional) \uured{to ask questions}.
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Lectures}

\begin{itemize}
\item Present overall theory, concepts and content (overview) \pause
\item Ask questions during the lecture!
\item Guest lectures on the course (worthwhile):
\begin{enumerate}
\item Jonas Wallin, Lund University (regularization)\pause
\item Erik Fredlund, CEO Codon AI (industry applications)\pause
\item Holli Sargeant, Cambridge University (fairness and law)\pause
\item Karim Jebari, The Institute for Futures Studies (AI and ethics)\pause
\item Väinö Yrjänäinen, UU (word embeddings) \pause
\end{enumerate}
\item No lectures after 19th/20th of December
\end{itemize}
\end{frame}


\begin{frame}{Examination}

\begin{itemize}
\item To pass (G): All labs, mini-project, and project review need to be passed (75\%)\pause
\item To pass with distinction (VG): 6/10 VG points\pause
\item Each assignment has an extra (VG) task worth 1 VG point.\pause
\item The mini-project is worth 2 VG-points (if it is passed with distinction).
\item Ph.D. students: I suggest you get VG to pass the course. Make the project a potential paper.
\item Reassesment of grades (supply form to course admin)
\item Failing the course: You will need to redo all assignments and mini-project.
\end{itemize}

\end{frame}


\begin{frame}{Computer Assignments}

\begin{itemize}
\item Main part of the course\\Learning by doing
\item Machine learning = Statistics + Computer Science\\Hence a lot of programming\pause
\item Both implementation of core components and state-of-the-art methods\pause
\item \emph{Warning!} There might be bugs in the assignments! \uured{Don't hesitate to ask questions!}\pause
\item Deadline \uured{Sundays 23.59}.\pause
\item All assignments can be turned in a three times. 2nd deadline last day of course. 3rd deadline approx 2-4 weeks after the course. \uured{If failed, resubmit right away!}\pause
\item We will mark and return each assignment within 10 working days.
\end{itemize}
\end{frame}

\begin{frame}{Computer Assignments}

\begin{itemize}
\item Don't write your name anywhere!\pause
\item Do the assignment evaluation\pause
\item \emph{Important!} Don't \uured{show your assignment} to any other student. But feel free to discuss!\pause
\item Zoom sessions:
\begin{enumerate}
  \item First lab each week will include a 15 min introduction
  \item Our focus: Help during computer labs - less focus on written feedback
  \item \uured{Ask questions!} This is \uured{your} time.\pause
\end{enumerate}
\end{itemize}
\end{frame}


\begin{frame}{Computer Assignments}

\begin{figure}[h]
\caption{Workload last year to pass (G)}
\centering
\includegraphics[width=0.9\textwidth]{figs/G.png}
\end{figure}

\end{frame}

\begin{frame}{Computer Assignments}

\begin{figure}[h]
\caption{Workload last year to pass with distinction (VG)}
\centering
\includegraphics[width=0.9\textwidth]{figs/VG.png}
\end{figure}

\end{frame}


\begin{frame}{Mini-project}

\begin{itemize}
\item See project instructions on webpage for details.\pause
\item \uured{Supervised problem} of choice on real data.
\item 2-3 students.\pause
\item Supply step 1 proposal at the end of block 3.\pause
\item Supply step 2 proposal at the end of block 6.\pause
\item Project will last two weeks (half time) - but start earlier. Good case to show potential employers.
\item Recommended data: Images, text or tabular data (e.g. avoid time series).
\item Feel free to build upon the Bayesian project (e.g. compare with Bayesian methods).
\item Approximate 40 hours of work \emph{per student}.\pause
\item The project should result in a 4 page report (PDF) using the ICML LaTeX template (see course page).
\item Project oral presentation (10-15 minutes)\pause
\item The first author is corresponding author\pause
\item Mini-project and master thesis:
\begin{itemize}
\item The mini-project can be used to explore thesis project
\item Master thesis proposals in November
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Practicalities}

\begin{itemize}
\item Course page: Github -- please do a PR if something is wrong!\pause
\item Acknowledgements: M{\aa}ns Thulin, Josef Wilzén, Anders Eklund\pause
\item Schedule: Time Edit/Studium
\item Assignments: Studium\pause
\item Literature
\begin{itemize}
\item Hastie, Tibshirani \& Friedman (2009). \emph{Elements of Statistical Learning}.
\item Chollet \& Allaire (2018) \emph{Deep Learning with R}.
\item Goodfellow, Bengio \& Courville (2017) \emph{Deep Learning}.
\item Sutton and Barto (2020) \emph{Reinforcement learning: An introduction}
\item Additional articles, tuturials, videos etc. posted on course (github) homepage
\item Mandatory and optional material: Overlap exists!
\end{itemize}
\pause
\item If the course is too easy - \uured{reach out to me}!
\end{itemize}

\end{frame}



\begin{frame}{Course improvements since last year}

\begin{itemize}
\item New course book (I hope it works!)
\item New lecture (maybe) on diffusion models
\end{itemize}

\end{frame}

\begin{frame}<handout:0>{Questions?}
Questions?
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction to Supervised Learning}
\frame{\sectionpage}

\begin{frame}{Supervised learning}

\begin{figure}[h]
\caption{Relationship between appartment size and price (\href{https://www.data-to-viz.com/story/TwoNum.html}{source})}
\centering
\includegraphics[width=0.8\textwidth]{figs/scatter_apartment.png}
\end{figure}

\emph{Problem}: We want to predict the price of a new apartment.

\end{frame}



\begin{frame}{Supervised learning}

\begin{itemize}
\item General problem: We have \emph{training} data
\[
\mathbf{d} = \{(y_i, \mathbf{x}_i), i = 1, ..., n\} \,.
\]
\item $\mathbf{x}_i = $ features/input/predictors/features/independent variables
\item $y_i = $ labels/output/dependent variable
\item We want to \emph{learn} a function $\hat{y} = f(x_{new})$ with as good performance as possible.\pause
\item Regression problems: $y_i \in \mathbb{R}$
\item Classification problems: $y_i \in {a,b,c,...}$ where $a,b,c ...$ are discrete classes.
\end{itemize}

\end{frame}



\begin{frame}{Example of supervised problems}

%[Take images - Ask are the classification problems or nor what is the features]

\uured{Any examples of applications?}

\pause

\begin{itemize}
\item Is this e-mail message spam (1) or not (0)?\pause
%\item Sequence to sequence (NLP) - machine translation?
\item Image recognition/classification\pause
\item Image object traction (position in a video)\pause
\item Will this patient recover from their illness or not?\pause
\item Does this fingerprint belong to an employee or not?\pause
\item Does this customer have stable finances or not?\pause
\item Face recognition\pause
\item Is this tumour malign (1) or not (0)?\pause
\end{itemize}

\end{frame}

\subsection{Example: Logistic regression}

\begin{frame}{Logistic regression and classification}
When the $y_i$ in a regression problem is binary (or more generally, categorical), it becomes a {\color{uured}classification problem}.\\[3mm]\pause
The question that the model tries to answer is: does this observation belong to class 0 or class 1?\\[3mm]\pause

Logistic regression is a workhorse for classification problems.

\end{frame}




\begin{frame}{Logistic regression}
When analysing binary data $y_1,\ldots,y_N$, we usually assume that the $Y_i$ follow binomial (or Bernoulli) distributions.\\[3mm ]\pause
Assume that $Y_1,\ldots,Y_N$ are independent with $Y_i \sim Bernoulli(\pi_i)$.\\[3mm ]\pause
$Y_i \in {0,1}$ with success probability $\pi_i$ and $\mu_i=E(Y_i)=\pi_i$.\\[3mm ]\pause
\begin{itemize}
\item The natural parameter of the binomial distribution is $$g(\pi_i)=\log\Big(\frac{\pi_i}{1-\pi_i}\Big),$$
called the {\color{uured}logit} or {\color{uured}log odds}.\\[3mm ]\pause
\item A GLM using this link function is called {\color{uured}logistic regression}, but other link functions are also often used in practice.\\
Many times we use likelihood functions\\
[3mm]
\end{itemize}
\end{frame}


\begin{frame}{Logistic regression}
There are two equivalent formulas for {\color{uured}logistic regression}:
$$\log\Big(\frac{\pi_i}{1-\pi_i}\Big)=\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2}+\cdots+\beta_p x_{ip},\qquad i=1,\ldots,N$$
and
$$\pi_i=\frac{\exp\Big(\beta_0+\sum_{j=1}^p\beta_jx_{ij}\Big)}{1+\exp\Big(\beta_0+\sum_{j=1}^p\beta_jx_{ij}\Big)}.$$
\end{frame}


\begin{frame}{Logistic regression: Prediction}
\begin{itemize}
\item We \emph{train} a logistic regression model using MLE using the training data.
\item Our estimation/traing output the MLE $\hat{\theta}$
\item We the compute $\hat{p}_i = g^{-1}(\hat{\theta} x_{new})$ a for a new observation
\item We use a {\color{uured} decision rule} to predict value 0 or 1:
\[
    \hat{y}_i(\hat{p}_i)=
\begin{cases}
    1,& \text{if } \hat{p}_i \geq 0.5\\
    0,              & \text{otherwise}
\end{cases}
\]
\end{itemize}
\end{frame}



\begin{frame}{Logistic regression: Example}


\begin{figure}[h]
\caption{Decision boundry with two covariates (Hastie et al, 2009, Figure 2.1) }
\centering
\includegraphics[width=0.7\textwidth]{figs/decision_fig_2_1.png}
\end{figure}

\end{frame}




\begin{frame}{An example: Spam and Ham}

\begin{columns}
	\begin{column}{0.47\textwidth}
		\textbf{E-mail Spam}\\
An e-mail provider what to help classify e-mails as spam (1) or ham (0). They have many previous e-mails that customers have already classified as spam, and e-mails people have responded (ham). They want to predict if a new, unseen e-mail is spam or ham.
	\end{column}
	\begin{column}{0.5\textwidth}
		\includegraphics[width=\textwidth]{figs/spam.jpg}
%		(CC BY-SA 3.0)
	\end{column}
\end{columns}
\end{frame}


\begin{frame}<handout:0>{Questions?}

Questions?

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Optimization in Machine Learning}
\frame{\sectionpage}

\begin{frame}{Training of ML algorithms}

\begin{enumerate}
\item Training is usually done by minimizing the objective/loss/cost function $L(\theta)$ for $\theta \in \mathbf{R}^P$.
\item Example: Logistic regression, here we can use the {\color{uured} negative} log-likelihood as loss function:
\[
L(\theta, \mathbf{y}, \mathbf{X}) = - \log \prod^N_{i=1} p_i^{y_i} (1 - p_i)^{1-y_i} \,,
\]
where
\[
\log \frac{p_i}{1-p_i} = \mathbf{x}_i \theta  \,,
\]\pause
\item In Machine Learning: $P$ and $N$ might be very large...
\end{enumerate}


\end{frame}



\begin{frame}{Gradient Decent}

\begin{enumerate}
\item The workhorse of Machine Learning
\[
\theta_t = \theta_{t-1} - \eta \nabla L(\theta_{t-1}, \mathbf{X}, \mathbf{y})\,,
\]
% the minus sign we go in the direction of steepest gradient - going down
where
\[
\nabla f(p)=\begin{bmatrix}{\frac {\partial f}{\partial x_{1}}}(p)\\\vdots \\{\frac {\partial f}{\partial x_{n}}}(p)\end{bmatrix}
\]
\item $L(\theta)$ needs to be differentiable
\end{enumerate}

\end{frame}


\begin{frame}{Gradient Descent Analogy}

\begin{figure}[h]
\caption{Gradient Descent Analogy (\href{https://en.wikipedia.org/wiki/Gradient_descent}{source})}
\centering
\includegraphics[width=0.8\textwidth]{figs/Okanogan-Wenatchee_National_Forest_morning_fog_shrouds_trees.jpg}
\end{figure}


\end{frame}



\begin{frame}{Gradient Descent (cont.)}

\begin{figure}[h]
\caption{Gradient Descent (\href{https://en.wikipedia.org/wiki/Gradient_descent}{source})}
\centering
\includegraphics[width=0.7\textwidth]{figs/GD.png}
\end{figure}

\end{frame}




\begin{frame}{Why Gradient Descent?}


\begin{itemize}
\item Gradient Descent is a poor algorithm \\ (Newtons method, Iteratively Reweighted Least Squares are 'better')
\item So why is gradient descent relevant?\pause
\item The two benefits with Gradient Descent:
\begin{enumerate}
\item Only uses the gradient---scales to large $P$
\item Can scale to large data with Stochastic Gradient Descent---scales to large $N$
\end{enumerate}
\end{itemize}

\end{frame}


\begin{frame}{Stochastic Gradient Descent}


\begin{itemize}
\item Many loss functions (and gradients) are a sum over $N$ observations (e.g. log-likelihoods).
\item We can estimate $\nabla L(\theta, X_{i}, y_{i})$ by choosing a random observation (with index $i$)
\[
E(\nabla L(\theta, X_{i}, y_{i})) = \frac{1}{Z} \nabla L(\theta, \mathbf{X}, \mathbf{y})\,,
\]
for some constant $Z$.
\item Goal -- we want to estimate a total.
\item This give us the following algorithm:
\[
\theta_t = \theta_{t-1} - \eta_t \hat{\nabla} L(\theta_{t-1}, X_{i}, y_{i})\,,
\]
where $i$ is random sampled index.
\item \emph{Note!} \\We need to have an unbiased estimator for $\nabla L(\theta, \mathbf{X}, \mathbf{y})$
\item \uured{What is an iteration?}\pause
\item Epochs vs. Iterations
\end{itemize}


\end{frame}


\begin{frame}{Stochastic Gradient Descent}


\begin{itemize}
\item Learning rate $\eta_t$ is important
\item \uured{Will it converge to an optimum?}\pause
\item We need to reduce $\eta_t$ over time
\item Robbins–Monro (1951) conditions:
\begin{enumerate}
\item $\eta_t \geq 0$ $\forall t \geq 0$
\item $\sum^\infty_t \eta_t = \infty$
\item $\sum^\infty_t \eta_t^2 < \infty$
\end{enumerate}
\end{itemize}

\end{frame}


\begin{frame}{Mini-batch gradient descent}

\begin{itemize}
\item Can we estimate the gradient better?\pause
\item We take a mini-batch of size $B$:
\[
\theta_t = \theta_{t-1} - \eta_t \nabla L(\theta, \mathbf{X}_{\mathcal(S)_i}, y_{\mathcal(S)_i})\,,
\]
where $\mathcal(S)_i$ is a set of random sample (without replacement) indices and $|\mathcal(S)_i| = B$.
\item $B$ is usually set to optimize hardware
\end{itemize}

\end{frame}


\begin{frame}{SGD with momentum}

\begin{itemize}
\item SGD can be slow to converge due to 'jumping' behaviour
\item Can improve behaviour using momentum -- the rolling mean of gradients
\item Additional hyperparemeter $\alpha$ to control the momentum
\[
m_t = \alpha m_{t-1} + \eta_t \hat{\nabla} L(\theta_{t-1}, X_{i}, y_{i})\,,
\]
\[
\theta_t = \theta_{t-1} - m_t,
\]

\end{itemize}

\begin{figure}[h]
\caption{SGD with momentum}
\centering
\includegraphics[width=0.9\textwidth]{figs/sgdm}
\end{figure}

\end{frame}


\begin{frame}{SGD with momentum, Intuition}

\begin{figure}[h]
\caption{SGD with momentum, Intuition (CC)}
\centering
\includegraphics[width=0.9\textwidth]{figs/zorb}
\end{figure}

\end{frame}

\begin{frame}{SGD with momentum}

Example of SGD with momentum \href{https://distill.pub/2017/momentum/}{here}.

\end{frame}


\begin{frame}{Adaptive Moment Estimation (Adam)}

\begin{itemize}
\item Adapt $\eta_t$ to individual parameters
\begin{align*}
m_t =& \beta_1 m_{t-1} + (1-\beta_1) \hat{\nabla} L(\theta_{t-1}, X_{i}, y_{i}) \\
v_t =& \beta_1 v_{t-1} + (1-\beta_2) \hat{\nabla} L(\theta_{t-1}, X_{i}, y_{i})^2
\end{align*}
\item Bias correction
\begin{align*}
\hat{m}_t =& \frac {m_{t}}{1-\beta_1}\\
\hat{v}_t =& \frac {v_{t}}{1-\beta_2}
\end{align*}
\item Update
\[
\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t,
\]
\item Common values: $\beta_1=0.9$, $\beta_2=0.999$, and $\epsilon=10^{-8}$
\item RMSprop is another (similar) alternative
\end{itemize}

\end{frame}

\begin{frame}{Adam}

\begin{figure}[h]
\caption{The Adam Optimizer (Kingma and Ba, 2014)}
\centering
\includegraphics[width=0.6\textwidth]{figs/ADAM}
\end{figure}

For convergence proofs, see:\\
Defossez et al (2020) "A Simple Convergence Proof of Adam and Adagrad"

\end{frame}






\end{document}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
