%\documentclass[10pt,handout]{beamer}
\documentclass[10pt]{beamer}
\usepackage{babel} % Anpassa efter svenska. Ger svensk logga.
\usepackage[utf8]{inputenc} % Anpassa efter linux
\usepackage{graphicx}
\usepackage{../common/beamerthemeUppsala}
%\usecolortheme{UU} % Anpassa efter UU:s frger och logga
%\hypersetup{pdfpagemode=FullScreen} % Adobe Reader ska ppna fullskrm
\setbeamertemplate{itemize items}[circle]

% \usepackage{beamerthemesplit}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{epsfig}
% \usepackage[latin1]{inputenc}
 \usepackage{color}
% \usepackage{fancybox}
% \usepackage{psfrag}
% \usepackage[english]{babel}
 \setbeamertemplate{footline}{\hfill\insertframenumber/\inserttotalframenumber}


% Read in commands
\input{../common/commands.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\parskip}{3mm}
\title[]{{\color{black}Machine learning -- Block 5}}
\author[]{M{\aa}ns Magnusson\\Department of Statistics, Uppsala University}
\date{\currentsemester}


\begin{document}

\frame{\titlepage
% \thispagestyle{empty}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{This week's lectures}
\begin{itemize}
\item Word embeddings
\item Recurrent Neural Networks
\item Transformers
\item Encoder-based (BERT) models
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Practicalities}

\begin{frame}{Practicalities}

\begin{itemize}
\item One lecture later this week on word embeddings (Väinö Yrjänäinen)
\item There is finally a book chapter!
\end{itemize}

\end{frame}


%\begin{frame}{Assignment 4: Evaluation}
%
%\begin{itemize}
%\item Validation or test set when evaluating the models not clear
%\item VGG could be expanded more to work more with fine-tuning
%\end{itemize}
%
%\end{frame}


\section{Word embeddings}
\frame{\sectionpage}

\begin{frame}{How do we represent words?}
\begin{itemize}
\item {\color{uured} One-hot} encoding
\begin{itemize}
\item A vector of length $V$ (vocabulary size)
\[
\text{Uppsala} = [0,...,1,...,0] = \mathbf{1}_i
\]
\end{itemize}
\pause
\item {\color{uured} Word embeddings}
\begin{itemize}
\item A vector of length $D$ (embedding dimension)
\[
\text{Uppsala} = [-0.1231,...,1.9001,...,0.012]
\]
\end{itemize}
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{fig/DLR_6_1_WE.png}
\caption{Representing words as word emnbeddings (Chollet and Allair, 2018, Fig. 6.1)}
\end{figure}

\end{frame}


\begin{frame}{Word embeddings vs. One-Hot}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{fig/DLR_6_2_WE_VS_OH.png}
\caption{One-Hot vs. Word embeddings (Chollet and Allair, 2018, Fig. 6.2)}
\end{figure}

\end{frame}

\begin{frame}{Word embeddings}

\begin{quote}%<⟨action specification⟩>
  The quick brown fox jumps over the lazy dog.
\end{quote}

\begin{itemize}
\item A word type represent {\color{uured} meaning in a low-dimensional semantic space}
\item The distributional hypothesis:
\begin{itemize}
  \item Harris (1954) and Firth (1957): \\ ``A word is characterized by the company it keeps''
  \item Semantics (broadly defined) is captured by  {\color{uured} context}
\end{itemize}
\item Lots of different embeddings: \\ \texttt{word2vec}, GloVe, Probabilistic Embeddings
\end{itemize}


\end{frame}


\begin{frame}{Word embeddings}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{fig/Mikolov_2013_word_projections.png}
\caption{Word embedding properties (Mikolov et al, 2013)}
\end{figure}
\[
\text{king} - \text{man} + \text{woman} \approx \text{queen}
\]

\pause
%\left
But also (Bolukbasi et al., 2016):
\[
\text{computer programmer} - \text{man} + \text{woman} \approx \text{homemaker}
\]


\end{frame}


\begin{frame}{Context Matters!}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/elmo-embedding-robin-williams.png}
\caption{Context matters (Alammar, 2020)}
\end{figure}

\end{frame}


\section{Recurrent Neural Networks}
\frame{\sectionpage}

\begin{frame}{Recurrent Neural Networks}

\begin{itemize}
\item Recurrent Neural Networks, Recurrent Nets, RNN, ...
\item Modeling of {\color{uured} temporal data structures}, such as
\begin{itemize}
\item Time series data
\item Sequences of words (language models)
\end{itemize}
\pause
\item Examples of applications:
\begin{itemize}
\item Text classification
\item Sequence / word classification
\item Time series predictions
\item Audio data
\end{itemize}

\end{itemize}

\end{frame}

\begin{frame}{Recurrent Neural Networks}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/DL_10_3_RNN.png}
\caption{Recurrent Neural Network (Goodfellow et al, 2017, Fig. 10.3)}
\end{figure}

\end{frame}


\begin{frame}{Recurrent Neural Networks}

\begin{align*}
a_t &= b + W h_{t-1} + U x_t \\
h_t &= \sigma_1(a_t) \\
o_t &= c + V h_{t} \\
\hat{y}_t & = \sigma_{\text{output}}(o_t) = \text{softmax}(o_t)
\end{align*}

Think of $h_t$ as the "state" at timepoint $t$

\end{frame}



\begin{frame}{Recurrent network with one output}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/DL_10_5_RNN_one_class.png}
\caption{Recurrent Neural Network with one output (Goodfellow et al, 2017, Fig. 10.5)}
\end{figure}

\end{frame}

\begin{frame}{Sequence to Sequence: Encoder-Decoder}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{fig/DL_10_12_Encoder_Decoder.png}
\caption{Encoder-Decoder Recurrent Networks (Goodfellow et al, 2017, Fig. 10.12)}
\end{figure}

\end{frame}


\begin{frame}{Problems with RNN}

\begin{itemize}
%\item Predicting sequences of {\color{uured} different lengths}
\item {\color{uured} Exploding and vanishing} gradients
\item {\color{uured} Long-term} dependencies
\end{itemize}
% TODO: Add 10.36-10.39 on the vanishing and exploding gradient

\end{frame}

%\begin{frame}{Bi-Directional RNN}

%\begin{figure}[h]
%\centering
%\includegraphics[width=0.8\textwidth]{fig/DLR_6_21_BiRNN}
%\caption{Bi-Directional RNN (Chollet and Allaire, 2018, Fig. 6.21)}
%\end{figure}

%\end{frame}

% We ignore LSTM from 2024
% \input{LSTM.tex}



\section{Transformers}
\frame{\sectionpage}

\begin{frame}{The Transformer}

\begin{itemize}
\item Introduced in 2017 in Vaswani et al. (2017)
\item Behind the recent progress in NLP: BERT, GPT-2, GPT-3, etc.
\pause
\item Becoming de-facto \uured{standard} in industry and academia
\pause
\item Four benefits:
\begin{itemize}
\item Enables more {\color{uured} parallelism}
\item Better handling of {\color{uured} long-range dependencies}
\item Brings {\color{uured} transfer learning} to text data
\item Enables {\color{uured} deeper} networks
\end{itemize}
\end{itemize}

\end{frame}



\begin{frame}{A Sequence-to-Sequence Model}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/alammar_the_transformer_3.png}
\caption{Attention (Allamar, 2018)}
\end{figure}

\end{frame}


\begin{frame}{Stacked Encoder-Decoder Structure}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/alammar_The_transformer_encoder_decoder_stack}
\caption{Attention (Allamar, 2018)}
\end{figure}

\end{frame}


\begin{frame}{Transformer}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{fig/Vaswani_1_transformer.png}
\caption{The Transformer Architecture (Vaswani et al., 2017)}
\end{figure}



\end{frame}

\begin{frame}{The encoder vs. the decoder}

\begin{itemize}
\item Encoder:
\begin{itemize}
\item Input: words (embeddings)
\item Output: contextualized embeddings
\end{itemize}
\item Decoder:
\begin{itemize}
\item Input: contextualized embeddings {\color{uured} and previous words} (embeddings)
\item Output: words (embeddings)
\end{itemize}
\end{itemize}
\end{frame}



\begin{frame}{The Transformer Layer (Encoder layer)}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/alammar_encoder_with_tensors_2.png}
\caption{The Encoder Layer (Alammar, 2018b)}
\end{figure}

\end{frame}


\subsection{Attention}

\begin{frame}{Scaled Dot-Product Attention}

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{fig/Vaswani_2_scaled_dot.png}
\caption{Scaled Dot-Product Attention (Vaswani et al., 2017)}
\end{figure}
\[
\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt {d_k}}\right) \mathbf{V}
\]

\end{frame}


\begin{frame}{Attention components}

\begin{itemize}
\item (Q)uery: Word $i$ query other words
\item (K)ey: The other words return their key to $i$
\item (V)alue: The value of the other words to $i$
\end{itemize}
\end{frame}


\begin{frame}{Computing Q, V and K}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/alammar_transformer_self_attention_vectors.png}
\caption{Attention heads (Alammar, 2018b)}
\end{figure}

\end{frame}

\begin{frame}{Computing Self-Attention}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/alammar_self-attention-output.png}
\caption{Attention (Alammar, 2018b)}
\end{figure}

\end{frame}


\subsection{Multi-Head Attention}

\begin{frame}{Multi-Head Attention}

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{fig/Vaswani_2_multi_head.png}
\caption{Scaled Dot-Product Attention (Vaswani et al., 2017)}
\end{figure}

\end{frame}


\begin{frame}{Attentions Heads}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/alammar_transformer_attention_heads_qkv.png}
\caption{Attention heads (Alammar, 2018b)}
\end{figure}

\end{frame}


\begin{frame}{Multi-head attention}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/alammar_transformer_multi-headed_self-attention-recap.png}
\caption{Attention heads (Alammar, 2018b)}
\end{figure}

\end{frame}


\begin{frame}{Multi-Head Attention example}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/Vaswani_3_attention.png}
\caption{Attention (Vaswani et al., 2017)}
\end{figure}

\end{frame}

\subsection{Tokenization}


\begin{frame}{Tokenization}

\begin{itemize}
    \item Subword tokenization is commonly used
    \pause
    \item The main problem with tokenization
    \begin{enumerate}
        \item Very large vocabulary size
        \item Out-of-vocabulary (OOV) tokens
        \item Different meanings of very similar words
    \end{enumerate}
    \pause
    \item Two common approaches:
    \begin{enumerate}
        \item Byte-pair encoding (GPT-2, RoBERTa)
        \item WordPiece (BERT)
    \end{enumerate}
\end{itemize}

\end{frame}

\begin{frame}{Byte-pair encoding}

\begin{itemize}
    \item Gage, Philip (1994). "A New Algorithm for Data Compression"
    \pause
    \item Encode the most common pairs iteratively
\begin{enumerate}
    \item look for the most frequent pairing
    \item merge them
    \item repeat (until token or iteration limit)
\end{enumerate}

    \pause
    \item Example (Wikipedia): \texttt{aaabdaaabac}\\ \pause \vspace{2mm}
    Step 1: \texttt{ZabdZabac}\\
    \texttt{Z=aa}\\ \pause \vspace{2mm}
    Step 2: \texttt{ZYdZYac}\\
    \texttt{Z=aa}\\
    \texttt{Y=ab}\\ \pause \vspace{2mm}
    Step 3: \texttt{XdXac}\\
    \texttt{Z=aa}\\
    \texttt{Y=ab}\\
    \texttt{X=ZY}
\end{itemize}

\end{frame}


\begin{frame}{Byte-pair encoding}

\begin{enumerate}
    \item look for the most frequent pairing
    \item merge them
    \item repeat (until token or iteration limit)
\end{enumerate}

\begin{itemize}
     \item Example : 9:\texttt{text\_}, 10:\texttt{texting\_},11:\texttt{context\_}\\ \pause \vspace{2mm}
    Step 1 (most common: "te"):
    \{\texttt{te}\} \pause \vspace{2mm} \\
    ... \\
    Step i (most common: "text\_"):
    \{\texttt{text\_}\} \pause \vspace{2mm} \\
    ... \\
    Step j (most common: "con"):
    \{\texttt{text\_},\text{con}\} \pause \vspace{2mm} \\
    ... \\
    Step k (most common: "texting"):
    \{\texttt{text\_},\text{con},\texttt{texting}\} \pause \vspace{2mm} \\

\end{itemize}

\end{frame}


\begin{frame}{WordPiece}

\begin{itemize}
    \item BPE difficulty: Which pair to choose (if they are approx. equally frequent)?
    \pause
    \item Schuster and Kaisuke (2012) present the WordPiece model
    \pause
    \item Let $P(i,j)$ be the probability of observing the pair $ij$ and $P(i)$ observing $i$.
    \pause
    \item \uured{BPE}: Choose highest $P(i,j)$
    \pause
    \item \uured{Wordpiece}: Choose highest $P(i,j)/(P(i)P(j))$
\end{itemize}

\end{frame}


\subsection{Positional encoding}

\begin{frame}{Positional Encoding}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/alammar_transformer_positional_encoding_vectors.png}
\caption{Attention heads (Alammar, 2018b)}
\end{figure}

\end{frame}

\begin{frame}{Positional Encoding}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/alammar_transformer_positional_encoding_example.png}
\caption{Adding positional encodings to embeddings (Alammar, 2018b)}
\end{figure}

\end{frame}

\subsection{Add and Normalize}

\begin{frame}{Add and Normalize}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{fig/alammar_transformer_resideual_layer_norm_2.png}
\caption{Add and Normalize (Alammar, 2018b)}
\end{figure}

\end{frame}



\begin{frame}{Transformer}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{fig/Vaswani_1_transformer.png}
\caption{The Transformer Architecture (Vaswani et al., 2017)}
\end{figure}

\end{frame}


%\begin{frame}{Transformer Decoder}
%\begin{figure}[h]
%\centering
%\includegraphics[width=1\textwidth]{fig/alammar_Transformer_decoder.png}
%\caption{Attention heads (Vaswani et al., 2017)}
%\end{figure}
%\end{frame}

\section{BERT}
\frame{\sectionpage}

\begin{frame}{BERT}

\begin{itemize}
\item Bidirectional Encoder Representations from Transformers (BERT)
\item Introduced in 2018/2019 in Devlin et al. (2018)
\pause
\item {\color{uured} State-of-the-Art} in many text prediction tasks, such as
\begin{itemize}
\item Question-Answering
\item Named-Entity Recognition
\item Text Classification
\end{itemize}
\pause
\item Many new versions has come out RoBERTa, DistillBERT, AlBERT etc.
\pause
\item {\color{uured} Pre-trained} on a large corpus
\pause
\item Available both in English, Swedish and many other languages (The National Library)
\end{itemize}

\end{frame}


\begin{frame}{BERT and transfer learning}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{fig/bert-transfer-learning.png}
\caption{Using BERT for Transfer Learning (Alammar, 2018c)}
\end{figure}

\end{frame}


\begin{frame}{The BERT model}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/bert-input-output.png}
\caption{The BERT model (Alammar, 2018c)}
\end{figure}

\end{frame}


\begin{frame}{BERT Architecture}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/bert-encoders-input.png}
\caption{Opening up BERT (Vaswani et al., 2017)}
\end{figure}

\end{frame}



\subsection{Training BERT}


\begin{frame}{Task 1: Masked Language Model}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/BERT-language-modeling-masked-lm.png}
\caption{Masked Language Modeling (Alammar, 2018c)}
\end{figure}

\end{frame}


\begin{frame}{Next Sentence Prediction}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/bert-next-sentence-prediction.png}
\caption{Next Sentence Prediction (Alammar, 2018c)}
\end{figure}

\end{frame}



\subsection{Using BERT}

\begin{frame}{Using BERT for Classification}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/bert-classifier.png}
\caption{Using BERT for classification (Alammar, 2018c)}
\end{figure}

\end{frame}

\begin{frame}{BERT and Contextualized embeddings}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/bert-contexualized-embeddings.png}
\caption{Contextualized Embeddings (Alammar, 2018c)}
\end{figure}

\end{frame}

\begin{frame}{Using Contextualized Embeddings}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig/bert-feature-extraction-contextualized-embeddings.png}
\caption{Using Contextualized Embeddings (Alammar, 2018c)}
\end{figure}

\end{frame}




\end{document}
